<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
    xmlns:content="http://purl.org/rss/1.0/modules/content/"
    xmlns:dc="http://purl.org/dc/elements/1.1/"
    xmlns:atom="http://www.w3.org/2005/Atom"
    xmlns:sy="http://purl.org/rss/1.0/modules/syndication/">

    <channel>
        <title>Martin Kleppmann's blog</title>
        <atom:link href="http://martin.kleppmann.com/feed.rss" rel="self" type="application/rss+xml" />
        <link>http://martin.kleppmann.com/</link>
        <description>Entrepreneurship, web technology and the user experience</description>
        <lastBuildDate>Fri, 24 May 2013 16:51:34 BST</lastBuildDate>
        <language>en</language>
        <sy:updatePeriod>hourly</sy:updatePeriod>
        <sy:updateFrequency>1</sy:updateFrequency>

        
            <item>
                <title>Improving the security of your SSH private key files</title>
                <link>http://martin.kleppmann.com/2013/05/24/improving-security-of-ssh-private-keys.html</link>
                <comments>http://martin.kleppmann.com/2013/05/24/improving-security-of-ssh-private-keys.html#disqus_thread</comments>
                <pubDate>Fri, 24 May 2013 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2013/05/24/improving-security-of-ssh-private-keys.html</guid>
                
                <description><![CDATA[ Ever wondered how those key files in ~/.ssh actually work? How secure are they actually? As you probably do too, I use ssh many times every single day &#8212; every git fetch and git push, every deploy, every login to a server. And recently I realised that to me, ssh... ]]></description>
                <content:encoded><![CDATA[
                    <p>Ever wondered how those key files in <code>~/.ssh</code> actually <em>work</em>? How secure are they actually?</p>

<p>As you probably do too, I use ssh many times every single day &#8212; every <code>git fetch</code> and <code>git push</code>, every deploy, every login to a server. And recently I realised that to me, ssh was just some crypto voodoo that I had become accustomed to using, but I didn&#8217;t really understand. That&#8217;s a shame &#8212; I like to know how stuff works. So I went on a little journey of discovery, and here are some of the things I found.</p>

<p>When you start reading about &#8220;crypto stuff&#8221;, you very quickly get buried in an avalanche of acronyms. I will briefly mention the acronyms as we go along; they don&#8217;t help you understand the concepts, but they are useful in case you want to Google for further details.</p>

<p>Quick recap: If you&#8217;ve ever used public key authentication, you probably have a file <code>~/.ssh/id_rsa</code> or <code>~/.ssh/id_dsa</code> in your home directory. This is your RSA/DSA private key, and <code>~/.ssh/id_rsa.pub</code> or <code>~/.ssh/id_dsa.pub</code> is its public key counterpart. Any machine you want to log in to needs to have your public key in <code>~/.ssh/authorized_keys</code> on that machine. When you try to log in, your SSH client uses a digital signature to prove that you have the private key; the server checks that the signature is valid, and that the public key is authorized for your username; if all is well, you are granted access.</p>

<p>So what is actually inside this private key file?</p>

<h2 id='the_unencrypted_private_key_format'>The unencrypted private key format</h2>

<p>Everyone recommends that you protect your private key with a passphrase (otherwise anybody who steals the file from you can log into everything you have access to). If you leave the passphrase blank, the key is not encrypted. Let&#8217;s look at this unencrypted format first, and consider passphrase protection later.</p>

<p>A ssh private key file typically looks something like this:</p>

<pre><code>-----BEGIN RSA PRIVATE KEY-----
MIIEogIBAAKCAQEArCQG213utzqE5YVjTVF5exGRCkE9OuM7LCp/FOuPdoHrFUXk
y2MQcwf29J3A4i8zxpES9RdSEU6iIEsow98wIi0x1/Lnfx6jG5Y0/iQsG1NRlNCC
aydGvGaC+PwwWiwYRc7PtBgV4KOAVXMZdMB5nFRaekQ1ksdH/360KCGgljPtzTNl
09e97QBwHFIZ3ea5Eih/HireTrRSnvF+ywmwuxX4ubDr0ZeSceuF2S5WLXH2+TV0
   ... etc ... lots of base64 blah blah ...
-----END RSA PRIVATE KEY-----</code></pre>

<p>The private key is an <a href='http://en.wikipedia.org/wiki/ASN.1'>ASN.1</a> data structure, serialized to a byte string using <a href='http://en.wikipedia.org/wiki/X.690#DER_encoding'>DER</a>, and then <a href='http://tools.ietf.org/html/rfc4648'>Base64</a>-encoded. ASN.1 is roughly comparable to JSON (it supports various data types such as integers, booleans, strings and lists/sequences that can be nested in a tree structure). It&#8217;s very widely used for cryptographic purposes, but it has somehow fallen out of fashion with the web generation (I don&#8217;t know why, it seems like a pretty decent format).</p>

<p>To look inside, let&#8217;s generate a fake RSA key without passphrase using <a href='http://www.openbsd.org/cgi-bin/man.cgi?query=ssh-keygen&amp;sektion=1'>ssh-keygen</a>, and then decode it using <a href='http://www.openssl.org/docs/apps/asn1parse.html'>asn1parse</a>:</p>

<pre><code>$ ssh-keygen -t rsa -N &#39;&#39; -f test_rsa_key
$ openssl asn1parse -in test_rsa_key
    0:d=0  hl=4 l=1189 cons: SEQUENCE
    4:d=1  hl=2 l=   1 prim: INTEGER           :00
    7:d=1  hl=4 l= 257 prim: INTEGER           :C36EB2429D429C7768AD9D879F98C...
  268:d=1  hl=2 l=   3 prim: INTEGER           :010001
  273:d=1  hl=4 l= 257 prim: INTEGER           :A27759F60AEA1F4D1D56878901E27...
  534:d=1  hl=3 l= 129 prim: INTEGER           :F9D23EF31A387694F03AD0D050265...
  666:d=1  hl=3 l= 129 prim: INTEGER           :C84415C26A468934F1037F99B6D14...
  798:d=1  hl=3 l= 129 prim: INTEGER           :D0ACED4635B5CA5FB896F88BB9177...
  930:d=1  hl=3 l= 128 prim: INTEGER           :511810DF9AFD590E11126397310A6...
 1061:d=1  hl=3 l= 129 prim: INTEGER           :E3A296AE14E7CAF32F7E493FDF474...</code></pre>

<p>Alternatively, you can paste the Base64 string into Lapo Luchini&#8217;s excellent <a href='http://lapo.it/asn1js/'>JavaScript ASN.1 decoder</a>. You can see that ASN.1 structure is quite simple: a sequence of nine integers. Their meaning is defined in <a href='http://tools.ietf.org/html/rfc2313#section-7.2'>RFC2313</a>. The first integer is a version number (0), and the third number is quite small (65537) &#8211; the public exponent <em>e</em>. The two important numbers are the 2048-bit integers that appear second and fourth in the sequence: the RSA modulus <em>n</em>, and the private exponent <em>d</em>. These numbers are used directly in the <a href='http://en.wikipedia.org/wiki/RSA_%28algorithm%29'>RSA algorithm</a>. The remaining five numbers can be derived from <em>n</em> and <em>d</em>, and are only cached in the key file to speed up certain operations.</p>

<p>DSA keys are similar, a <a href='http://blog.ngas.ch/archives/2008/10/23/asn_1_for_dsa_public_and_private_keys/index.html'>sequence of six integers</a>:</p>

<pre><code>$ ssh-keygen -t dsa -N &#39;&#39; -f test_dsa_key
$ openssl asn1parse -in test_dsa_key
    0:d=0  hl=4 l= 444 cons: SEQUENCE
    4:d=1  hl=2 l=   1 prim: INTEGER           :00
    7:d=1  hl=3 l= 129 prim: INTEGER           :E497DFBFB5610906D18BCFB4C3CCD...
  139:d=1  hl=2 l=  21 prim: INTEGER           :CF2478A96A941FB440C38A86F22CF...
  162:d=1  hl=3 l= 129 prim: INTEGER           :83218C0CA49BA8F11BE40EE1A7C72...
  294:d=1  hl=3 l= 128 prim: INTEGER           :16953EA4012988E914B466B9C37CB...
  425:d=1  hl=2 l=  21 prim: INTEGER           :89A356E922688EDEB1D388258C825...</code></pre>

<h2 id='passphraseprotected_keys'>Passphrase-protected keys</h2>

<p>Next, in order to make life harder for an attacker who manages to steal your private key file, you protect it with a passphrase. How does this actually work?</p>

<pre><code>$ ssh-keygen -t rsa -N &#39;super secret passphrase&#39; -f test_rsa_key
$ cat test_rsa_key
-----BEGIN RSA PRIVATE KEY-----
Proc-Type: 4,ENCRYPTED
DEK-Info: AES-128-CBC,D54228DB5838E32589695E83A22595C7

3+Mz0A4wqbMuyzrvBIHx1HNc2ZUZU2cPPRagDc3M+rv+XnGJ6PpThbOeMawz4Cbu
lQX/Ahbx+UadJZOFrTx8aEWyZoI0ltBh9O5+ODov+vc25Hia3jtayE51McVWwSXg
wYeg2L6U7iZBk78yg+sIKFVijxiWnpA7W2dj2B9QV0X3ILQPxbU/cRAVTd7AVrKT
    ... etc ...
-----END RSA PRIVATE KEY-----</code></pre>

<p>We&#8217;ve gained two header lines, and if you try to parse that Base64 text, you&#8217;ll find it&#8217;s no longer valid ASN.1. That&#8217;s because the entire ASN.1 structure we saw above has been encrypted, and the Base64-encoded text is the output of the encryption. The header tells us the encryption algorithm that was used: <a href='http://en.wikipedia.org/wiki/Advanced_Encryption_Standard'>AES-128</a> in <a href='http://en.wikipedia.org/wiki/Block_cipher_modes_of_operation#Cipher-block_chaining_.28CBC.29'>CBC mode</a>. The 128-bit hex string in the <code>DEK-Info</code> header is the <a href='http://en.wikipedia.org/wiki/Initialization_vector'>initialization vector</a> (IV) for the cipher. This is pretty standard stuff; all common crypto libraries can handle it.</p>

<p>But how do you get from the passphrase to the AES encryption key? I couldn&#8217;t find it documented anywhere, so I had to dig through the OpenSSL source to find it:</p>

<ol>
<li>Append the first 8 bytes of the IV to the passphrase, without a separator (serves as a salt).</li>

<li>Take the MD5 hash of the resulting string (once).</li>
</ol>

<p>That&#8217;s it. To prove it, let&#8217;s decrypt the private key manually (using the IV/salt from the <code>DEK-Info</code> header above):</p>

<pre><code>$ tail -n +4 test_rsa_key | grep -v &#39;END &#39; | base64 -D |    # get just the binary blob
  openssl aes-128-cbc -d -iv D54228DB5838E32589695E83A22595C7 -K $(
    ruby -rdigest/md5 -e &#39;puts Digest::MD5.hexdigest([&quot;super secret passphrase&quot;,0xD5,0x42,0x28,0xDB,0x58,0x38,0xE3,0x25].pack(&quot;a*cccccccc&quot;))&#39;
  ) |
  openssl asn1parse -inform DER</code></pre>

<p>&#8230;which prints out the sequence of integers from the RSA key in the clear. Of course, if you want to inspect the key, it&#8217;s much easier to do this:</p>

<pre><code>$ openssl rsa -text -in test_rsa_key -passin &#39;pass:super secret passphrase&#39;</code></pre>

<p>but I wanted to demonstrate exactly how the AES key is derived from the password. This is important because the private key protection has two weaknesses:</p>

<ul>
<li>The digest algorithm is hard-coded to be MD5, which means that without changing the format, it&#8217;s not possible to upgrade to another hash function (e.g. SHA-1). This could be a problem if MD5 turns out not to be good enough.</li>

<li>The hash function is only applied once &#8212; there is no <a href='http://en.wikipedia.org/wiki/Key_stretching'>stretching</a>. This is a problem because MD5 and AES are both fast to compute, and thus a short passphrase is quite easy to break with brute force.</li>
</ul>

<p>If your private SSH key ever gets into the wrong hands, e.g. because someone steals your laptop or your backup hard drive, the attacker can try a huge number of possible passphrases, even with moderate computing resources. If your passphrase is a dictionary word, it can probably be broken in a matter of seconds.</p>

<p>That was the bad news: the passphrase on your SSH key isn&#8217;t as useful as you thought it was. But there is good news: you can upgrade to a more secure private key format, and everything continues to work!</p>

<h2 id='better_key_protection_with_pkcs8'>Better key protection with PKCS#8</h2>

<p>What we want is to derive a symmetric encryption key from the passphrase, and we want this derivation to be slow to compute, so that an attacker needs to buy more computing time if they want to brute-force the passphrase. If you&#8217;ve seen the <a href='http://codahale.com/how-to-safely-store-a-password/'>use bcrypt</a> meme, this should sound very familiar.</p>

<p>For SSH private keys, there are a few standards with clumsy names (acronym alert!) that can help us out:</p>

<ul>
<li><a href='http://tools.ietf.org/html/rfc2898#section-5.2'>PKCS #5 (RFC 2898)</a> defines <a href='http://en.wikipedia.org/wiki/PBKDF2'>PBKDF2</a> (Password-Based Key Derivation Function 2), an algorithm for deriving an encryption key from a password by applying a hash function repeatedly. PBES2 (Password-Based Encryption Scheme 2) is also defined here; it simply means using a PBKDF2-generated key with a symmetric cipher.</li>

<li><a href='http://tools.ietf.org/html/rfc5208'>PKCS #8 (RFC 5208)</a> defines a format for storing encrypted private keys that supports PBKDF2. OpenSSL transparently supports private keys in PKCS#8 format, and OpenSSH uses OpenSSL, so if you&#8217;re using OpenSSH that means you can swap your traditional SSH key files for PKCS#8 files and everything continues to work as normal!</li>
</ul>

<p>I don&#8217;t know why <code>ssh-keygen</code> still generates keys in SSH&#8217;s traditional format, even though a better format has been available for years. Compatibility with servers is not a concern, because the private key never leaves your machine. Fortunately it&#8217;s easy enough to <a href='http://www.openssl.org/docs/apps/pkcs8.html'>convert to PKCS#8</a>:</p>

<pre><code>$ mv test_rsa_key test_rsa_key.old
$ openssl pkcs8 -topk8 -v2 des3 \
    -in test_rsa_key.old -passin &#39;pass:super secret passphrase&#39; \
    -out test_rsa_key -passout &#39;pass:super secret passphrase&#39;</code></pre>

<p>If you try using this new PKCS#8 file with a SSH client, you should find that it works exactly the same as the file generated by <code>ssh-keygen</code>. But what&#8217;s inside it?</p>

<pre><code>$ cat test_rsa_key
-----BEGIN ENCRYPTED PRIVATE KEY-----
MIIFDjBABgkqhkiG9w0BBQ0wMzAbBgkqhkiG9w0BBQwwDgQIOu/S2/v547MCAggA
MBQGCCqGSIb3DQMHBAh4q+o4ELaHnwSCBMjA+ho9K816gN1h9MAof4stq0akPoO0
CNvXdtqLudIxBq0dNxX0AxvEW6exWxz45bUdLOjQ5miO6Bko0lFoNUrOeOo/Gq4H
dMyI7Ot1vL9UvZRqLNj51cj/7B/bmfa4msfJXeuFs8jMtDz9J19k6uuCLUGlJscP
    ... etc ...
-----END ENCRYPTED PRIVATE KEY-----</code></pre>

<p>Notice that the header/footer lines have changed (<code>BEGIN ENCRYPTED PRIVATE KEY</code> instead of <code>BEGIN RSA PRIVATE KEY</code>), and the plaintext <code>Proc-Type</code> and <code>DEK-Info</code> headers have gone. In fact, the whole key file is once again a ASN.1 structure:</p>

<pre><code>$ openssl asn1parse -in test_rsa_key
    0:d=0  hl=4 l=1294 cons: SEQUENCE
    4:d=1  hl=2 l=  64 cons: SEQUENCE
    6:d=2  hl=2 l=   9 prim: OBJECT            :PBES2
   17:d=2  hl=2 l=  51 cons: SEQUENCE
   19:d=3  hl=2 l=  27 cons: SEQUENCE
   21:d=4  hl=2 l=   9 prim: OBJECT            :PBKDF2
   32:d=4  hl=2 l=  14 cons: SEQUENCE
   34:d=5  hl=2 l=   8 prim: OCTET STRING      [HEX DUMP]:3AEFD2DBFBF9E3B3
   44:d=5  hl=2 l=   2 prim: INTEGER           :0800
   48:d=3  hl=2 l=  20 cons: SEQUENCE
   50:d=4  hl=2 l=   8 prim: OBJECT            :des-ede3-cbc
   60:d=4  hl=2 l=   8 prim: OCTET STRING      [HEX DUMP]:78ABEA3810B6879F
   70:d=1  hl=4 l=1224 prim: OCTET STRING      [HEX DUMP]:C0FA1A3D2BCD7A80DD61F4C0287F8B2D...</code></pre>

<p>Use Lapo Luchini&#8217;s <a href='http://lapo.it/asn1js/'>JavaScript ASN.1 decoder</a> to display a nice ASN.1 tree structure:</p>

<pre><code>Sequence (2 elements)
|- Sequence (2 elements)
|  |- Object identifier: 1.2.840.113549.1.5.13            // using PBES2 from PKCS#5
|  `- Sequence (2 elements)
|     |- Sequence (2 elements)
|     |  |- Object identifier: 1.2.840.113549.1.5.12      // using PBKDF2 -- yay! :)
|     |  `- Sequence (2 elements)
|     |     |- Byte string (8 bytes): 3AEFD2DBFBF9E3B3    // salt
|     |     `- Integer: 2048                              // iteration count
|     `- Sequence (2 elements)
|          Object identifier: 1.2.840.113549.3.7          // encrypted with Triple DES, CBC
|          Byte string (8 bytes): 78ABEA3810B6879F        // initialization vector
`- Byte string (1224 bytes): C0FA1A3D2BCD7A80DD61F4C0287F8B2DAB46A43E...  // encrypted key blob</code></pre>

<p>The format uses <a href='http://en.wikipedia.org/wiki/Object_identifier'>OIDs</a>, numeric codes allocated by a registration authority to unambiguously refer to algorithms. The OIDs in this key file tell us that the encryption scheme is <a href='http://oid-info.com/get/1.2.840.113549.1.5.13'>pkcs5PBES2</a>, that the key derivation function is <a href='http://oid-info.com/get/1.2.840.113549.1.5.12'>PBKDF2</a>, and that the encryption is performed using <a href='http://oid-info.com/get/1.2.840.113549.3.7'>des-ede3-cbc</a>. The hash function can be explicitly specified if needed; here it&#8217;s omitted, which means that it <a href='http://tools.ietf.org/html/rfc3370#section-4.4.1'>defaults</a> to <a href='http://tools.ietf.org/html/rfc2104'>hMAC-SHA1</a>.</p>

<p>The nice thing about having all those identifiers in the file is that if better algorithms are invented in future, we can upgrade the key file without having to change the container file format.</p>

<p>You can also see that the key derivation function uses an iteration count of 2,048. Compared to just one iteration in the traditional SSH key format, that&#8217;s good &#8212; it means that it&#8217;s much slower to brute-force the passphrase. The number 2,048 is currently hard-coded in OpenSSL; I hope that it will be configurable in future, as you could probably increase it without any noticeable slowdown on a modern computer.</p>

<h2 id='conclusion_better_protection_for_your_ssh_private_keys'>Conclusion: better protection for your SSH private keys</h2>

<p>If you already have a strong passphrase on your SSH private key, then converting it from the traditional private key format to PKCS#8 is roughly comparable to adding two extra keystrokes to your passphrase, for free. And if you have a weak passphrase, you can take your private key protection from &#8220;easily breakable&#8221; to &#8220;slightly harder to break&#8221;.</p>

<p>It&#8217;s so easy, you can do it right now:</p>

<pre><code>$ mv ~/.ssh/id_rsa ~/.ssh/id_rsa.old
$ openssl pkcs8 -topk8 -v2 des3 -in ~/.ssh/id_rsa.old -out ~/.ssh/id_rsa
$ chmod 600 ~/.ssh/id_rsa
# Check that the converted key works; if yes, delete the old one:
$ rm ~/.ssh/id_rsa.old</code></pre>

<p>The <code>openssl pkcs8</code> command asks for a passphrase three times: once to unlock your existing private key, and twice for the passphrase for the new key. It doesn&#8217;t matter whether you use a new passphrase for the converted key or keep it the same as the old key.</p>

<p>Not all software can read the PKCS8 format, but that&#8217;s fine &#8212; only your SSH client needs to be able to read the private key, after all. From the server&#8217;s point of view, storing the private key in a different format changes nothing at all.</p>
                ]]></content:encoded>
            </item>
        
            <item>
                <title>Schema evolution in Avro, Protocol Buffers and Thrift</title>
                <link>http://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html</link>
                <comments>http://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html#disqus_thread</comments>
                <pubDate>Wed, 05 Dec 2012 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html</guid>
                
                <description><![CDATA[ So you have some data that you want to store in a file or send over the network. You may find yourself going through several phases of evolution: Using your programming language&#8217;s built-in serialization, such as Java serialization, Ruby&#8217;s marshal, or Python&#8217;s pickle. Or maybe you even invent your own... ]]></description>
                <content:encoded><![CDATA[
                    <p>So you have some data that you want to store in a file or send over the network. You may find yourself going through several phases of evolution:</p>

<ol>
<li>Using your programming language&#8217;s built-in serialization, such as <a href='http://docs.oracle.com/javase/6/docs/platform/serialization/spec/serialTOC.html'>Java serialization</a>, Ruby&#8217;s <a href='http://www.ruby-doc.org/core-1.9.3/Marshal.html'>marshal</a>, or Python&#8217;s <a href='http://docs.python.org/3.3/library/pickle.html'>pickle</a>. Or maybe you even invent your own format.</li>

<li>Then you realise that being locked into one programming language sucks, so you move to using a widely supported, language-agnostic format like JSON (or XML if you like to party like it&#8217;s 1999).</li>

<li>Then you decide that JSON is too verbose and too slow to parse, you&#8217;re annoyed that it doesn&#8217;t differentiate integers from floating point, and think that you&#8217;d quite like binary strings as well as Unicode strings. So you invent some sort of binary format that&#8217;s kinda like JSON, but binary (<a href='http://msgpack.org/'>1</a>, <a href='http://bsonspec.org/'>2</a>, <a href='http://ubjson.org/'>3</a>, <a href='http://bjson.org/'>4</a>, <a href='http://kaijaeger.com/articles/introducing-bison-binary-interchange-standard.html'>5</a>, <a href='https://github.com/voldemort/voldemort/wiki/Binary-JSON-Serialization'>6</a>).</li>

<li>Then you find that people are stuffing all sorts of random fields into their objects, using inconsistent types, and you&#8217;d quite like a <strong>schema</strong> and some <strong>documentation</strong>, thank you very much. Perhaps you&#8217;re also using a statically typed programming language and want to generate model classes from a schema. Also you realize that your binary JSON-lookalike actually isn&#8217;t all that compact, because you&#8217;re still storing field names over and over again; hey, if you had a schema, you could avoid storing objects&#8217; field names, and you could save some more bytes!</li>
</ol>

<p>Once you get to the fourth stage, your options are typically <a href='http://thrift.apache.org/'>Thrift</a>, <a href='http://code.google.com/p/protobuf/'>Protocol Buffers</a> or <a href='http://avro.apache.org/'>Avro</a>. All three provide efficient, cross-language serialization of data using a schema, and code generation for the Java folks.</p>

<p>Plenty of comparisons have been written about them already (<a href='http://floatingsun.net/articles/thrift-vs-protocol-buffers/'>1</a>, <a href='http://www.igvita.com/2011/08/01/protocol-buffers-avro-thrift-messagepack/'>2</a>, <a href='http://blog.mirthlab.com/2009/06/01/thrift-vs-protocol-bufffers-vs-json/'>3</a>, <a href='http://tech.puredanger.com/2011/05/27/serialization-comparison/'>4</a>). However, many posts overlook a detail that seems mundane at first, but is actually cruicial: <strong>What happens if the schema changes?</strong></p>

<p>In real life, data is always in flux. The moment you think you have finalised a schema, someone will come up with a use case that wasn&#8217;t anticipated, and wants to &#8220;just quickly add a field&#8221;. Fortunately Thrift, Protobuf and Avro all support <strong>schema evolution</strong>: you can change the schema, you can have producers and consumers with different versions of the schema at the same time, and it all continues to work. That is an extremely valuable feature when you&#8217;re dealing with a big production system, because it allows you to update different components of the system independently, at different times, without worrying about compatibility.</p>

<p>Which brings us to the topic of today&#8217;s post. I would like to explore how Protocol Buffers, Avro and Thrift actually encode data into bytes &#8212; and this will also help explain how each of them deals with schema changes. The design choices made by each of the frameworks are interesting, and by comparing them I think you can become a better engineer (by a little bit).</p>

<p>The example I will use is a little object describing a person. In JSON I would write it like this:</p>
<div class='highlight'><pre><code class='js'><span class='p'>{</span>
    <span class='s2'>&quot;userName&quot;</span><span class='o'>:</span> <span class='s2'>&quot;Martin&quot;</span><span class='p'>,</span>
    <span class='s2'>&quot;favouriteNumber&quot;</span><span class='o'>:</span> <span class='mi'>1337</span><span class='p'>,</span>
    <span class='s2'>&quot;interests&quot;</span><span class='o'>:</span> <span class='p'>[</span><span class='s2'>&quot;daydreaming&quot;</span><span class='p'>,</span> <span class='s2'>&quot;hacking&quot;</span><span class='p'>]</span>
<span class='p'>}</span>
</code></pre></div>
<p>This JSON encoding can be our baseline. If I remove all the whitespace it consumes 82 bytes.</p>

<h2 id='protocol_buffers'>Protocol Buffers</h2>

<p>The Protocol Buffers schema for the person object might look something like this:</p>

<pre><code>message Person {
    required string user_name        = 1;
    optional int64  favourite_number = 2;
    repeated string interests        = 3;
}</code></pre>

<p>When we <a href='https://developers.google.com/protocol-buffers/docs/encoding'>encode</a> the data above using this schema, it uses 33 bytes, as follows:</p>
<a href='http://martin.kleppmann.com/2012/12/protobuf.png'><img height='230' src='http://martin.kleppmann.com/2012/12/protobuf_small.png' width='550' /></a>
<p>Look exactly at how the binary representation is structured, byte by byte. The person record is just the concatentation of its fields. Each field starts with a byte that indicates its tag number (the numbers <code>1</code>, <code>2</code>, <code>3</code> in the schema above), and the type of the field. If the first byte of a field indicates that the field is a string, it is followed by the number of bytes in the string, and then the UTF-8 encoding of the string. If the first byte indicates that the field is an integer, a variable-length encoding of the number follows. There is no array type, but a tag number can appear multiple times to represent a multi-valued field.</p>

<p>This encoding has consequences for schema evolution:</p>

<ul>
<li>There is no difference in the encoding between <code>optional</code>, <code>required</code> and <code>repeated</code> fields (except for the number of times the tag number can appear). This means that you can change a field from <code>optional</code> to <code>repeated</code> and vice versa (if the parser is expecting an <code>optional</code> field but sees the same tag number multiple times in one record, it discards all but the last value). <code>required</code> has an additional validation check, so if you change it, you risk runtime errors (if the sender of a message thinks that it&#8217;s optional, but the recipient thinks that it&#8217;s required).</li>

<li>An <code>optional</code> field without a value, or a <code>repeated</code> field with zero values, does not appear in the encoded data at all &#8212; the field with that tag number is simply absent. Thus, it is safe to remove that kind of field from the schema. However, you must never reuse the tag number for another field in future, because you may still have data stored that uses that tag for the field you deleted.</li>

<li>You can add a field to your record, as long as it is given a new tag number. If the Protobuf parser parser sees a tag number that is not defined in its version of the schema, it has no way of knowing what that field is called. But it <em>does</em> roughly know what type it is, because a 3-bit type code is included in the first byte of the field. This means that even though the parser can&#8217;t exactly interpret the field, it can figure out how many bytes it needs to skip in order to find the next field in the record.</li>

<li>You can rename fields, because field names don&#8217;t exist in the binary serialization, but you can never change a tag number.</li>
</ul>

<p>This approach of using a tag number to represent each field is simple and effective. But as we&#8217;ll see in a minute, it&#8217;s not the only way of doing things.</p>

<h2 id='avro'>Avro</h2>

<p>Avro schemas can be written in two ways, either in a JSON format:</p>
<div class='highlight'><pre><code class='js'><span class='p'>{</span>
    <span class='s2'>&quot;type&quot;</span><span class='o'>:</span> <span class='s2'>&quot;record&quot;</span><span class='p'>,</span>
    <span class='s2'>&quot;name&quot;</span><span class='o'>:</span> <span class='s2'>&quot;Person&quot;</span><span class='p'>,</span>
    <span class='s2'>&quot;fields&quot;</span><span class='o'>:</span> <span class='p'>[</span>
        <span class='p'>{</span><span class='s2'>&quot;name&quot;</span><span class='o'>:</span> <span class='s2'>&quot;userName&quot;</span><span class='p'>,</span>        <span class='s2'>&quot;type&quot;</span><span class='o'>:</span> <span class='s2'>&quot;string&quot;</span><span class='p'>},</span>
        <span class='p'>{</span><span class='s2'>&quot;name&quot;</span><span class='o'>:</span> <span class='s2'>&quot;favouriteNumber&quot;</span><span class='p'>,</span> <span class='s2'>&quot;type&quot;</span><span class='o'>:</span> <span class='p'>[</span><span class='s2'>&quot;null&quot;</span><span class='p'>,</span> <span class='s2'>&quot;long&quot;</span><span class='p'>]},</span>
        <span class='p'>{</span><span class='s2'>&quot;name&quot;</span><span class='o'>:</span> <span class='s2'>&quot;interests&quot;</span><span class='p'>,</span>       <span class='s2'>&quot;type&quot;</span><span class='o'>:</span> <span class='p'>{</span><span class='s2'>&quot;type&quot;</span><span class='o'>:</span> <span class='s2'>&quot;array&quot;</span><span class='p'>,</span> <span class='s2'>&quot;items&quot;</span><span class='o'>:</span> <span class='s2'>&quot;string&quot;</span><span class='p'>}}</span>
    <span class='p'>]</span>
<span class='p'>}</span>
</code></pre></div>
<p>&#8230;or in an IDL:</p>

<pre><code>record Person {
    string               userName;
    union { null, long } favouriteNumber;
    array&lt;string&gt;        interests;
}</code></pre>

<p>Notice that there are no tag numbers in the schema! So how does it work?</p>

<p>Here is the same example data <a href='http://avro.apache.org/docs/current/spec.html'>encoded</a> in just 32 bytes:</p>
<a href='http://martin.kleppmann.com/2012/12/avro.png'><img height='259' src='http://martin.kleppmann.com/2012/12/avro_small.png' width='550' /></a>
<p>Strings are just a length prefix followed by UTF-8 bytes, but there&#8217;s nothing in the bytestream that tells you that it is a string. It could just as well be a variable-length integer, or something else entirely. The only way you can parse this binary data is by reading it alongside the schema, and the schema tells you what type to expect next. You need to have the <strong>exact same version</strong> of the schema as the writer of the data used. If you have the wrong schema, the parser will not be able to make head or tail of the binary data.</p>

<p>So how does Avro support schema evolution? Well, although you need to know the exact schema with which the data was written (the writer&#8217;s schema), that doesn&#8217;t have to be the same as the schema the consumer is expecting (the reader&#8217;s schema). You can actually give <em>two different</em> schemas to the Avro parser, and it uses <a href='http://avro.apache.org/docs/1.7.2/api/java/org/apache/avro/io/parsing/doc-files/parsing.html'>resolution rules</a> to translate data from the writer schema into the reader schema.</p>

<p>This has some interesting consequences for schema evolution:</p>

<ul>
<li>The Avro encoding doesn&#8217;t have an indicator to say which field is next; it just encodes one field after another, in the order they appear in the schema. Since there is no way for the parser to know that a field has been skipped, there is no such thing as an optional field in Avro. Instead, if you want to be able to leave out a value, you can use a union type, like <code>union { null, long }</code> above. This is encoded as a byte to tell the parser which of the possible union types to use, followed by the value itself. By making a union with the <code>null</code> type (which is simply encoded as zero bytes) you can make a field optional.</li>

<li>Union types are powerful, but you must take care when changing them. If you want to add a type to a union, you first need to update all readers with the new schema, so that they know what to expect. Only once all readers are updated, the writers may start putting this new type in the records they generate.</li>

<li>You can reorder fields in a record however you like. Although the fields are encoded in the order they are declared, the parser matches fields in the reader and writer schema by name, which is why no tag numbers are needed in Avro.</li>

<li>Because fields are matched by name, changing the name of a field is tricky. You need to first update all <em>readers</em> of the data to use the new field name, while keeping the old name as an alias (since the name matching uses aliases from the reader&#8217;s schema). Then you can update the writer&#8217;s schema to use the new field name.</li>

<li>You can add a field to a record, provided that you also give it a default value (e.g. <code>null</code> if the field&#8217;s type is a union with <code>null</code>). The default is necessary so that when a reader using the new schema parses a record written with the old schema (and hence lacking the field), it can fill in the default instead.</li>

<li>Conversely, you can remove a field from a record, provided that it previously had a default value. (This is a good reason to give all your fields default values if possible.) This is so that when a reader using the <em>old</em> schema parses a record written with the <em>new</em> schema, it can fall back to the default.</li>
</ul>

<p>This leaves us with the problem of knowing the exact schema with which a given record was written. The best solution depends on the context in which your data is being used:</p>

<ul>
<li>In Hadoop you typically have large files containing millions of records, all encoded with the same schema. <a href='http://avro.apache.org/docs/1.7.2/spec.html#Object+Container+Files'>Object container files</a> handle this case: they just include the schema once at the beginning of the file, and the rest of the file can be decoded with that schema.</li>

<li>In an RPC context, it&#8217;s probably too much overhead to send the schema with every request and response. But if your RPC framework uses long-lived connections, it can negotiate the schema once at the start of the connection, and amortize that overhead over many requests.</li>

<li>If you&#8217;re storing records in a database one-by-one, you may end up with different schema versions written at different times, and so you have to annotate each record with its schema version. If storing the schema itself is too much overhead, you can use a <a href='http://avro.apache.org/docs/1.7.2/spec.html#Schema+Fingerprints'>hash</a> of the schema, or a sequential schema version number. You then need a <a href='https://issues.apache.org/jira/browse/AVRO-1124'>schema registry</a> where you can look up the exact schema definition for a given version number.</li>
</ul>

<p>One way of looking at it: in Protocol Buffers, every field in a record is tagged, whereas in Avro, the entire record, file or network connection is tagged with a schema version.</p>

<p>At first glance it may seem that Avro&#8217;s approach suffers from greater complexity, because you need to go to the additional effort of distributing schemas. However, I am beginning to think that Avro&#8217;s approach also has some distinct advantages:</p>

<ul>
<li>Object container files are wonderfully self-describing: the writer schema embedded in the file contains all the field names and types, and even documentation strings (if the author of the schema bothered to write some). This means you can load these files directly into interactive tools like <a href='http://pig.apache.org/'>Pig</a>, and it Just Worksâ„¢ without any configuration.</li>

<li>As Avro schemas are JSON, you can add your own metadata to them, e.g. describing application-level semantics for a field. And as you distribute schemas, that metadata automatically gets distributed too.</li>

<li>A schema registry is probably a good thing in any case, serving as <a href='https://github.com/ept/avrodoc'>documentation</a> and helping you to find and reuse data. And because you simply can&#8217;t parse Avro data without the schema, the schema registry is guaranteed to be up-to-date. Of course you can set up a protobuf schema registry too, but since it&#8217;s not <em>required</em> for operation, it&#8217;ll end up being on a best-effort basis.</li>
</ul>

<h2 id='thrift'>Thrift</h2>

<p>Thrift is a much bigger project than Avro or Protocol Buffers, as it&#8217;s not just a data serialization library, but also an entire RPC framework. It also has a somewhat different culture: whereas Avro and Protobuf standardize a single binary encoding, Thrift <a href='http://mail-archives.apache.org/mod_mbox/hadoop-general/200904.mbox/%3CC5FEF47F.90BAC%25cwalter%40microsoft.com%3E'>embraces</a> a whole variety of different serialization formats (which it calls &#8220;protocols&#8221;).</p>

<p>Indeed, Thrift has <a href='https://builds.apache.org//job/Thrift/javadoc/org/apache/thrift/protocol/TJSONProtocol.html'>two</a> <a href='https://builds.apache.org//job/Thrift/javadoc/org/apache/thrift/protocol/TSimpleJSONProtocol.html'>different</a> JSON encodings, and no fewer than three different binary encodings. (However, one of the binary encodings, DenseProtocol, is <a href='http://wiki.apache.org/thrift/LibraryFeatures'>only supported in the C++ implementation</a>; since we&#8217;re interested in cross-language serialization, I will focus on the other two.)</p>

<p>All the encodings share the same schema definition, in Thrift IDL:</p>

<pre><code>struct Person {
  1: string       userName,
  2: optional i64 favouriteNumber,
  3: list&lt;string&gt; interests
}</code></pre>

<p>The BinaryProtocol encoding is very straightforward, but also fairly wasteful (it takes 59 bytes to encode our example record):</p>
<a href='http://martin.kleppmann.com/2012/12/binaryprotocol.png'><img height='269' src='http://martin.kleppmann.com/2012/12/binaryprotocol_small.png' width='550' /></a>
<p>The CompactProtocol encoding is semantically equivalent, but uses variable-length integers and bit packing to reduce the size to 34 bytes:</p>
<a href='http://martin.kleppmann.com/2012/12/compactprotocol.png'><img height='276' src='http://martin.kleppmann.com/2012/12/compactprotocol_small.png' width='550' /></a>
<p>As you can see, Thrift&#8217;s approach to schema evolution is the same as Protobuf&#8217;s: each field is manually assigned a tag in the IDL, and the tags and field types are stored in the binary encoding, which enables the parser to skip unknown fields. Thrift defines an explicit list type rather than Protobuf&#8217;s repeated field approach, but otherwise the two are very similar.</p>

<p>In terms of philosophy, the libraries are very different though. Thrift favours the &#8220;one-stop shop&#8221; style that gives you an entire integrated RPC framework and many choices (with <a href='http://wiki.apache.org/thrift/LibraryFeatures'>varying cross-language support</a>), whereas Protocol Buffers and Avro appear to follow much more of a <a href='http://www.faqs.org/docs/artu/ch01s06.html'>&#8220;do one thing and do it well&#8221;</a> style.</p>

<p><em>This post has been translated into <a href='http://www.sjava.net/319'>Korean</a> by Justin Song.</em></p>
                ]]></content:encoded>
            </item>
        
            <item>
                <title>The complexity of user experience</title>
                <link>http://martin.kleppmann.com/2012/10/08/complexity-of-user-experience.html</link>
                <comments>http://martin.kleppmann.com/2012/10/08/complexity-of-user-experience.html#disqus_thread</comments>
                <pubDate>Mon, 08 Oct 2012 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2012/10/08/complexity-of-user-experience.html</guid>
                
                <description><![CDATA[ The problem of overly complex software is nothing new; it is almost as old as software itself. Over and over again, software systems become so complex that they become very difficult to maintain and very time-consuming and expensive to modify. Most developers hate working on such systems, yet nevertheless we... ]]></description>
                <content:encoded><![CDATA[
                    <p>The problem of overly complex software is nothing new; it is almost as old as software itself. Over and over again, software systems become so complex that they become very difficult to maintain and very time-consuming and expensive to modify. Most developers hate working on such systems, yet nevertheless we keep creating new, overly complex systems all the time.</p>

<p>Much has been written about this, including classic papers by Fred Brooks (<a href='http://people.eecs.ku.edu/~saiedian/Teaching/Sp08/816/Papers/Background-Papers/no-silver-bullet.pdf'>No Silver Bullet</a>), and Ben Moseley and Peter Marks (<a href='http://shaffner.us/cs/papers/tarpit.pdf'>Out of the Tar Pit</a>). They are much more worth reading than this post, and it is presumptuous of me to think I could add anything significant to this debate. But I will try nevertheless.</p>

<p>Pretty much everyone agrees that if you have a choice between a simpler software design and a more complex design, all else being equal, that simpler is better. It is also widely thought to be worthwhile to deliberately invest in simplicity &#8212; for example, to spend effort refactoring existing code into a cleaner design &#8212; because the one-off cost of refactoring today is easily offset by the benefits of easier maintenance tomorrow. Also, much thought by many smart people has gone into finding ways of breaking down complex systems into manageable parts with manageable dependencies. I don&#8217;t wish to dispute any of that.</p>

<p>But there is a subtlety that I have been missing in discussions about software complexity, that I feel somewhat ambivalent about, and that I think is worth discussing. It concerns the points where external humans (people outside of the team maintaining the system) touch the system &#8212; as developers using an API exposed by the system, or as end users interacting with a user interface. I will concentrate mostly on user interfaces, but much of this discussion applies to APIs too.</p>

<h2 id='examples'>Examples</h2>

<p>Let me first give a few examples, and then try to extract a pattern from them. They are examples of situations where, if you want, you can go to substantial engineering effort in order to make a user interface a little bit nicer. (Each example based on a true story!)</p>

<ul>
<li>You have an e-commerce site, and need to send out order confirmation emails that explain next steps to the customer. Those next steps differ depending on availability, the tax status of the product, the location of the customer, the type of account they have, and a myriad other parameters. You want the emails to only include the information that is applicable to this particular customer&#8217;s situation, and not burden them with edge cases that don&#8217;t apply to them. You also want the emails to read as coherent prose, not as a bunch of fragmented bullet points generated by <code>if</code> statements based on the order parameters. So you go and build a natural language grammar model for constructing emails based on sentence snippets (providing pluralisation, agreement, declension in languages that have it, etc), in such a way that for any one out of 100 million possible parameter combinations, the resulting email is grammatically correct and easy to understand.</li>

<li>You have a multi-step user flow that is used in various different contexts, but ultimatively achieves the same thing in each context. (For example, <a href='http://rapportive.com/'>Rapportive</a> has several OAuth flows for connecting your account with various social networks, and there are several different buttons in different places that all lead into the same user flow.) The simple solution is to make the flow generic, and not care how the user got there. But if you want to make the user feel good, you need to imagine what state their mind was in when they entered the flow, and customise the images, text and structure of the flow in order to match their goal. This means you have to keep track of where the user came from, what they were trying to do, and thread that context through every step of the flow. This is not fundamentally hard, but it is fiddly, time-consuming and error-prone.</li>

<li>You have an application that requires some arcane configuration. You could take the stance that you will give the user a help page and they will have to figure it out from there. Or you could write a sophisticated auto-configuration tool that inspects the user&#8217;s environment, analyses thousands of possible software combinations and configurations (and updates this database as new versions of other products in the environment are released), and automatically chooses the correct settings &#8212; hopefully without having to ask the user for help. With auto-configuration, the users never even know that they were spared a confusing configuration dialog. But somehow, word gets around that the product &#8220;just works&#8221;.</li>
</ul>

<h2 id='whats_a_user_requirement'>What&#8217;s a user requirement?</h2>

<p>We said above that simplicity is good. However, taking simplicity to an exaggerated extreme, you end up with software that does nothing. This implies that there are aspects of software complexity that are <strong>essential</strong> to the user&#8217;s problem that is being solved. (Note that I don&#8217;t mean complexity of the user interface, but complexity of the actual code that implements the solution to the user&#8217;s problem.)</p>

<p>Unfortunately, there is a lot of additional complexity introduced by stuff that is not directly visible or useful to users: stuff that is only required to &#8220;grease the wheels&#8221;, for example to make legacy components work or to improve performance. Moseley and Marks call this latter type <strong>accidental</strong> complexity, and argue that it should be removed or abstracted away as much as possible. (Other authors define essential and accidental complexity slightly differently, but the exact definition is not important for the purpose of this post.)</p>

<p>This suggests that it is important to understand what <strong>user problem</strong> is being solved, and that&#8217;s where things start getting tricky. When you say that something is essential because it fulfils a <strong>user requirement</strong> (as opposed to an implementation constraint or a performance optimisation), that presupposes a very utilitarian view of software. It assumes that the user is trying to get a job done, and that they are a rational actor. But what if, say, you are taking an emotional approach and optimising for <strong>user delight</strong>?</p>

<p>What if the user didn&#8217;t know they had a problem, but you solve it anyway? If you introduce complexity in the system for the sake of making things a little nicer for the user (but without providing new core functionality), is that complexity really essential? What if you add a little detail that is surprising but delightful?</p>

<p>You can try to reduce an emotional decision down to a rational one &#8212; for example, you can say that when a user plays a game, it is solving the user&#8217;s problem of boredom by providing distraction. Thus any feature which substantially contributes towards alleviating boredom may be considered essential. Such reductionism can sometimes provide useful angles of insight, but I think a lot would be lost by ignoring the emotional angle.</p>

<p>You can state categorically that &#8220;great user experience is an essential feature&#8221;. But what does that mean? By itself, that statement is so general that could be used to argue for anything or nothing. User experience is subjective. What&#8217;s preferable for one user may be an annoyance for another user, even if both users are in the application&#8217;s target segment. Sometimes it just comes down to taste or fashion. User experience tends to have an emotional angle that makes it hard to fit into a rational reasoning framework.</p>

<p>What I am trying to get at: there are things in software that introduce a lot of complexity (and that we should consequently be wary of), and that can&#8217;t be directly mapped to a bullet point on a list of user requirements, but that are nevertheless important and valuable. These things do not necessarily provide important functionality, but they contribute to how the user <strong>feels</strong> about the application. Their effect may be invisible or subconscious, but that doesn&#8217;t make them any less essential.</p>

<h2 id='datadriven_vs_emotional_design'>Data-driven vs. emotional design</h2>

<p>Returning to the examples above: as an application developer, you can choose whether to take on substantial additional complexity in the software in order to simplify or improve the experience for the user. The increased software complexity actually <strong>reduces</strong> the complexity from the user&#8217;s point of view. These examples also illustrate how user experience concerns are not just a matter of graphic design, but can also have a big impact on how things are engineered.</p>

<p>The features described above arguably do not contribute to the utility of the software &#8212; in the e-commerce example, orders will be fulfilled whether or not the confirmation emails are grammatical. In that sense, the complexity is unnecessary. But I would argue that these kind of user experience improvements are just as important as the utility of the product, because they determine how users <strong>feel</strong> about it. And how they feel ultimately determines whether they come back, and thus the success or failure of the product.</p>

<p>One could even argue that the utility of a product is a subset of its user experience: if the software doesn&#8217;t do the job that it&#8217;s supposed to, then that&#8217;s one way of creating a pretty bad experience; however, there are also many other ways of creating a bad experience, while remaining fully functional from a utilitarian point of view.</p>

<p>The emotional side of user experience can be a difficult thing for organisations to grapple with, because it doesn&#8217;t easily map to metrics. You can measure things like how long a user stayed on your site, how many things they clicked on, conversion rates, funnels, repeat purchase rates, lifetime values&#8230; but those numbers tell you very little about how happy you made a user. So you can take a &#8220;data-driven&#8221; approach to design decisions and say that a feature is worthwhile if and only if it makes the metrics go up &#8212; but I fear that an important side of the story is missed if you go solely by the numbers.</p>

<h2 id='questions'>Questions</h2>

<p>This is as far as my thinking has got: believing that a great user experience is essential for many products; and recognising that building a great UX is hard, can require substantial additional complexity in engineering, and can be hard to justify in terms of logical arguments and metrics. Which leaves me with some unanswered questions:</p>

<ul>
<li>Every budget is finite, so you have to prioritise things, and not everything will get done. When you consider building something that improves user experience without strictly adding utility, it has to be traded off against features that do add utility (is it better to shave a day off the delivery time than to have a nice confirmation email?), and the cost of the increased complexity (will that clever email generator be a nightmare to localise when we translate the site into other languages?). How do you decide about that kind of trade-offs?</li>

<li>User experience choices are often emotional and <a href='http://martin.kleppmann.com/2010/10/30/intuition-has-no-transfer-encoding.html'>intuitive</a> (no number of focus groups and usability tests can replace good taste). That doesn&#8217;t make them any more or less important than rational arguments, but combining emotional and rational arguments can be tricky. Emotionally-driven people tend to let emotional choices overrule rational arguments, and rationally-driven people vice versa. How do you find the healthy middle ground?</li>

<li>If you&#8217;re aiming for a minimum viable product in order to test out a market (as opposed to improving a mature product), does that change how you prioritise core utility relative to &#8220;icing on the cake&#8221;?</li>
</ul>

<p>I suspect that the answers to the questions above are <em>&#8220;it depends&#8221;</em>. More precisely, <em>&#8220;how one thing is valued relative to another is an aspect of your particular organisation&#8217;s culture, and there&#8217;s no one right answer&#8221;</em>. That would imply that each of us should think about it; you should have your own personal answers for how you decide these things in your own projects, and be able to articulate them. But it&#8217;s difficult &#8212; I don&#8217;t think hard-and-fast rules have a chance of working here.</p>

<p>I&#8217;d love to hear your thoughts in the comments below. If you liked this post, you can <a href='http://eepurl.com/csJmf'>subscribe to email notifications</a> when I write something new :)</p>
                ]]></content:encoded>
            </item>
        
            <item>
                <title>Rethinking caching in web apps</title>
                <link>http://martin.kleppmann.com/2012/10/01/rethinking-caching-in-web-apps.html</link>
                <comments>http://martin.kleppmann.com/2012/10/01/rethinking-caching-in-web-apps.html#disqus_thread</comments>
                <pubDate>Mon, 01 Oct 2012 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2012/10/01/rethinking-caching-in-web-apps.html</guid>
                
                <description><![CDATA[ Having spent a lot of the last few years worrying about the scalability of data-heavy applications like Rapportive, I have started to get the feeling that maybe we have all been &#8220;doing it wrong&#8221;. Maybe what we consider to be &#8220;state of the art&#8221; application architecture is actually holding us... ]]></description>
                <content:encoded><![CDATA[
                    <p>Having spent a lot of the last few years worrying about the scalability of data-heavy applications like <a href='http://rapportive.com/'>Rapportive</a>, I have started to get the feeling that maybe we have all been &#8220;doing it wrong&#8221;. Maybe what we consider to be &#8220;state of the art&#8221; application architecture is actually holding us back.</p>

<p>I don&#8217;t have a definitive answer for how we should be architecting things differently, but in this post I&#8217;d like to outline a few ideas that I have been fascinated by recently. My hope is that we can develop ways of better managing scale (in terms of complexity, volume of data and volume of traffic) while keeping our applications nimble, easy and safe to modify, test and iterate.</p>

<p>My biggest problem with web application architecture is how <strong>network communication concerns</strong> are often intermingled with <strong>business logic concerns</strong>. This makes it hard to rearrange the logic into new architectures, such as the precomputed cache architecture described below. In this post I explore why it important to be able to try new architectures for things like caching, and what it would take to achieve that flexibility.</p>

<h2 id='an_example'>An example</h2>

<p>To illustrate, consider the clichÃ©d Rails blogging engine example:</p>
<div class='highlight'><pre><code class='ruby'><span class='k'>class</span> <span class='nc'>Post</span> <span class='o'>&lt;</span> <span class='ss'>ActiveRecord</span><span class='p'>:</span><span class='ss'>:Base</span>
  <span class='n'>attr_accessible</span> <span class='ss'>:title</span><span class='p'>,</span> <span class='ss'>:content</span><span class='p'>,</span> <span class='ss'>:author</span>
  <span class='n'>has_many</span> <span class='ss'>:comments</span>
<span class='k'>end</span>

<span class='k'>class</span> <span class='nc'>Comment</span> <span class='o'>&lt;</span> <span class='ss'>ActiveRecord</span><span class='p'>:</span><span class='ss'>:Base</span>
  <span class='n'>attr_accessible</span> <span class='ss'>:content</span><span class='p'>,</span> <span class='ss'>:author</span>
  <span class='n'>belongs_to</span> <span class='ss'>:post</span>
<span class='k'>end</span>

<span class='k'>class</span> <span class='nc'>PostsController</span> <span class='o'>&lt;</span> <span class='no'>ApplicationController</span>
  <span class='k'>def</span> <span class='nf'>show</span>
    <span class='vi'>@post</span> <span class='o'>=</span> <span class='no'>Post</span><span class='o'>.</span><span class='n'>find</span><span class='p'>(</span><span class='n'>params</span><span class='o'>[</span><span class='ss'>:id</span><span class='o'>]</span><span class='p'>)</span>
    <span class='n'>respond_to</span> <span class='k'>do</span> <span class='o'>|</span><span class='nb'>format</span><span class='o'>|</span>
      <span class='nb'>format</span><span class='o'>.</span><span class='n'>html</span>  <span class='c1'># show.html.erb</span>
      <span class='nb'>format</span><span class='o'>.</span><span class='n'>json</span>  <span class='p'>{</span> <span class='n'>render</span> <span class='ss'>:json</span> <span class='o'>=&gt;</span> <span class='vi'>@post</span> <span class='p'>}</span>
    <span class='k'>end</span>
  <span class='k'>end</span>
<span class='k'>end</span>

<span class='c1'># posts/show.html.erb:</span>
</code></pre></div>
<p><div class='highlight'><pre><code class='html+erb'><span class='nt'>&lt;h1&gt;</span><span class='cp'>&lt;%=</span> <span class='vi'>@post</span><span class='o'>.</span><span class='n'>title</span> <span class='cp'>%&gt;</span><span class='nt'>&lt;/h1&gt;</span>
<span class='nt'>&lt;p</span> <span class='na'>class=</span><span class='s'>&quot;author&quot;</span><span class='nt'>&gt;</span>By <span class='cp'>&lt;%=</span> <span class='vi'>@post</span><span class='o'>.</span><span class='n'>author</span> <span class='cp'>%&gt;</span><span class='nt'>&lt;/p&gt;</span>
<span class='nt'>&lt;div</span> <span class='na'>class=</span><span class='s'>&quot;content&quot;</span><span class='nt'>&gt;</span>
  <span class='cp'>&lt;%=</span> <span class='n'>simple_format</span><span class='p'>(</span><span class='vi'>@post</span><span class='o'>.</span><span class='n'>content</span><span class='p'>)</span> <span class='cp'>%&gt;</span>
<span class='nt'>&lt;/div&gt;</span>
<span class='nt'>&lt;h2&gt;</span>Comments<span class='nt'>&lt;/h2&gt;</span>
<span class='nt'>&lt;ul</span> <span class='na'>class=</span><span class='s'>&quot;comments&quot;</span><span class='nt'>&gt;</span>
  <span class='cp'>&lt;%</span> <span class='vi'>@post</span><span class='o'>.</span><span class='n'>comments</span><span class='o'>.</span><span class='n'>each</span> <span class='k'>do</span> <span class='o'>|</span><span class='n'>comment</span><span class='o'>|</span> <span class='cp'>%&gt;</span>
    <span class='nt'>&lt;li&gt;</span>
      <span class='nt'>&lt;blockquote&gt;</span><span class='cp'>&lt;%=</span> <span class='n'>simple_format</span><span class='p'>(</span><span class='n'>comment</span><span class='o'>.</span><span class='n'>content</span><span class='p'>)</span> <span class='cp'>%&gt;</span><span class='nt'>&lt;/blockquote&gt;</span>
      <span class='nt'>&lt;p</span> <span class='na'>class=</span><span class='s'>&quot;author&quot;</span><span class='nt'>&gt;</span><span class='cp'>&lt;%=</span> <span class='n'>comment</span><span class='o'>.</span><span class='n'>author</span> <span class='cp'>%&gt;</span><span class='nt'>&lt;/p&gt;</span>
    <span class='nt'>&lt;/li&gt;</span>
  <span class='cp'>&lt;%</span> <span class='k'>end</span> <span class='cp'>%&gt;</span>
<span class='nt'>&lt;/ul&gt;</span>
</code></pre></div></p>

<p>Pretty good code by various standards, but it has always irked me a bit that I can&#8217;t see where the network communication (i.e. making database queries) is happening. When I look at that <code>Post.find</code> in the controller, I can guess that probabably translates into a <code>SELECT * FROM posts WHERE id = ?</code> internally &#8211; unless the same query was already made recently, and ActiveRecord cached the result. And another database query of the form <code>SELECT * FROM comments WHERE post_id = ?</code> might be made as a result of the <code>@post.comments</code> call in the template. Or maybe the comments were already previously loaded by some model logic, and then cached? Or someone decided to eagerly load comments with the original post? Who knows.</p>

<p>The execution flow for a MVC framework request like <code>PostsController#show</code> probably looks something like this:</p>
<p><a href='http://martin.kleppmann.com/2012/10/architecture-high-01.png'><img alt='Typical MVC request flow' height='119' src='http://martin.kleppmann.com/2012/10/architecture-01.png' width='550' /></a></p>
<p>Of course it is deliberately designed that way. Your template and your controller shouldn&#8217;t have to worry about database queries &#8212; those are encapsulated by the model for many good reasons. I am violating abstraction by even thinking about the database whilst I&#8217;m in the template code! I should just think of my models as pure, beautiful pieces of application state. How that state gets loaded from a database is a matter that only the models need to worry about.</p>

<h2 id='adding_complexity'>Adding complexity</h2>

<p>In the example above, the amount of logic in the model is minimal, but it typically doesn&#8217;t stay that way for long. As the application becomes popular (say, the blogging engine morphs to become Twitter, Tumblr, Reddit or Pinterest), all sorts of stuff gets added: memcache to stop the database from falling over, spam filtering, analytics features, email sending, notifications, A/B testing, more memcache, premium features, ads, upsells for viral loops, more analytics, even more memcache. As the application inevitably grows in complexity, the big monolithic beast is split into several smaller services, and different services end up being maintained by different teams.</p>

<p>As all of this is happening, the programming model typically stays the same: each service in the architecture (which may be a user-facing web server, or an internal service e.g. for user authentication) communicates over the network with a bunch of other nodes (memcached instances, database servers, other application services), processes and combines the data in some way, and then serves it out to a client.</p>

<p>That processing and combining of data we can abstractly call &#8220;business logic&#8221;. It might be trivially simple, or it might involve half a million lines of parsing, rendering or machine learning code. It might behave differently depending on which A/B test bucket the user is in. It might deal with hundreds of hairy edge cases. Whatever.</p>

<p>At the root of the matter, business logic should be a <a href='http://en.wikipedia.org/wiki/Pure_function'>pure function</a>. It takes a bunch of inputs (request parameters from the client, data stored in various databases and caches, responses from various other services) and produces a bunch of outputs (data to return to the client, data to write back to various databases and caches). It is usually deterministic: given the same inputs, the business logic should produce exactly the same output again. It is also stateless: any data that is required to produce the output or to make a decision has to be provided as an input.</p>

<p>By contrast, the network communication logic is all about &#8216;wiring&#8217;. It may end up having a lot of complexity in its own right: sending requests to the right node of a sharded database, retrying failed requests with exponential back-off, making requests to different services in parallel, cross-datacenter failover, service authentication, etc. But the network communication logic ought to be general-purpose and completely independent of your application&#8217;s business logic.</p>

<p>Both business logic and network communication logic are needed to build a service. But how do you combine the two into a single process? Most commonly, we build abstractions for each type of logic, hiding the gory implementation details. Much like in the blog example above, you end up calling a method somewhere inside the business logic, not really knowing or caring whether it will immediately return a value that the object has already computed, or whether it will talk to another process on the same machine, or load the value from some remote cache, or make a query on a database cluster somewhere.</p>

<p>It&#8217;s good that the business logic doesn&#8217;t need to worry about how and when the communication happens. And it&#8217;s good that the communication logic is general-purpose and not polluted with application-specific concerns. But I think it&#8217;s problematic that network communication may happen somewhere deeply inside a business logic call stack. Let me try to explain why.</p>

<h2 id='precomputed_caches'>Precomputed caches</h2>

<p>As your volume of data and your number of users grow, database access often becomes a bottleneck (there are more queries competing for I/O, and each query takes longer when there&#8217;s more data). The standard answer to the problem is of course caching. You can cache at many different levels: an individual database row, or a model object generated by combining several sources, or even an entire HTML page ready to serve to a client. I will focus on the mid-to-high-level caches, where the raw data has gone through some sort of business logic before it ends up in the cache.</p>

<p>Most commonly, caches are set up in read-through style: on every query, you first check the cache, and return the value from the cache if it&#8217;s a hit; otherwise it&#8217;s a miss, so you do whatever is required to generate the value (query databases, apply business logic, perform voodoo), and return it to the client whilst also storing it in the cache for next time. As long as you can generate the value on the fly in a reasonable time, this works pretty well.</p>

<p>I will gloss over cache invalidation and expiry for now, and return to it below.</p>

<p>The most apparent problem with a read-through cache is that the first time a value is requested, it&#8217;s always slow. (And if your cache is too small to hold the entire dataset, rarely accessed values will get evicted and thus be slow every time.) That may or may not be a problem for you. One reason why it may be a problem is that on many sites, the first client to request a given page is typically the Googlebot, and Google <a href='http://www.mattcutts.com/blog/site-speed/'>penalises</a> slow sites in rankings. So if you have the kind of site where Google juice is lifeblood, then your SEO guys may tell you that a read-through cache is not good enough.</p>

<p>So, can you make sure that the data is in the cache even before it is requested for the first time? Well, if your dataset isn&#8217;t too huge, you can actually <strong>precompute every possible cache entry</strong>, put them in a big distributed key-value store and serve them with minimal latency. That has a great advantage: cache misses no longer exist. If you&#8217;ve precomputed every possible cache entry, and a key isn&#8217;t in the cache, you can be sure that there&#8217;s no data for that key.</p>

<p>If that sounds crazy to you, consider these points:</p>

<ul>
<li>A database index is a special case of a precomputed cache. For every value you might want to search for, the index tells you where to find occurrences of that value. If it&#8217;s not in the index, it&#8217;s not in the database. The initial index creation is a one-off batch job, and thereafter the database automatically keeps it in sync with the raw data. Yes, databases have been doing this for a long time.</li>

<li>With Hadoop you can process terabytes of data without breaking a sweat. That is truly awesome power.</li>

<li>There are several datastores that allow you to precompute their files in Hadoop, which makes them very well suited for serving the cache that you precomputed. We are currently using <a href='http://www.project-voldemort.com/voldemort/'>Voldemort</a> in read-only mode (<a href='http://static.usenix.org/events/fast12/tech/full_papers/Sumbaly.pdf'>research paper</a>), but <a href='http://hbase.apache.org/book/arch.bulk.load.html'>HBase</a> and <a href='https://github.com/nathanmarz/elephantdb'>ElephantDB</a> can do this too.</li>

<li>If you&#8217;re currently storing data in denormalized form (to avoid joins on read queries), you can stop doing that. You can keep your primary database in a very clean, normalized schema, and any caches you derive from it can denormalize the data to your heart&#8217;s content. This gives you the best of both worlds.</li>
</ul>

<h2 id='separating_communication_from_business_logic'>Separating communication from business logic</h2>

<p>Ok, say you&#8217;ve decided that you want to precompute a cache in Hadoop. As we&#8217;ve not yet addressed cache invalidation (see below), let&#8217;s just say you&#8217;re going to rebuild the entire cache once a day. That means the data you serve out of the cache will be stale, out of date by up to a day, but that&#8217;s still acceptable for some applications.</p>

<p>The first step is to get your raw data into HDFS. That&#8217;s not hard, assuming you have daily database backups: you can take your existing backup, transform it into a more MapReduce-friendly format such as <a href='http://avro.apache.org/'>Avro</a>, and write it straight to HDFS. Do that with all your production databases and you&#8217;ve got a fantastic resource to work with in Hadoop.</p>

<p>Now, to build your precomputed cache, you need to apply the same business logic to the same data as you would in an uncached service that does it on the fly. As described above, your business logic takes as input the request parameters from the user and any data that is loaded from databases or services in order to serve that request. If you have all that data in HDFS, and you can work out all possible request parameters, then in theory, you should be able to take your existing business logic implementation and run it in Hadoop.</p>

<p>Business logic can be very complex, so you should probably aim to reuse the existing implementation rather than rewriting it. But doing so requires untangling the real business logic from all the network communication logic.</p>

<p>When your business logic is running as a service processing individual requests, you&#8217;re used to making several small requests to databases, caches or other services as part of generating a response (see the blog example above). Those small requests constitute gathering all the inputs needed by the business logic in order to produce its output (e.g. a rendered HTML page).</p>

<p>But when you&#8217;re running in Hadoop, this is all turned on its head. You don&#8217;t want to be making individual random-access requests to data, because that would be an order of magnitude too slow. Instead you need to use MapReduce to gather all the inputs for one particular evaluation of the business logic into one place, and then run the business logic given those inputs without any network communication. Rather than the business logic <em>pulling</em> together all the bits of data it needs in order to produce a response, the MapReduce job has already gathered all the data it knows the business logic is going to need, and <em>pushes</em> it into the business logic function.</p>

<p>Let&#8217;s use the blog example to make this more concrete. The data dependency is fairly simple: when the blog post <code>params[:id]</code> is requested, we require the row in the <code>posts</code> table whose <code>id</code> column matches the requested post, and we require all the rows in the <code>comments</code> table whose <code>post_id</code> column matches the requested post. If the <code>posts</code> and <code>comments</code> tables are in HDFS, it&#8217;s a very simple MapReduce job to group together the post with <code>id = x</code> and all the comments with <code>post_id = x</code>.</p>

<p>We can then use a stub database implementation to feed those database rows into the existing <code>Post</code> and <code>Comment</code> model objects. That way we can make the models think that they loaded the data from a database, even though actually we had already gathered all the data we knew it was going to need. The model objects can keep doing their job as normally, and the output they produce can be written straight to the cache.</p>

<p>By this point, two problems should be painfully clear:</p>

<ul>
<li>How does the MapReduce job know what inputs the business logic is going to need in order to work?</li>

<li>OMG, implementing stub database drivers, isn&#8217;t that a bit too much pain for limited gain? (Note that in testing frameworks it&#8217;s not unusual to stub out your database, so that you can run your unit tests without a real database. Still, it&#8217;s non-trivial and annoying.)</li>
</ul>

<p>Both problems have the same cause, namely that the network communication logic is triggered from deep inside the business logic.</p>

<h2 id='data_dependencies'>Data dependencies</h2>

<p>When you look at the business logic in the light of precomputing a cache, it seems like the following pattern would make more sense:</p>

<ol>
<li>Declare your data dependencies: &#8220;if you want me to render the blog post with ID <code>x</code>, I&#8217;m going to need the row in the <code>posts</code> table with <code>id = x</code>, and also all the rows in the <code>comments</code> table with <code>post_id = x</code>&#8221;.</li>

<li>Let the communication logic deal with resolving those dependencies. If you&#8217;re running as a normal web app, that means making database (or memcache) queries to one or more databases, and maybe talking to other services. If you&#8217;re running in Hadoop, it means configuring the MapReduce job to group together all the pieces of data on which the business logic depends.</li>

<li>Once all the dependencies have been loaded, the business logic is now a pure function, deterministic and side-effect-free, that produces our desired output. It can perform whatever complicated computation it needs to, but it&#8217;s not allowed access to the network or data stores that weren&#8217;t declared as dependencies up front.</li>
</ol>

<p>This separation would make application architecture very different from the way it is commonly done today. I think this new style would have several big advantages:</p>

<ul>
<li>By removing the assumption that the business logic is handling one request at a time, it becomes much easier to run the business logic in completely different contexts, such as in a batch job to precompute a cache. (No more stubbing out database drivers.)</li>

<li>Testing becomes much easier. All the tricky business logic for which you want to write unit tests is now just a function with a bunch of inputs and a bunch of outputs. You can easily vary what you put in, and easily check that the right thing comes out. Again, no more stubbing out the database.</li>

<li>The network communication logic can become a lot more helpful. For example, it can make several queries in parallel without burdening the business logic with a lot of complicated concurrency stuff, and it can deduplicate similar requests.</li>

<li>Because the data dependencies are very clearly and explicitly modelled, the system becomes easier to understand, and it becomes easier to move modules around, split a big monolithic beast into smaller services, or combine smaller services into bigger, logical units.</li>
</ul>

<p>I hope you agree that this is a very exciting prospect. But is it practical?</p>

<p>In most cases, I think it would not be very hard to make business logic pure (i.e. stop making database queries from deep within) &#8212; it&#8217;s mostly a matter of refactoring. I have done it to substantial chunks of the Rapportive code base, and it was a bit tedious but perfectly doable. And the network communication logic wouldn&#8217;t have to change much at all.</p>

<p>The problem of making this architecture practical hinges on having a good mechanism for declaring data dependencies. The idea is not new &#8212; for instance, LinkedIn have an internal framework for resolving data dependencies that queries several services in parallel &#8212; but I&#8217;ve not yet seen a language or framework that really gets to the heart of the problem.</p>

<p>Adapting the blog example above, this is what I imagine such an architecture would look like:</p>
<p><a href='http://martin.kleppmann.com/2012/10/architecture-high-02.png'><img alt='Concept for using a dependency resolver' height='119' src='http://martin.kleppmann.com/2012/10/architecture-02.png' width='550' /></a></p>
<p>We still have models, and they are still used as encapsulations of state, but they are no longer wrappers around a database connection. Instead, the dependency resolver can take care of the messy business of talking to the database; the models are pure and can focus on the business logic. The models don&#8217;t care whether they are instantiated in a web app or in a Hadoop cluster, and they don&#8217;t care whether the data was loaded from a SQL database or from HDFS. That&#8217;s the way it should be.</p>

<p>In my spare time I have started working on a language called <strong>Flowquery</strong> (don&#8217;t bother searching, there&#8217;s nothing online yet) to solve the problem of declaring data dependencies. If I can figure it out, it should make precomputed caches and all the good things above very easy. But it&#8217;s not there yet, so I don&#8217;t want to oversell it.</p>

<p>But wait, there is one more thing&#8230;</p>

<h2 id='cache_invalidation'>Cache invalidation</h2>

<blockquote>
<p>There are only two hard things in Computer Science: cache invalidation and naming things. &#8212; <a href='http://martinfowler.com/bliki/TwoHardThings.html'>Phil Karlton</a></p>
</blockquote>

<p>How important is it that the data in your cache is up-to-date and consistent with your &#8220;source of truth&#8221; database? The answer depends on the application and the circumstances. For example, if the user edits their own data, you almost certainly want to show them an up-to-date version of their own data post-editing, otherwise they will assume that your app is broken. But you might be able to get away with showing stale data to other users for a while. For data that is not directly edited by users, stale data may always be ok.</p>

<p>If staleness is acceptable, caching is fairly simple: on a read-through cache you set an expiry time on a cache key, and when that time is reached, the entry falls out of the cache. On a precomputed cache you do nothing, and just wait until the next time you recompute the entire thing.</p>

<p>In cases where greater consistency is required, you have to explicitly invalidate cache entries when the original data changes. If just one cache key is affected by a change, you can write-through to that cache key when the &#8220;source of truth&#8221; database is updated. If many keys may be affected, you can use <a href='http://37signals.com/svn/posts/3113-how-key-based-cache-expiration-works'>generational caching</a> and <a href='https://groups.google.com/forum/#!msg/memcached/OiScvRbGaU8/C1vny7DiGakJ'>clever generalisations thereof</a>. Whatever technique you use, it usually ends up being a lot of manually written, fiddly and error-prone code. Not a great joy to work with, hence the terribly clichÃ©d quote above.</p>

<p>But&#8230; observe the following: in our efforts to separate pure business logic from network communication logic, we decided that we needed to explicitly model the data dependencies, and only data sources declared there are permitted as inputs to the business logic. In other words, the data dependency framework knows exactly which pieces of data are required in order to generate a particular piece of output &#8212; and conversely, when a piece of (input) data changes, it can know exactly which outputs (cache entries) may be affected by the change!</p>

<p>This means that if we have a real-time feed of changes to the underlying databases, we can feed it into a stream processing framework like <a href='http://storm-project.net/'>Storm</a>, run the data dependency analysis in reverse on every change, recompute the business logic for each output affected by the change in input, and write the results to another datastore. This store sits alongside the precomputed cache we generated in a batch process in Hadoop. When you want to query the cache, check both the output of the batch process and the output of the stream process. If the stream process has generated more recent data, use that, otherwise use the batch process output.</p>

<p>If you&#8217;ve been following recent news in Big Data, you may recognise this as an application of Nathan Marz&#8217; <a href='http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html'>lambda architecture</a> (described in detail in his <a href='http://www.manning.com/marz/'>upcoming book</a>). I cannot thank Nathan enough for his amazing work in this area.</p>

<p>In this architecture, you get the benefits of a precomputed cache (every request is fast, including the first one), it keeps itself up-to-date with the underlying data, and because you have already declared your data dependencies, you don&#8217;t need to manually write cache invalidation code! The same dependency declaration can be used in three different ways:</p>

<ol>
<li>In &#8216;online&#8217; mode in a service or web app, for driving the network communication logic in order to make all the required queries and requests in order to serve an incoming request, and to help with read-through caching.</li>

<li>In &#8216;offline&#8217; mode in Hadoop, to configure a MapReduce pipeline that brings together all the required data in order to run it through the business logic and generate a precomputed cache of all possible queries.</li>

<li>In &#8216;nearline&#8217; mode in Storm, to configure a stream processing topology that tracks changes to the underlying data, determines which cache keys need to be invalidated, and recomputes the cache values for those keys using the business logic.</li>
</ol>

<p>I am designing Flowquery so that it can be used in all three modes &#8212; you should be able to write your data dependencies just once, and let the framework take care of bringing all the necessary data together so that the business logic can act on it.</p>

<p>My hope is to make caching and cache invalidation as simple as database indexes. You declare an index once, the database runs a one-off batch job to build the index, and thereafter automatically keeps it up-to-date as the table contents change. It&#8217;s so simple to use that we don&#8217;t even think about it, and that&#8217;s what we should be aiming for in the realm of caching.</p>

<p>The project is still at a very early stage, but hopefully I&#8217;ll be posting more about it as it progresses. If you&#8217;d like to hear more, please <a href='http://eepurl.com/csJmf'>leave your email address</a> and I&#8217;ll send you a brief note when I post more. Or you can follow me on <a href='https://twitter.com/martinkl'>Twitter</a> or <a href='https://alpha.app.net/martinkl'>App.net</a>.</p>

<p><em>Thanks to Nathan Marz, Pete Warden, Conrad Irwin, Rahul Vohra and Sam Stokes for feedback on drafts of this post.</em></p>
                ]]></content:encoded>
            </item>
        
            <item>
                <title>Java's hashCode is not safe for distributed systems</title>
                <link>http://martin.kleppmann.com/2012/06/18/java-hashcode-unsafe-for-distributed-systems.html</link>
                <comments>http://martin.kleppmann.com/2012/06/18/java-hashcode-unsafe-for-distributed-systems.html#disqus_thread</comments>
                <pubDate>Mon, 18 Jun 2012 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2012/06/18/java-hashcode-unsafe-for-distributed-systems.html</guid>
                
                <description><![CDATA[ As you probably know, hash functions serve many different purposes: Network and storage systems use them (in the guise of checksums) to detect accidental corruption of data. Crypographic systems use them to detect malicious corruption of data and to implement signatures. Password authentication systems use them to make it harder... ]]></description>
                <content:encoded><![CDATA[
                    <p>As you probably know, hash functions serve many different purposes:</p>

<ol>
<li>Network and storage systems use them (in the guise of checksums) to detect accidental corruption of data.</li>

<li>Crypographic systems use them to detect malicious corruption of data and to implement signatures.</li>

<li>Password authentication systems use them to make it harder to extract plaintext passwords from a database.</li>

<li>Programming languages use them for hash maps, to determine in which hash bucket a key is placed.</li>

<li>Distributed systems use them to determine which worker in a cluster should handle a part of a large job.</li>
</ol>

<p>All those purposes have different requirements, and different hash functions exist for the various purposes. For example, <a href='http://en.wikipedia.org/wiki/Cyclic_redundancy_check'>CRC32</a> is fine for detecting bit corruption in Ethernet, as it&#8217;s really fast and easy to implement in hardware, but it&#8217;s useless for cryptographic purposes. <a href='http://tools.ietf.org/html/rfc3174'>SHA-1</a> is fine for protecting the integrity of a message against attackers, as it&#8217;s cryptographically secure and also reasonably fast to compute; but if you&#8217;re storing passwords, you&#8217;re probably better off with something like <a href='http://codahale.com/how-to-safely-store-a-password/'>bcrypt</a>, which is <em>deliberately</em> slow in order to make brute-force attacks harder.</p>

<p>Anyway, that&#8217;s all old news. Today I want to talk about points 4 and 5, and why they are also very different from each other.</p>

<p><strong>Hashes for hash tables</strong></p>

<p>We use hash tables (dictionaries) in programming languages all the time without thinking twice. When you insert an item into a hash table, the language computes a hash code (an integer) for the key, uses that number to choose a bucket in the hash table (typically <code>mod n</code> for a table of size <code>n</code>), and then puts the key and value in that bucket in the table. If there&#8217;s already a value there (a hash collision), a linked list typically takes care of storing the keys and values within the same hash bucket. In Ruby, for example:</p>
<pre><span class='ansi1 ansi31'>$</span> ruby --version
ruby 1.8.7 (2011-06-30 patchlevel 352) [i686-darwin11.0.0]

<span class='ansi1 ansi31'>$</span> pry
[1] pry(main)&gt; hash_table = {<span class='ansi1 ansi32'>'</span><span class='ansi32'>answer</span><span class='ansi1 ansi32'>'</span> =&gt; <span class='ansi1 ansi34'>42</span>}
=&gt; {<span class='ansi1 ansi32'>&quot;</span><span class='ansi32'>answer</span><span class='ansi1 ansi32'>&quot;</span>=&gt;<span class='ansi1 ansi34'>42</span>}
[2] pry(main)&gt; <span class='ansi1 ansi32'>'</span><span class='ansi32'>answer</span><span class='ansi1 ansi32'>'</span>.hash
=&gt; <span class='ansi1 ansi34'>-1246806696</span>
[3] pry(main)&gt; <span class='ansi1 ansi32'>'</span><span class='ansi32'>answer</span><span class='ansi1 ansi32'>'</span>.hash
=&gt; <span class='ansi1 ansi34'>-1246806696</span>
[4] pry(main)&gt; ^D

<span class='ansi1 ansi31'>$</span> pry
[1] pry(main)&gt; <span class='ansi1 ansi32'>'</span><span class='ansi32'>answer</span><span class='ansi1 ansi32'>'</span>.hash
=&gt; <span class='ansi1 ansi34'>-1246806696</span>
[2] pry(main)&gt; <span class='ansi1 ansi32'>&quot;</span><span class='ansi32'>don't panic</span><span class='ansi1 ansi32'>&quot;</span>.hash
=&gt; <span class='ansi1 ansi34'>-464783873</span>
[3] pry(main)&gt; ^D
</pre>
<p>When you add the key <code>&#39;answer&#39;</code> to the hash table, Ruby internally calls the <code>#hash</code> method on that string object. The method returns an arbitrary number, and as you see above, the number is always the same for the same string. A different string usually has a different hash code. Occasionally you might get two keys with the same hash code, but it&#8217;s extremely unlikely that you get a large number of collisions in normal operation.</p>

<p>The problem with the example above: when I quit Ruby (<code>^D</code>) and start it again, and compute the hash for the same string, I still get the same result. <em>But why is that a problem,</em> you say, <em>isn&#8217;t that what a hash function is supposed to do?</em> &#8211; Well, the problem is that I can now put on my evil genius hat, and generate a list of strings that all have the same hash code:</p>
<pre><span class='ansi1 ansi31'>$</span> pry
[1] pry(main)&gt; <span class='ansi1 ansi32'>&quot;</span><span class='ansi32'>a</span><span class='ansi1 ansi32'>&quot;</span>.hash
=&gt; <span class='ansi1 ansi34'>100</span>
[2] pry(main)&gt; <span class='ansi1 ansi32'>&quot;\0</span><span class='ansi32'>a</span><span class='ansi1 ansi32'>&quot;</span>.hash
=&gt; <span class='ansi1 ansi34'>100</span>
[3] pry(main)&gt; <span class='ansi1 ansi32'>&quot;\0\0</span><span class='ansi32'>a</span><span class='ansi1 ansi32'>&quot;</span>.hash
=&gt; <span class='ansi1 ansi34'>100</span>
[4] pry(main)&gt; <span class='ansi1 ansi32'>&quot;\0\0\0</span><span class='ansi32'>a</span><span class='ansi1 ansi32'>&quot;</span>.hash
=&gt; <span class='ansi1 ansi34'>100</span>
[5] pry(main)&gt; <span class='ansi1 ansi32'>&quot;\0\0\0\0</span><span class='ansi32'>a</span><span class='ansi1 ansi32'>&quot;</span>.hash
=&gt; <span class='ansi1 ansi34'>100</span>
[6] pry(main)&gt; <span class='ansi1 ansi32'>&quot;\0\0\0\0\0</span><span class='ansi32'>a</span><span class='ansi1 ansi32'>&quot;</span>.hash
=&gt; <span class='ansi1 ansi34'>100</span>
</pre>
<p>Any server in the world running the same version of Ruby will get the same hash values. This means that I can send a specially crafted web request to your server, in which the request parameters contain lots of those strings with the same hash code. Your web framework will probably parse the parameters into a hash table, and they will all end up in the same hash bucket, no matter how big you make the hash table. Whenever you want to access the parameters, you now have to iterate over a long list of hash collisions, and your swift O(1) hash table lookup is suddenly a crawling slow O(n).</p>

<p>I just need to make a small number of these evil requests to your server and I&#8217;ve brought it to its knees. This type of denial of service attack was already <a href='http://www.cs.rice.edu/~scrosby/hash/CrosbyWallach_UsenixSec2003.pdf'>described</a> back in 2003, but it only became widely known last year, when Java, Ruby, Python, PHP and Node.js all suddenly <a href='http://www.ocert.org/advisories/ocert-2011-003.html'>scrambled</a> to fix the issue.</p>

<p>The solution is for the hash code to be consistent within one process, but to be different for different processes. For example, here is a more recent version in Ruby, in which the flaw is fixed:</p>
<pre><span class='ansi1 ansi31'>$</span> ruby --version
ruby 1.9.3p125 (2012-02-16 revision 34643) [x86_64-darwin11.3.0]

<span class='ansi1 ansi31'>$</span> pry
[1] pry(main)&gt; <span class='ansi1 ansi32'>'</span><span class='ansi32'>answer</span><span class='ansi1 ansi32'>'</span>.hash
=&gt; <span class='ansi1 ansi34'>968518855724416885</span>
[2] pry(main)&gt; <span class='ansi1 ansi32'>'</span><span class='ansi32'>answer</span><span class='ansi1 ansi32'>'</span>.hash
=&gt; <span class='ansi1 ansi34'>968518855724416885</span>
[3] pry(main)&gt; ^D

<span class='ansi1 ansi31'>$</span> pry
[1] pry(main)&gt; <span class='ansi1 ansi32'>'</span><span class='ansi32'>answer</span><span class='ansi1 ansi32'>'</span>.hash
=&gt; <span class='ansi1 ansi34'>-150894376904371785</span>
[2] pry(main)&gt; ^D
</pre>
<p>When I quit Ruby and start it again, and ask for the hash code of the same string, I get a completely different answer. This is obviously not what you want for cryptographic hashes or checksums, since it would render them useless &#8212; but for hash tables, it&#8217;s exactly right.</p>

<p><strong>Hashes for distributed systems</strong></p>

<p>Now let&#8217;s talk about distributed systems &#8212; systems in which you have more than process, probably on more than one machine, and they are talking to each other. If you have something that&#8217;s too big to fit on one machine (too much data to fit on one machine&#8217;s disks, too many requests to be handled by one machine&#8217;s CPUs, etc), you need to spread it across multiple machines.</p>

<p>How do you know which machine to use for a given request? Unless you have some application-specific partitioning that makes more sense, a hash function is a simple and effective solution: hash the name of the thing you&#8217;re requesting, mod number of servers, and that&#8217;s your server number. (Though if you ever want to change the number of machines, <a href='http://michaelnielsen.org/blog/consistent-hashing/'>consistent hashing</a> is probably a better bet.)</p>

<p>For this setup you obviously don&#8217;t want a hash function in which different processes may compute different hash codes for the same value, because you&#8217;d end up routing requests to the wrong server. You can&#8217;t use the same hash function as the programming language uses for hash tables.</p>

<p>Unfortunately, this is <a href='http://squarecog.wordpress.com/2011/02/20/hadoop-requires-stable-hashcode-implementations/'>exactly</a> what Hadoop does. <a href='http://storm-project.net/'>Storm</a>, a stream processing framework, <a href='https://github.com/nathanmarz/storm/blob/33a2ea5/src/clj/backtype/storm/tuple.clj#L7-8'>does too</a>. Both use the Java Virtual Machine&#8217;s <a href='http://docs.oracle.com/javase/7/docs/api/java/lang/Object.html#hashCode()'>Object.hashCode()</a> method.</p>

<p>I understand the use of <code>hashCode()</code> &#8212; it&#8217;s very tempting. On strings, numbers and collection classes, <code>hashCode()</code> always returns a consistent value, apparently even across different JVM vendors. It&#8217;s like that despite the <a href='http://docs.oracle.com/javase/7/docs/api/java/lang/Object.html#hashCode()'>documentation</a> for <code>hashCode()</code> explicitly <em>not</em> guaranteeing consistency across different processes:</p>

<blockquote>
<p>Whenever it is invoked on the same object more than once during an execution of a Java application, the hashCode method must consistently return the same integer, provided no information used in equals comparisons on the object is modified. <em>This integer need not remain consistent from one execution of an application to another execution of the same application.</em></p>
</blockquote>

<p>And once in a while, a bold library comes along that actually returns different <code>hashCode()</code> values in different processes &#8211; <a href='http://code.google.com/p/protobuf/'>Protocol Buffers</a>, for example &#8211; and <a href='https://groups.google.com/forum/?fromgroups#!topic/protobuf/MCk1moyWgIk'>people get quite confused</a>.</p>

<p>The problem is that although the documentation says <code>hashCode()</code> doesn&#8217;t provide a consistency guarantee, the Java standard library behaves as if it <em>did</em> provide the guarantee. People start relying on it, and since backwards-compatibility is rated so highly in the Java community, it will probably never ever be changed, even though the documentation would allow it to be changed. So the JVM gets the worst of both worlds: a hash table implementation that is open to DoS attacks, but also a hash function that can&#8217;t always safely be used for communication between processes. :(</p>

<p><strong>Therefore&#8230;</strong></p>

<p>So what I&#8217;d like to ask for is this: if you&#8217;re building a distributed framework based on the JVM, <strong>please don&#8217;t</strong> use Java&#8217;s <code>hashCode()</code> for anything that needs to work across different processes. Because it&#8217;ll look like it works fine when you use it with strings and numbers, and then someday a brave soul will use (e.g.) a protocol buffers object, and then spend days banging their head against a wall trying to figure out why messages are getting sent to the wrong servers.</p>

<p>What should you use instead? First, you probably need to serialize the object to a byte stream (which you need to do anyway if you&#8217;re going to send it over the network). If you&#8217;re using a serialization that always maps the same values to the same sequence of bytes, you can just hash that byte stream. A cryptographic hash such as MD5 or SHA-1 would be ok for many cases, but might be a bit heavyweight if you&#8217;re dealing with a really high-throughput service. I&#8217;ve heard good things about <a href='http://code.google.com/p/smhasher/'>MurmurHash</a>, which is non-cryptographic but lightweight and claims to be well-behaved.</p>

<p>If your serialization doesn&#8217;t always produce the same sequence of bytes for a given value, then you can still define a hash function on the objects themselves. Just please don&#8217;t use <code>hashCode()</code>. It&#8217;s ok for in-process hash tables, but distributed systems are a different matter.</p>

<p>(Oh, and in case you were wondering: it looks like the web servers affected by Java&#8217;s hashCode collisions fixed the problem not by changing to a different hash function, but simply by limiting the number of parameters: <a href='http://svn.apache.org/viewvc/tomcat/tc7.0.x/trunk/java/org/apache/tomcat/util/http/Parameters.java?r1=1195977&amp;r2=1195976&amp;pathrev=1195977'>Tomcat</a>, <a href='https://github.com/eclipse/jetty.project/commit/085c79d7d6cfbccc02821ffdb64968593df3e0bf'>Jetty</a>.)</p>
                ]]></content:encoded>
            </item>
        
            <item>
                <title>My FounderLY interview</title>
                <link>http://martin.kleppmann.com/2011/08/16/founderly-interview.html</link>
                <comments>http://martin.kleppmann.com/2011/08/16/founderly-interview.html#disqus_thread</comments>
                <pubDate>Tue, 16 Aug 2011 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2011/08/16/founderly-interview.html</guid>
                
                <description><![CDATA[ Matthew from FounderLY wondered what it would have been like to watch raw video footage of Steve Jobs, Bill Gates, and other tech founders during their formative years. So he&#8217;s been going around interviewing young startup founders, for posterity and for other founders&#8217; inspiration. A pretty interesting effort. A few... ]]></description>
                <content:encoded><![CDATA[
                    <p>Matthew from <a href='http://www.founderly.com/'>FounderLY</a> wondered what it would have been like to watch raw video footage of Steve Jobs, Bill Gates, and other tech founders during their formative years. So he&#8217;s been going around interviewing young startup founders, for posterity and for other founders&#8217; inspiration. A pretty interesting effort.</p>

<p>A few weeks ago he asked whether he could <a href='http://www.founderly.com/2011/07/martin-kleppmann-rapportive-1-of-2/'>interview me</a> for the site. Although it would be rather presumptious to put myself in the category of potential future Steve Jobses, I agreed.</p>

<p>So here you go &#8211; a tidily scripted set of questions from Matthew, and some chaotically unscripted stream-of-consciousness replies from me. The video comes in two parts, about 22 minutes in total, and a transcript is below.</p>
<iframe frameborder='0' height='309' src='http://player.vimeo.com/video/25790273?title=0&amp;byline=0&amp;portrait=0' width='549'>
  <a href='http://vimeo.com/25790273'>View on Vimeo</a>
</iframe>
<p><a href='http://vimeo.com/25790273'>Martin Kleppmann interview, part 1</a> from <a href='http://vimeo.com/founderly'>FounderLY</a> on <a href='http://vimeo.com'>Vimeo</a></p>
<iframe frameborder='0' height='309' src='http://player.vimeo.com/video/25790604?title=0&amp;byline=0&amp;portrait=0' width='549'>
  <a href='http://vimeo.com/25790604'>View on Vimeo</a>
</iframe>
<p><a href='http://vimeo.com/25790604'>Martin Kleppmann interview, part 2</a> from <a href='http://vimeo.com/founderly'>FounderLY</a> on <a href='http://vimeo.com'>Vimeo</a></p>

<p><strong>Transcript</strong></p>

<p><strong>Matthew Wise:</strong> Hi this is Matthew Wise with FounderLY.com. We empower entrepreneurs to have a voice and share their story with the world, enabling others to learn about building products and starting companies.</p>

<p>I&#8217;m really excited today because I&#8217;m here with Martin Kleppmann, founder of Rapportive. Rapportive shows you everything about your contacts inside your email box, enabling you to see who people are and where they are based, so that you can connect and collaborate over shared interests. So, Martin, we&#8217;d love you to give our audience a brief bio.</p>

<p><strong>Martin:</strong> Sure. I&#8217;m originally from Germany, which explains my weird accent, and then I went to the UK for several years to study computer science. That was in Cambridge. After that, I started a startup; it was called Go Test It, we made a tool for automated cross-browser testing of websites. That was pretty cool, and it was acquired a few years ago. After that, I was looking around for something new to do, and together with two friends we started Rapportive.</p>

<p>What we do now is to pull photos, job details from LinkedIn, recent tweets and all of this stuff into Gmail, and show it right there.</p>

<p><strong>Matthew:</strong> What makes Rapportive unique, who is it for and why are you so passionate about it?</p>

<p><strong>Martin:</strong> It&#8217;s really for people who do a lot of email, particularly emailing with people who you don&#8217;t really know well. If you only ever email with ten different people, then you wouldn&#8217;t need it â€” but most of us, particularly startup founders, are constantly dealing with investors, outside advisors, users emailing us, potential customers, potential partners, people on emailing listsâ€¦ all of these people, we vaguely know who they are, but not really. And actually, it is really important that you build this personal contact with them, and get to know them personally.</p>

<p>Previously, when people got an email from someone, they would go and search Google, try to find their Twitter account, try to find them on LinkedIn, and this just takes a lot of time. And we&#8217;ve just automated all of that. The idea is that now, you can actually respond to people personally and build up that personal connection. It&#8217;s little things: even just being able to see the photo of someone in your emailâ€¦ firstly, that&#8217;s a deep visceral connection: you connect much more with them than if you&#8217;re just looking at a wall of text; and also, if you meet them in real life, well, you&#8217;re much more likely to be able to recognise them. I think that makes your email a better place; it&#8217;s really excellent.</p>

<p><strong>Matthew:</strong> What are some of the technology and market trends that currently exist, and how do you see things developing in the future in your space?</p>

<p><strong>Martin:</strong> I&#8217;m not sure about the big trends. There are a lot of things, but they are all very subtle things. For example, people caring a lot about user experience, and we take that really seriously. We put ridiculous amounts of effort into making sure that stuff works really nicely.</p>

<p>Other things that are happening: we are having to deal with more and more people, and people expect that you don&#8217;t just get an automated stock reply, but that people actually engage with you personally. That&#8217;s the future, I think. We&#8217;ve already got that in one-to-one communication between individuals, but the big trend is that companies as a whole are starting to be more personal with the outside. They are no longer this corporate brand, this cold, anonymous thing, but you actually expect to be able to see the people behind that brand, and be able to engage with them directly and build a relationship. And those relationships are what matter, becauseâ€¦ if you&#8217;re just competing on price, your customers can just go somewhere else, but if you can build up a relationship with your customers, that&#8217;s really really powerful.</p>

<p>We think that&#8217;s what we are enabling, by giving you this social substrate for your communications.</p>

<p><strong>Matthew:</strong> Can you tell us what inspired you to start Rapportive? Was there an &#8220;aha&#8221; moment, or did market research lead you to the opportunity? Whatâ€™s the story behind it?</p>

<p><strong>Martin:</strong> It really came from something we wanted ourselves. I think everyone says this! In my previous startup (and my cofounders also had a previous startup), we were all trying to do a lot of engaging with people personally, getting out there, learning a lot from people, really understanding where they were coming from. And that was so much effort! Iâ€™d keep lists of people in a custom database or in spreadsheets or in CRM systems like Highrise, and I&#8217;d have to keep them up to date by hand. Iâ€™d make a lot of notes about people, even just for myself, just so that I could remember when I came back to them six months later: what interactions Iâ€™d had with them, what weâ€™d talked about.</p>

<p>But I then found that all of this information would go stale: for example, I had entered someone&#8217;s job details and then theyâ€™d change jobâ€¦ and Iâ€™m not going to go and re-enter all of this stuff! Itâ€™s already out there on the web â€” really, software should just do this stuff automatically; thereâ€™s no reason why I should have to type this in again.</p>

<p>And then, also, why should I have to always change over to another browser tab in order to search for something, and have five tabs open with different searches for stuff? Itâ€™s just ridiculous, this stuff should be in the tool which I use all the time anyway, which is email.</p>

<p>And so, those are the two premises we started with. We wanted something which keeps itself up-to-date automatically from all the data which is already out there; you shouldnâ€™t have to re-enter anything. And secondly, it should be in the workflow of the tool you already use, which, for most of us is primarily email. And on that premise we said: what can we build? Oh, well, letâ€™s just stick something on the side of Gmail, see how it works. And people loved it.</p>

<p><strong>Matthew:</strong> Excellent! Who is your cofounder, how did you meet, what qualities were you looking for in a cofounder, and how did you know theyâ€™d be a good fit?</p>

<p><strong>Martin:</strong> I have two cofounders, Rahul and Sam; there are three of us. They are both really excellent people. I had known them for a while before starting: we were together in an office space, a kind of co-working space in Cambridge, UK. They were working on their previous startup, and I was working on my previous startup; we worked together a bit, we had lunch together every day, and just ended up talking about a lot of things.</p>

<p>We found that partly we thought the same in a lot of ways, and partly we also had different but nicely complementary ways of thinking. We had a shared culture but often different perspectives, which helped us to together find the best way of doing stuff. And thatâ€™s really the basis on which we work. I think we have a very strong sense of a culture and making sure we work together very well, so we are constantly getting better at what we do.</p>

<p><strong>Matthew:</strong> From idea to product launch, how long did it take, and when did you actually launch?</p>

<p><strong>Martin:</strong> It was pretty quick actually: from first UI mockups to launch it was less than two months. We werenâ€™t actually intending to launch: we had just put up this little website. We were applying for Y Combinator at the time and we also had some other people who were interested, so we wanted to show some potential investors what we were doing. Put up a little website; it wasnâ€™t protected, but just at unknown URL.</p>

<p>And then somehow the press got hold of this, and within a day we found ourselves with 10,000 users on our hands, because it just went wild through all of the blogs. That was a totally crazy experience: we had thought, &#8220;well, weâ€™ve built this little thing, letâ€™s give it to 10 people and see how it works&#8221;, and suddenly we have this massive load of people coming in. And we were working, working very hard, firstly trying to keep the servers up, but fortunately they held up quite nicely. Then also responding to all of the tweets, responding to all of the emails that were coming in. There was lots and lots of stuff happening very quickly; at that point we knew that we were on to something pretty exciting.</p>

<p><strong>Matthew:</strong> And then you formally launched when?</p>

<p><strong>Martin:</strong> We considered that our launch after the fact; we then said, &#8220;Well, OK, I guess weâ€™ve launched now. Oh well, weâ€™ve launched.&#8221; And then since then weâ€™ve, at times, launched new features but that original bit of press we regard as our real launch.</p>

<p><strong>Matthew:</strong> Are there any unique metrics or social proof about Rapportive that youâ€™d like to share with our audience?</p>

<p><strong>Martin:</strong> I think the thing I find most exciting: we always have a Twitter search going on â€” we have a big screen in the office, showing what people are saying about Rapportive on Twitter â€” and thereâ€™s just this constant stream of people loving it. Iâ€™m really humbled all of the time I see this. Every hour thereâ€™s stuff coming in from people saying things like, &#8220;This product has changed our life.&#8221;</p>

<p>And thatâ€™s just amazing: when people will actually go out of their way to say something like that, and weâ€™re not even particularly prompting them. So yeah, we have hundreds of thousands of users at the moment, but the important thing is really how much people care about it.</p>

<p><strong>Matthew:</strong> We know founders face unique challenges when they start a company. What was the hardest part about launching or starting Rapportive, and how did you overcome this obstacle?</p>

<p><strong>Martin:</strong> So we had a bit of a frustrating phase over the last summer. We were working very, very hard and there was lots going on, but our product was making very little visible progress, because we were spending all of our time firefighting, scaling our database because we had so much stuff coming in that we had to do a lot of work to re-architect it. We were doing a lot of groundwork for features which are just coming out now, but in technical groundwork there are months of work which is just invisible. We were moving country because we were all coming from the UK, moving to San Francisco, and we were fighting with US immigration. We were also spending a lot of time on support â€” which is good, itâ€™s really valuable, because we learn a lot about the problems that people have, but again itâ€™s very time consuming.</p>

<p>So, with all of those things, itâ€™s all useful stuff; thereâ€™s nothing really wasteful there. But on the other hand, our product wasnâ€™t making progress, and people were starting to ask, &#8220;Well, youâ€™ve been around for six months now, nine months now, and youâ€™ve not really released any exciting new features. Whatâ€™s going on?&#8221; And we were just saying, &#8220;Yeah, weâ€™re trying to get to it, weâ€™re doing what we can!&#8221;</p>

<p>And then I was so happy when, towards the end of 2010, we got over this big hump of stuff, and now weâ€™re putting out features again and there is much more visible progress. So that was a fairly hard phase to go through, but Iâ€™m really glad we got over it. In the end you just have to work through it. You just have to not give up, just keep on going, keep on going, even if itâ€™s getting tough.</p>

<p><strong>Matthew:</strong> Since youâ€™ve been in operation, what have you learned about your business and your users that you didnâ€™t realize before you launched?</p>

<p><strong>Martin:</strong> When we first launched I was a bit cautious. I was wondering: &#8220;are people going to be really freaked out by seeing how much information is actually publicly available about them on the web?&#8221; You know, when you think about it rationally, itâ€™s obvious: you can just search for someone on Google, and for most people youâ€™ll actually get a pretty good idea of who this person is just by looking at the search results. And weâ€™ve just taken away a step by automating a lot of that search, making it more convenient by putting it in email.</p>

<p>And so I was expecting that thereâ€™d be a lot of people who would go, &#8220;Oh my God, no, privacy is dead!&#8221; But we tried to manage that very carefully: whenever anyone was concerned, we listen to them and respond to any concerns very quickly, and explain what weâ€™re doing, why weâ€™re doing it and why we think itâ€™s absolutely fine. We are all very privacy conscious and we make that very clear as well. We donâ€™t mess around with peopleâ€™s private data; we only show information which people actually want to be public.</p>

<p>And that is something I found surprising: just how quickly we can defuse any situation. If anyone was upset weâ€™d just talk to them quietly, patiently, and explain whatâ€™s going on. If there was any problem, fix it quickly â€” and all the problems suddenly go away. And thatâ€™s really encouraging, because it means that we seem to actually be doing the right thing: pushing the envelope a bit. But yes, it works.</p>

<p><strong>Matthew:</strong> What is it that you make look easy? What skill or talent comes easy or intuitively to you, and what has been difficult and how do you manage that?</p>

<p><strong>Martin:</strong> Iâ€™d say: what we, as a team, are particularly good at is product design. Making something which is very neat, stays out of your way, but is still powerful; which does exactly the stuff you need, not more, not less; and just behaves the way people expect it to behave, without running into a weird corner where you donâ€™t know what to do.</p>

<p>And that is actually really hard to achieve. The amount of time we spend on optimizing the workflows for different users, depending on which starting state theyâ€™re coming from, which screens they have to go through and exactly what button we can show in which place, exactly what copy we use, what words we use to describe things, then taking them through the flowâ€¦ and then, to the user, all it looks like is: &#8220;oh, I clicked a button, a pop-up appeared, I clicked another button and it worked.&#8221;</p>

<p>Thatâ€™s something we really enjoy: making that look easy, but a lot of work goes into it. In the end people just appreciate it as a product which is really nicely designed, which just works and which gives them a kind of warm, fuzzy feeling.</p>

<p><strong>Matthew:</strong> Whatâ€™s the most important lesson youâ€™ve learned since launching Rapportive?</p>

<p><strong>Martin:</strong> The most important lesson? Iâ€™ve not really graded them in a particular priority.</p>

<p>Iâ€™d say, off the top of my head: caring about user experience and caring about users was something we thought from the start was really important â€” and that has really been validated. People appreciate us for having a product which just works nicely, and which has the little details thought out.</p>

<p>People appreciate that we get back to them quickly, that weâ€™re always very friendly when responding to them, that weâ€™re trying to be personal where we can.</p>

<p><strong>Matthew:</strong> Martin, what bit of advice do you wish you would have known before starting Rapportive?</p>

<p><strong>Martin:</strong> I think whatâ€™s really interesting is that in a startup everything is magnified. If you have any issue early on, that will just continue, continue, get bigger and bigger, so if you have any issue early on then make sure you fix it early on. I think weâ€™ve generally done a pretty good job of that. But itâ€™s worth doing that really consciously.</p>

<p>Certain things are really hard, but you need to get good at them. For example, communicating and sharing intuitions, thatâ€™s a topic that Iâ€™ve been thinking about a lot. We find that, since weâ€™re three cofounders, we often have similar ideas about things, but and then often find that they differ in subtle ways. Really what we want to do is to combine our three intuitions into one, so that together, we have a really good broad and also deep insight into what people want. That requires that you find ways of explaining to the others not just <em>what</em> you think, but <em>why</em> you think it.</p>

<p>And thatâ€™s really hard to learn, and weâ€™ve gradually been getting better at that. As you go about things, just be conscious of the fact that itâ€™s going to take a lot of effort and time, even just to learn to speak the same language. You think you all speak English, but then you find, of course, that you make up your own words to describe the domain youâ€™re working in. A lot of things are just completely non-obvious.</p>

<p>You get a lot of conflicting advice from outside mentors. We have a lot of really good investors, advisors, mentors, and often they say completely contradictory things â€” and thatâ€™s fine. You just need to learn to absorb those things into your own intuition, and within the team work out how you can share those intuitions. Then you can have a coherent vision, all together, for what youâ€™re going to build, why itâ€™s important, how youâ€™re going to go forward.</p>

<p><strong>Matthew:</strong> What bit of advice would you like to share with our audience about launching a startup? If you have to distill it, what are the key elements?</p>

<p><strong>Martin:</strong> One thing, which worked in our favor but is not necessarily particularly replicable: if your product works well for journalists, then journalists will write about it quite a lot. We didnâ€™t realize this initially, but it happened to be the fact that, Rapportive works really well for people who deal with a lot of incoming weird stuff from lots of people they donâ€™t know, and need to assess very quickly whether the sources are reliable. And, well, thatâ€™s pretty much what journalists do.</p>

<p>It was also the case that when we started Rapportive, a lot of the data we had about people was not particularly great, but bloggers tend to be the kind of people who are very present on social media, so we had really great data for them! And that worked in our favor. Since then weâ€™ve got a lot better at data for everyone else, and now weâ€™ve got a pretty high coverage rate for everyone. But for that initial launch, just working well for reporters and bloggers was pretty good.</p>

<p>But of course, you canâ€™t choose your startup based on the fact that itâ€™s going to be useful for bloggers, so thatâ€™s not very useful advice.</p>

<p>There are lots of different schools of thought for launching and they all kind of make sense. Thereâ€™s the &#8220;launch small and make sure that youâ€™re continuously learning&#8221; school, and that makes a lot of sense. And then thereâ€™s also the school which observes that, if you can get a lot of very quick press that generates a lot of excitement and a lot of buzz, thatâ€™s also valuable. In the end, with these things thereâ€™s never a right answer; you just have to take in all of the bits of advice you hear and create your personal conglomerate of what makes sense.</p>

<p><strong>Matthew:</strong> Before we close, I would love for you to give our audience your vision of Rapportive and how you hope it will change the world.</p>

<p><strong>Martin:</strong> Weâ€™ve got a lot of really exciting things coming. I donâ€™t want to talk about them in too much detail, but to give a rough outline:</p>

<p>I think, firstly, the inbox is a really, really interesting place, because thatâ€™s where all of your communications come together. Email is the primary one we use at the moment; I donâ€™t know, maybe itâ€™ll be Facebook mail within two yearsâ€™ time, but that doesnâ€™t really matter, thatâ€™s beside the point.</p>

<p>The point is that people are really, really opinionated about which tool they want to use, and getting people to change tool is really, really hard. So weâ€™re building Rapportive in the philosophy that we donâ€™t people to change behavior; we just want people to continue doing what theyâ€™re doing already, and just make it better.</p>

<p>Just add those little magic touches, add little things which either save you time, or which take something which was previously laborious (and required switching to other browser tabs and required re-entering of data), and make all of that go away. Just make it be there, and make common tasks feel natural.</p>

<p>Thatâ€™s the philosophy with which weâ€™re going about things, and that seems to be working pretty well.</p>

<p><strong>Matthew:</strong> Excellent. Well, Martin, itâ€™s been a pleasure having you as a guest on FounderLY. Weâ€™re rooting for your success at Rapportive. For those in our audience whoâ€™d like to learn more you can visit their website at www.rapportive.com and register to become a user and join their community. This is Matthew Wise with FounderLY. Thanks so much, Martin.</p>

<p><strong>Martin:</strong> Thank you, Matthew.</p>
                ]]></content:encoded>
            </item>
        
            <item>
                <title>Evolution of Rapportive's new design</title>
                <link>http://martin.kleppmann.com/2011/05/24/evolution-of-rapportive-new-design.html</link>
                <comments>http://martin.kleppmann.com/2011/05/24/evolution-of-rapportive-new-design.html#disqus_thread</comments>
                <pubDate>Tue, 24 May 2011 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2011/05/24/evolution-of-rapportive-new-design.html</guid>
                
                <description><![CDATA[ This is a re-post from the Rapportive blog. Today we are launching a new design for Rapportive. We put a huge amount of effort into this design, because we all believe deeply great user experience, and we know that our users really appreciate it too. Here it is (best viewed... ]]></description>
                <content:encoded><![CDATA[
                    <p><em>This is a re-post from the <a href='http://blog.rapportive.com/53827077'>Rapportive blog</a>.</em></p>

<p>Today we are launching a new design for Rapportive. We put a huge amount of effort into this design, because we all believe deeply great user experience, and we know that our users really appreciate it too.</p>

<p>Here it is (best viewed fullscreen):</p>
<iframe allowfullscreen='allowfullscreen' frameborder='0' height='309' src='http://www.youtube.com/embed/DPaSxa2vopU' width='549'>
  <a href='http://www.youtube.com/watch?v=DPaSxa2vopU'>View on YouTube</a>
</iframe>
<p>In this post I&#8217;d like to explain some of our process and thinking in the creation of the new design.</p>

<p>Our old design, which we had <a href='http://blog.rapportive.com/the-accidental-launch'>since launch</a>, has served us very well. It was subtle, simple, effective, and blended in well with Gmail. Unfortunately, it was beginning to show some limitations:</p>

<ol>
<li>If you&#8217;re using Gmail on a small screen, on a laptop or even a netbook, the Rapportive sidebar would often be too tall to fit on screen. Of course you can scroll down, but the main Gmail scrollbar is already used to scroll down in the conversation. We hooked into the page scrolling, but if you were on a long conversation, you had to scroll all the way to the bottom of the conversation if you wanted to see the rest of the Rapportive sidebar. That was really annoying.</li>

<li>Over the months we&#8217;ve been adding more and more useful stuff to the sidebar: <a href='http://blog.rapportive.com/address-book-inbox-together-at-last'>phone numbers</a>, <a href='http://blog.rapportive.com/40551428'>Facebook status updates</a>, <a href='http://blog.rapportive.com/grow-your-network-with-rapportive'>LinkedIn invitations</a>, and a choice of <a href='http://raplets.com/'>Raplets</a> for a <a href='http://thenextweb.com/apps/2010/04/29/rapportive/'>variety of</a> <a href='http://blog.rapportive.com/get-out-of-your-inbox-and-meet-people-in-pers'>different</a> <a href='http://blog.rapportive.com/rapportive-for-developers-bitbucket-github-st'>purposes</a>. This has pushed the previous design to its limits: some profiles would become unmanageably tall.</li>

<li>Different people find different parts of the sidebar useful. Some find <a href='http://twitter.com/GraemeF/status/25286282993213440'>recent tweets</a> most useful, others swear by our <a href='http://twitter.com/smsbnyc/status/40436337681104896'>CRM raplets</a>, others again leave <a href='http://twitter.com/nickcernis/status/15001395635691520'>lots of notes</a> about their contacts. But it sucks if the thing you&#8217;re most interested is often scrolled off the bottom of the screen because the things higher up in the sidebar are taking all the space.</li>
</ol>

<p>These three points have a common theme: we do not handle long sidebars well.</p>

<p>How long can a sidebar get? Well, here&#8217;s my sidebar, with the CrunchBase and the MailChimp Raplets:</p>
<p style='text-align: center'>
  <img alt='Example of very tall Rapportive profile' height='1361' src='http://martin.kleppmann.com/2011/05/tall_profile.png' width='230' />
</p>
<p>As we add more features to Rapportive, these problems would only get worse, so we decided that it was time to rethink our design.</p>

<h2 id='our_design_principles'>Our design principles</h2>

<p>We had several guiding principles for the redesign. Rapportive should:</p>

<ul>
<li>Remain very subtle and unobtrusive: Rapportive should be there for you when you want it, but should not try to grab your attention or use more space than necessary. Your <em>email</em> is what&#8217;s important, not the sidebar!</li>

<li>Allow you to serendipitously discover things about your contacts: the information should simply be there when you glance at the sidebar, and shouldn&#8217;t require a lot of clicking or scrolling.</li>

<li>Look good on both large and small screens, both with lots of data and little data. That means it has to make efficient use of screen space.</li>

<li>Avoid configuration dialogs. The interface should just do the right thing.</li>

<li>Be clear, obvious to use, beautiful and <em>enjoyable</em> to interact with :)</li>
</ul>

<h2 id='buttons'>Buttons</h2>

<p>First of all, I started with some graphical tweaking. Here are some nice buttons:</p>
<p style='text-align:center'>
<a href='http://martin.kleppmann.com/2011/05/connect_buttons.png'><img alt='Different sylings for &apos;add friend&apos;/&apos;connect&apos; buttons' src='http://martin.kleppmann.com/2011/05/connect_buttons.png' /></a>
</p>
<h2 id='how_do_we_handle_long_sidebars'>How do we handle long sidebars?</h2>

<p>Our first thought: we could simply give the Rapportive sidebar its own scrollbar. That would avoid having to scroll down to the bottom of a long conversation, because you could scroll the sidebar separately. But that approach is pretty bad. A large part of the sidebar may still end up being hidden off-screen, which means you have to go out of your way to scroll down, making it unlikely that you&#8217;ll discover things serendipitously.</p>

<p>Another idea that we ruled out quickly was a &#8216;tabbed&#8217; interface. Tabs work well in a browser, where you have exactly one web page per tab, and each tab is independent. Rapportive isn&#8217;t like that: we might have several pages of information for one person (i.e. the tabs aren&#8217;t independent), and while we have several full tabs for one person, the information for another person might be a lot more sparse. That means that either you have to either have to waste a lot of space (e.g. always have a tab for tweets, even if the person doesn&#8217;t have a Twitter account), or you have things appearing in different places for different people (which is confusing). Finally, tabs require a lot of laborious clicking: you can&#8217;t see what&#8217;s in a tab without clicking it, and you can&#8217;t see the contents of two tabs at the same time. Tabs would have been an unpleasant, clunky interface.</p>

<p>An &#8216;accordion&#8217; interface, like you find in Outlook for example, seemed like a step in a more promising direction:</p>
<p style='text-align:center'>
<a href='http://martin.kleppmann.com/2011/05/accordion.png'><img alt='Accordion interface example' src='http://martin.kleppmann.com/2011/05/accordion.png' /></a>
</p>
<p>You can have several sections, and each section can expand to show additional information when you click it. When you expand one section, another one collapses to make room. (In the screenshot above, I could click &#8216;Contacts&#8217; to see my list of contacts; the list of mailboxes under the &#8216;Mail&#8217; heading would be hidden to make room for the list of contacts.)</p>

<p>Accordions are fairly efficient when space is very limited, but they suck if you have a large screen. If you have enough space that all your sections could be comfortably expanded side-by-side, why limit it to only one expanded section at a time? I don&#8217;t want to have to click a section to see what&#8217;s inside. It&#8217;s the same problem as with tabs.</p>

<p>So I started experimenting with a kind of adaptive accordion which could have several sections expanded at the same time, if there was enough screen space available. Here are some early design ideas:</p>
<p style='text-align:center'>
<a href='http://martin.kleppmann.com/2011/05/section_headings.png'><img alt='Three designs for separators/headings between sidebar sections' src='http://martin.kleppmann.com/2011/05/section_headings.png' width='550' /></a>
</p>
<p>The designs were fairly ugly, but I could see an algorithm emerging here. I figured that we needed an accordion with the following improvements:</p>

<ul>
<li>It should be possible for several sections to be expanded at the same time, up to a maximum of what will fit on screen without scrolling.</li>

<li>When you expand a section, other sections may need to collapse in order to make space on screen. The application should be intelligent about which sections to collapse &#8211; for example, if you haven&#8217;t clicked a section in a long time, you probably find it less useful than a section which you clicked just now. So we should keep the recently-used sections expanded, if possible.</li>

<li>The collapsed version of a section should be useful too; for example, the collapsed Twitter section could show just the username, whereas the expanded version could show the username and three most recent tweets. If the contact doesn&#8217;t have a Twitter account, we shouldn&#8217;t show the section at all, since it would just be a waste of space.</li>

<li>Sometimes a section gets very tall, for example a Facebook status with lots of comments. In that case, we need to limit the section&#8217;s height and give it a scrollbar, to avoid it dominating the entire sidebar. But if we can avoid scrollbars, we should do without.</li>
</ul>

<p>Even scrollbars, despite being such a standard part of user interface design, have their problems:</p>
<p style='text-align:center'>
<a href='http://martin.kleppmann.com/2011/05/scrolling.png'><img alt='Misalignment of text due to scrollbar spacing' src='http://martin.kleppmann.com/2011/05/scrolling.png' /></a>
</p>
<p>Fortunately such spacing issues are easy to iron out. A harder question is: how do we communicate to the user that they can expand a section?</p>

<h2 id='expandable_sections'>Expandable sections</h2>

<p>In the old design, if you clicked someone&#8217;s Twitter username, we would open up a new browser tab to show their tweet stream. In an accordion design, however, you&#8217;d expect that clicking the collapsed section will cause it to expand (i.e. show recent tweets, not open a new browser tab). Do we break the old interaction and force users to learn a new behaviour, or do we add an extra button for expanding a section?</p>
<p style='text-align:center'>
<a href='http://martin.kleppmann.com/2011/05/expand1.png'><img alt='Up/down arrow button to trigger expansion of a section' src='http://martin.kleppmann.com/2011/05/expand1.png' /></a>
</p>
<p>That was my first attempt. The arrows serve two purposes: to indicate that the section can be expanded, and to act as a button to trigger the actual expanding. But I didn&#8217;t like it. Some sections would have arrows and others wouldn&#8217;t (because we don&#8217;t always have additional information), so the &#8220;connect&#8221;/&#8221;add friend&#8221; buttons could become strangely misaligned. It also made the interface look more cluttered and complicated.</p>

<p>Surely we could do better? For example, could we show the button for expanding a section only when you hover over it? That&#8217;s an interesting idea. What&#8217;s more, we could then calculate how tall the section would be if it was expanded, and indicate it with the height of the arrow:</p>
<p style='text-align:center'>
<a href='http://martin.kleppmann.com/2011/05/expand2.png'><img alt='Large arrow when hovering mouse over expanded section; click the arrow to expand' src='http://martin.kleppmann.com/2011/05/expand2.png' width='550' /></a>
</p>
<p>At this point we&#8217;re really moving away from established user interface patterns. Will users notice the arrow, and understand what it means? Will they figure out that you can click it? It contains a lot of information: the fact that the section can be expanded, how big it will be and where it will be placed when expanded. It&#8217;s also a much bigger click target than the previous arrow button, which is good. But it still has the problem of not looking particularly like a button. (It goes grey when you hover over it, to indicate that you can interact with it, but still it&#8217;s not exactly obvious.)</p>

<p>I was starting to get sick of this redesign, but fortunately, inspiration struck again. If we&#8217;re already using an arrow on hover to indicate how tall the expanded section will be, well&#8230; why don&#8217;t we use <em>the expanded section itself</em> to indicate how tall it will be? Yes, we can just show the expanded section itself on hover!</p>
<p style='text-align:center'>
<a href='http://martin.kleppmann.com/2011/05/expand3.png'><img alt='Showing the expanded version when hovering over a collapsed section' src='http://martin.kleppmann.com/2011/05/expand3.png' width='550' /></a>
</p>
<p>It&#8217;s obvious in retrospect, but it took a surprisingly long time to come up with this design. I call it the <a href='http://en.wikipedia.org/wiki/Jinn_in_popular_culture'>&#8220;genie&#8221;</a> because it looks like a ghost that has come out of an oil lamp. Although the version above still isn&#8217;t pretty, it really got to the bottom of many of the challenges I discussed at the beginning:</p>

<ul>
<li>When you&#8217;re not hovering your mouse over the sidebar, Rapportive remains minimalistic and uncluttered. No more buttons than necessary.</li>

<li>If you&#8217;re on a large screen, the sections of the sidebar expand to fill the screen space, so you can see everything at a glance. No need to click anything (unless you want to add them as a friend, of course).</li>

<li>If you&#8217;re on a smaller screen, we collapse your lesser-used sections to make them more compact; in most cases, this allows us to fit everything on screen without need for scrolling.</li>

<li>When you&#8217;re interacting with the sidebar, we allow Rapportive to pop up additional information outside of the sidebar. But we don&#8217;t encroach on your email space when you&#8217;re not interacting with the sidebar.</li>

<li>The visual cues makes it pretty obvious that the expanded section is a bigger, more verbose version of the collapsed section. (I think it&#8217;s a bit like a magnifying glass.)</li>

<li>If there are several collapsed sections next to each other in the sidebar, you can skim the content of all of them by slowly moving the mouse across each of the collapsed sections. As the mouse moves from one section to another, the genie for the previous section disappears and the one for the new section appears. Wonderful for getting a quick overview of someone&#8217;s online activity before you reply to their email.</li>
</ul>

<p>Not all is great though. If there is space, we&#8217;d still like to show you the expanded section in the sidebar right away, without you having to hover the mouse. But how do we make clear to users that sections can be expanded?</p>

<p>In the screenshot above, I tried using a button with an arrow pointing right. If you clicked it, the expanded section would slide into the sidebar, and the other sections would move out of the way to make room. I liked the animation, but the button was ugly. Fortunately, Rahul had the idea that we could use a right-pointing mouse cursor to indicate the same thing. So now you can click anywhere on the genie and it will slide into the sidebar.</p>

<p>After some graphical tweaking, this is what the final design looks like:</p>
<p style='text-align:center'>
<a href='http://martin.kleppmann.com/2011/05/expand4.png'><img alt='End result: Rapportive&apos;s new sidebar design with collapsible section' src='http://martin.kleppmann.com/2011/05/expand4.png' width='550' /></a>
</p>
<p>I hope you&#8217;ll agree that it is <em>gorgeous</em>. We have added very little new user interface (and we&#8217;ve taken a lot away, compared to earlier designs), but the result is very effective. It fits neatly on both big and small screens, it is easy to use, and it is actually lots of fun to play with. Give it a try for yourself, and let us know what you think!</p>

<p>Usability without design is dreary. Design without usability is pretentious. Design and usability together are delightful.</p>
                ]]></content:encoded>
            </item>
        
            <item>
                <title>What's so special about Y Combinator?</title>
                <link>http://martin.kleppmann.com/2011/03/15/whats-so-special-about-y-combinator.html</link>
                <comments>http://martin.kleppmann.com/2011/03/15/whats-so-special-about-y-combinator.html#disqus_thread</comments>
                <pubDate>Tue, 15 Mar 2011 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2011/03/15/whats-so-special-about-y-combinator.html</guid>
                
                <description><![CDATA[ Since we joined Y Combinator&#8217;s Summer 2010 funding cycle, I keep getting asked how much value we got from it, and whether I would do it again. tl;dr The short answer: we got a huge amount of value during the three-month main programme, and we are continuing to benefit all... ]]></description>
                <content:encoded><![CDATA[
                    <p>Since we joined <a href='http://ycombinator.com/'>Y Combinator&#8217;s</a> Summer 2010 funding cycle, I keep getting asked how much value we got from it, and whether I would do it again.</p>

<p><strong>tl;dr</strong> The short answer: we got a huge amount of value during the three-month main programme, and we are continuing to benefit all the time from being part of YC. And yes, I would do it again without a moment&#8217;s doubt.</p>

<p>The long answer deserves a bit more explanation.</p>

<p>On the surface, it seems like the Y Combinator formula is <em>&#8220;money and mentoring for equity&#8221;</em>. That seems like a simple enough formula, and clones around the world are rushing to replicate it. Unfortunately I think they mostly miss the point, because there is so much more to YC which the others lack.</p>

<p>When <a href='http://rapportive.com'>we</a> applied to YC, we had already <a href='http://blog.rapportive.com/the-accidental-launch'>launched</a> and didn&#8217;t really need the $20k they were offering (investors had started contacting us from the day that Rapportive first hit the press). Looking at the other startups who joined YC at the same time of us, most of them got profitable or secured substantial angel/VC funding at good valuations soon afterwards. So although the money is nice, it&#8217;s probably not the main attraction.</p>

<p>As to mentoring, press and investor contacts&#8230; as an entrepreneur, you&#8217;re probably already quite good at getting attention and feedback from relevant people, using your personal network. How much more value could you get?</p>

<p><strong>Network and knowledge</strong></p>

<p>The things that are really special about Y Combinator, in my opinion, are less frequently talked about:</p>

<ul>
<li>
<p>There is a genuine, strong sense of camaraderie and mutual support amongst YC alumni. Many organisations claim to have strong alumni networks, but I have often found these claims to be empty words: often, the only participants in these networks are those who want to take, not those who want to give, because those who have something to give feel their time is wasted. It is hard to maintain a high standard in a network, and YC&#8217;s is the only example I have seen where the high standard is consistently upheld. Even the most busy founders of the most successful startups are genuinely supportive, and will spend surprisingly large amounts of time to answer questions, give practical advice, make introductions and recommendations, etc. They are real friends, not just utilitarian business contacts. This culture is truly remarkable.</p>
</li>

<li>
<p>It sounds trite, but the YC partners are all really great people. They are exceptionally bright and talented, outspoken, honest, straight-to-the-point and totally bullshit-free &#8211; hardly anyone else I know even comes near in terms of these qualities. They have lots of time for their companies (surprisingly, given the number of companies they&#8217;ve funded), and I have only ever seen them do things that are good for founders. It seems altruistic, but Paul Graham maintains that it&#8217;s simply the best business strategy for YC.</p>
</li>

<li>
<p>Between them, the YC partners have seen it all. They&#8217;ve seen big exits and small exits, profitable companies and train-wrecks, fast growth and slow growth, dream teams and founder disputes, and all manner of great ideas and screw-ups. Startups are unpredictable; you never know what surprises lie around the next corner. When we talk to YC, no matter whether we have good news or bad news, chances are that they&#8217;ve seen the situation before, and their pattern-matching will enable them to make good predictions.</p>
</li>

<li>
<p>The YC team know everybody who&#8217;s worth knowing in the startup scene, and everyone respects them. This makes them ideally placed for introducing you to the kind of people you want to meet. Demo Day is one aspect of this, a maximum-efficiency batch process for introducing startups and investors to each other, but YC also make individual introductions all the time. They also know who is worth talking to, who is actively investing, and which people are just fishing for information.</p>
</li>

<li>
<p>If there is a <a href='http://www.paulgraham.com/future.html'>change in the startup ecosystem</a>, YC are amongst the first to see it &#8212; because they have insight into a uniquely high proportion of startup deals, and they work actively to spot patterns and learn from them. As a YC startup, you get to hear about these changes first, something that you can use to your advantage.</p>
</li>

<li>
<p>If you are new to Silicon Valley, like we were, YC is a fantastic way to get yourself established and find your feet. They are welcoming to outsiders, and you can bootstrap your valley network from those who already know their way around. You can celebrate with them when things go well, and to get encouragement from them in hard times. You couldn&#8217;t ask for a better group of people to hang out with.</p>
</li>

<li>
<p>Because YC has a proven track record of funding great companies, being accepted to YC carries a strong signalling effect, making it more likely for others to <a href='http://thenextweb.com/industry/2011/02/07/y-combinator-partner-harjeet-taggar-start-fund-is-bad-news-for-bad-investors-interview/'>believe</a> that you are doing something interesting by default.</p>
</li>
</ul>

<p><strong>Being part of the best</strong></p>

<p>A lot of the points above are remarkable, but not intrinsically unique to Y Combinator; what&#8217;s particularly special about YC is that it is <strong>simply <a href='http://blog.businessofsoftware.org/2009/09/joel-spolskys-talk-at-business-of-software-2008-on-being-number-one.html'>number one</a></strong>. Being number one is very different from being number two or any number below.</p>

<p>One friend from our YC batch remarked at Demo Day: <em>&#8220;This room contains the future of the IT industry.&#8221;</em> He was right. Since many of the world&#8217;s best startups go through YC, they collectively form a force which has the power and drive to shape the entire industry for many years to come. That&#8217;s not because there is any centralised agenda; it&#8217;s more comparable to being a graduate from one of the top universities in the world.</p>

<p>Ever wonder why so many leading figures in business, science and politics are graduates of Harvard, Yale, Cambridge, Oxford, MIT or Stanford? I don&#8217;t think their teaching is really that much better than any other university, or than reading the textbooks by yourself. But you do get two valuable things from a top university:</p>

<ol>
<li>a signalling effect: other people can see that a reputable organisation thinks that at one point in your life (when you took your exams), you were reasonably motivated and not entirely stupid;</li>

<li>you build a network of talented people.</li>
</ol>

<p>I think we are seeing something similar with YC. The fact that YC has brought forth a number of successful startups is merely a correlation; by itself it doesn&#8217;t say anything about cause and effect. But when people see a correlation, it has a signalling effect nevertheless. (The reputation of a good university is also mostly due to the observed correlation between its graduates and their later success.)</p>

<p>Startups are risky and full of unknowns, and you as startup founder are in the business of convincing everyone that you are going to be the big, successful one. You need all the positive signals you can get.</p>

<p><strong>But is it worth the cost?</strong></p>

<p>Y Combinator takes 2&#8211;10% of your company&#8217;s equity. How do you figure out whether the value you get from YC is worth the cost? If you are wondering whether to apply for YC, this is probably the question you&#8217;re trying to answer.</p>

<p>Firstly, note this: <strong>dilution</strong> makes an <strong>incremental</strong> difference to your outcome: if you sell 5% of the company to an investor, you reduce your pay-out by 5% if you sell the company. However, whether you <strong>have or don&#8217;t have</strong> an investor can make a huge difference. Say you give 5% of the company to&#8230;</p>

<ul>
<li>someone who later makes that one critical introduction that leads to the deal which saves your company.</li>

<li>someone who helps you negotiate with your acquirer and doubles the value of your exit.</li>

<li>someone who encourages you to make a particular pivot, which turns out to unlock a billion-dollar market.</li>

<li>someone who prevents you from making a stupid mistake that would have set you back by 12 months.</li>
</ul>

<p>Whether any of these scenarios will actually happen is unknown in advance, but my point is: probability of success tends to move in discontinuous jumps. An extra per cent of equity may make absolutely no difference at all to your success, or it may turn out to make the difference between epic fail and massive win. A bit of equity may buy you an &#8220;unfair advantage&#8221;, or it may be a complete waste.</p>

<p>You need to figure out which investors might make the big difference, and which probably won&#8217;t. Your job as founder is to figure out how to play your cards such as to maximise the chances of massive win. Dilution changes incrementally, but probability of success is much more variable. Therefore, if you can figure out a way of substantially increasing your chances of success (putting yourself on the good side of some of those discontinuities), the equity cost is secondary. It&#8217;s not irrelevant, but as long as it&#8217;s in the right ballpark, it&#8217;s ok.</p>

<p>Of course you should be prudent to whom you give equity, but I would argue that if someone can give you &#8220;unfair advantages&#8221;, it&#8217;s well worth bringing them on board and not worrying too much about the cost.[<a href='#footnote-cap-table'>1</a>]</p>

<p>So, does Y Combinator give you that big advantage which has a disproportionately large positive impact on your chances of success?</p>

<p>I&#8217;d say yes. If you don&#8217;t need any of the benefits mentioned above, maybe not&#8230; but honestly, I&#8217;d be very surprised if your network and your group of advisors is already so perfect that you wouldn&#8217;t benefit from YC. Whether you&#8217;ve already launched or not makes very little difference.</p>

<p>YC is a package consisting of a variety of good things. In principle you may be able to assemble yourself a similar package from component parts &#8212; e.g. using <a href='http://angel.co/'>AngelList</a> for your investor intros, asking around to find suitable advisors, and spending lots of time networking and taking speculative meetings. But somehow that feels to me like buying individual CPUs and RAM and rack-mount cases to assemble your servers, when you could just spend 10 minutes to buy computing resources from Heroku, EC2 or Rackspace. It might make sense for some people, but for most of us, the time saving and assured quality you get from a good pre-built package is well worth a bit of extra cost. (That doesn&#8217;t mean you can&#8217;t use component parts <em>as well</em> &#8211; for example, we used AngelList to fill up our <a href='http://techcrunch.com/2010/08/02/rapportive-funding/'>seed round</a>.)</p>

<p>I hope you find this useful when deciding whether to apply to YC. :)</p>
<hr /><p id='footnote-cap-table'>[1]
In my opinion, the main reason to be careful with distribution of equity is not because dilution
reduces the size of your payout, but rather because you should avoid odd-looking things on your
cap table. Later investors will see weird things on the cap table during due diligence, may assume
that you are irresponsible in the way you run the business, and pull out of the deal. That's
something you can avoid by keeping your capital structure nice and clean. But honestly, no investor
in the world could possibly object to seeing YC on your cap table, because they know perfectly well
how much value YC brings. So that concern is irrelevant here.</p>
                ]]></content:encoded>
            </item>
        
            <item>
                <title>Accounting for Computer Scientists</title>
                <link>http://martin.kleppmann.com/2011/03/07/accounting-for-computer-scientists.html</link>
                <comments>http://martin.kleppmann.com/2011/03/07/accounting-for-computer-scientists.html#disqus_thread</comments>
                <pubDate>Mon, 07 Mar 2011 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2011/03/07/accounting-for-computer-scientists.html</guid>
                
                <description><![CDATA[ Every educated person really ought to have a basic understanding of accounting. Just like maths, science, programming, music, literature, history, etc., it&#8217;s one of those things which helps you make sense of the world. Although dealing with money is not much fun, it&#8217;s an unavoidable part of life, so you... ]]></description>
                <content:encoded><![CDATA[
                    <p>Every educated person really ought to have a basic understanding of accounting. Just like maths, science, programming, music, literature, history, etc., it&#8217;s one of those things which helps you make sense of the world. Although dealing with money is not much fun, it&#8217;s an unavoidable part of life, so you might as well take a few minutes to understand it.</p>

<p>Sadly, in my opinion, most accountants do a terrible job of explaining their work in an accessible way; it&#8217;s a field full of jargon, acronyms and weird historical legacies. Even &#8220;Bookkeeping for Dummies&#8221; makes my head spin. Surely this stuff can&#8217;t be that difficult?</p>

<p>(We computing people are probably guilty of the same offence of bad explanations and jargon. The problem is, once you have become intimately familiar with a field, it&#8217;s very hard to imagine how you thought about things before you understood it.)</p>

<p>Eventually I figured it out: basic accounting is just graph theory. The traditional ways of representing financial information hide that structure astonishingly well, but once I had figured out that it was just a graph, it suddenly all made sense.</p>

<p>I&#8217;m a computer scientist, and I think of stuff in graphs all the time. If only someone had explained it like that in the first place! It would have saved me so much confusion. So I want to try to fix that. If you like graphs, then by the time you reach the end of this article, you should know everything you need in order to understand the financial statements for a small company/startup (and even calculate them yourself, in a spreadsheet or programming language of your choice).</p>

<p>It&#8217;s really not that hard. Let&#8217;s go!</p>

<p><strong>Accounts = Nodes, Transactions = Edges</strong></p>

<p>Say you go to the bagel shop and buy a Super Club bagel for $5 on the company credit card. You also visit some random Silicon Valley startup and buy one of their surplus Aeron chairs, second hand, for $500 (by writing a cheque from the company account). Those are two transactions. Each transaction is an edge in our graph, and the edge is labelled with the amount.</p>

<p>An edge always goes from one node to another. What are those nodes? Well, you can define them as you like (although there are some conventions). For now, let&#8217;s say:</p>
<p><a href='http://martin.kleppmann.com/2011/03/accounting1.dot.png'>
  <img alt='Graph representation of accounts' src='http://martin.kleppmann.com/2011/03/accounting1.dot.png' width='550' />
</a></p>
<p>Let&#8217;s add some more details. You pay the $5 credit card bill from the company account. And where did the money in the company account come from in the first place? Ah, I see, you put in $5,000 of your savings to start the company. Ok, now the graph looks like this:</p>
<p><a href='http://martin.kleppmann.com/2011/03/accounting2.dot.png'>
  <img alt='Graph representation of accounts' src='http://martin.kleppmann.com/2011/03/accounting2.dot.png' width='550' />
</a></p>
<p>Hopefully pretty self-explanatory so far. Money flows in the direction of the arrows.</p>

<p>Hungry once again, you go to the taqueria and buy a Super Burrito for $8 on the credit card. Now we could create another node for the taqueria, but this is starting to get messy &#8211; we don&#8217;t really care how much money we spent on bagels vs. how much on burritos. Let&#8217;s just lump them together as &#8220;food&#8221;. Also, &#8220;Random startup&#8221; is a bit unhelpful &#8211; I&#8217;ve already forgotten what those $500 were for. Let&#8217;s call it &#8220;furniture&#8221; instead.</p>
<p><a href='http://martin.kleppmann.com/2011/03/accounting3.dot.png'>
  <img alt='Graph representation of accounts' src='http://martin.kleppmann.com/2011/03/accounting3.dot.png' width='550' />
</a></p>
<p>See, that&#8217;s perfectly fine. We can have nodes which represent actual bank accounts or cards, others which represent people or companies, and others again which represent abstract categories like &#8220;food&#8221; or &#8220;furniture&#8221;. Just throw it all into the same graph.</p>

<p>Note also that you can have several edges between the same pair of nodes. You can keep track of the individual edges, or you can simply add them up. (Using the credit card, you spent a total of $13 on food.)</p>

<p><strong>Accounts have balances</strong></p>

<p>Every node in this graph is an <em>account</em> in accountant-speak (whether or not it is held by a bank), and every account has a <em>balance</em>. The balance is a single number for each account, and it is determined completely by the transactions in and out of the account:</p>

<ol>
<li>At the beginning of time, the value at each node is zero.</li>

<li>At each node, for each incoming edge, add the edge&#8217;s label to the node&#8217;s value; for each outgoing edge, subtract the edge&#8217;s label from the value.</li>
</ol>

<p>After you&#8217;ve processed all the edges, the value at each node is that account&#8217;s balance. Our graph now looks like this:</p>
<p><a href='http://martin.kleppmann.com/2011/03/accounting4.dot.png'>
  <img alt='Graph representation of accounts' src='http://martin.kleppmann.com/2011/03/accounting4.dot.png' width='550' />
</a></p>
<p>Note that the account balances have two nice properties:</p>

<ol>
<li>Because every transaction appears twice &#8211; once positive and once negative &#8211; the sum of all account balances is always zero.</li>

<li>If you partition the set of nodes into any two disjoint sets, and add up all of the balances in each set, then the sum for the one set is always the negative sum of the other set (because, after all, they have to add up to zero).</li>
</ol>

<p>These properties are useful for sanity-checking your numbers; if they are violated, <em>&#8220;ur doin it wrong&#8221;</em>. (This is what accountants mean when they talk about &#8220;balancing the books&#8221;.)</p>

<p><strong>Doing business</strong></p>

<p>Strengthened by a bagel and a burrito, you go out and talk to some potential customers. And hey, they love your product! It has a price tag of $5,000, and you sell it to two big enterprise customers. One pays you right away (good stuff!); the other gives you $2,500 up front, but insists that before they pay the rest, you need to implement that additional feature you foolishly promised.</p>

<p>So you received $5,000 + $2,500 in cash from your customers, wired straight the company bank account. Let&#8217;s add that to the graph:</p>
<p><a href='http://martin.kleppmann.com/2011/03/accounting5.dot.png'>
  <img alt='Graph representation of accounts' src='http://martin.kleppmann.com/2011/03/accounting5.dot.png' width='550' />
</a></p>
<p>But that&#8217;s not quite right. The price was $5,000 for each customer, and now it looks like you charged two different prices. How do we represent our arrangement with customer 2?</p>

<p>The solution is to deconstruct the deal into two separate transactions: the sale (in which the buyer agrees to buy, but no actual money changes hands) and the payment (when the cash actually hits your bank account). We can draw it like this:</p>
<p><a href='http://martin.kleppmann.com/2011/03/accounting6.dot.png'>
  <img alt='Graph representation of accounts' src='http://martin.kleppmann.com/2011/03/accounting6.dot.png' width='550' />
</a></p>
<p>See what I&#8217;ve done here? I&#8217;ve just made up a new node, generically called it &#8220;sales&#8221;, and added the actual $5,000 sales as a transaction from this &#8220;sales&#8221; account to the customer accounts. Adding this extra node hasn&#8217;t changed your bank balance.</p>

<p>This makes sense when you think about the intuitive meaning of the balances. The balance of each customer&#8217;s account is the amount they owe you: customer 1 has fully paid up (their incoming and outgoing transactions add up to the same), so their balance is zero; customer 2 has contractually agreed to give you $5,000, but has so far only given you half of that, so their balance is $2,500.</p>

<p>And the balance on the sales account is the value of stuff you&#8217;ve sold. Or rather, the negative value. That looks a bit weird&#8230; but I&#8217;ll come back to that later. (BTW, if you wanted to separately track sales for different customers or different products, no problem &#8212; just add whatever nodes make sense for you. Just make sure that every transaction appears only once as an edge, otherwise you&#8217;re making stuff up!)</p>

<p><strong>Finishing off the example</strong></p>

<p>To round it off, let me add some more events to the story (= some more edges to the graph).</p>

<p>Not only have you made some sales, but now you also receive a $20,000 investment from Y Combinator &#8212; congratulations! You and your co-founder can now afford to pay yourselves a salary. You take $8,000 out of the company account.</p>

<p>Then you get set up with a company accountant, and they talk lots of jargon at you. For some strange reason they are obsessed with correctly accounting for your office chair; they want it to depreciate over four years, i.e. its value is gradually reduced to zero over the course of that time. Fair enough, you say (even though you couldn&#8217;t care less what your chair will be worth in four years&#8217; time &#8212; surely by that time you&#8217;ll be the next Google or Facebook, and you&#8217;ll have other things to worry about than chairs).</p>

<p>The resulting graph now looks like this:</p>
<p><a href='http://martin.kleppmann.com/2011/03/accounting7.dot.png'>
  <img alt='Graph representation of accounts' src='http://martin.kleppmann.com/2011/03/accounting7.dot.png' width='550' />
</a></p>
<p>Note how I have represented the transactions:</p>

<ul>
<li>
<p>I have lumped together your founder investment with that of Y Combinator, under the heading of &#8220;capital&#8221;. Put simply, this is money you got into the company by selling your company&#8217;s shares, rather than by selling a product or service to a customer. As usual, you can split founders and YC into separate accounts if you feel like it.</p>
</li>

<li>
<p>I&#8217;ve represented payroll (salaries) as just money straight out of the bank account. In reality it&#8217;s a bit more complicated due to taxes, healthcare, benefits, etc. but the principles stay the same. It&#8217;s just more nodes and edges in the graph.</p>
</li>

<li>
<p>I made depreciation for one year (one quarter of $500 = $125) go <em>away</em> from the &#8220;furniture&#8221; account. Intuitively, this means that the balance of the &#8220;furniture&#8221; account is the value that your furniture still has now. Each year, you add another $125 edge from &#8220;furniture&#8221; to &#8220;depreciation&#8221;, until after four years, the balance of &#8220;furniture&#8221; drops to zero (assuming you haven&#8217;t bought any more chairs in the meantime, in your quest for world domination).</p>
</li>
</ul>

<p><strong>The profit and loss statement</strong></p>

<p>At this point, if you&#8217;re getting weary, I don&#8217;t blame you. But the good news: we&#8217;ve finished building our graph! Now I will show you how this graph representation maps to two standard financial statements most commonly used in managing a company: the profit and loss statement (&#8220;P&amp;L&#8221;), and the balance sheet. This is useful, because as a startup founder you&#8217;ll sooner or later have to discuss these documents with your investors/advisors, and so you might as well learn what the hell they mean.</p>

<p>In order to produce these statements, I need to get out the crayons. Here is the same graph as before, with the nodes coloured in:</p>
<p><a href='http://martin.kleppmann.com/2011/03/accounting8.dot.png'>
  <img alt='Graph representation of accounts' src='http://martin.kleppmann.com/2011/03/accounting8.dot.png' width='550' />
</a></p>
<p>Explaining the colours (putting the accounting terminology in brackets, since you&#8217;re likely to encounter these words):</p>

<ul>
<li>Green for <strong>stuff that you have</strong> (<em>&#8220;assets&#8221;</em>), e.g. money in the bank, or things which you bought and you could sell again, such as furniture. Also green for people/companies who <strong>owe you money</strong> (<em>&#8220;debtors&#8221;</em>, such as Customer 2), and people/companies to whom <strong>you owe money</strong> (<em>&#8220;liabilities&#8221;/&#8221;creditors&#8221;</em>, such as your upcoming credit card bill for that burrito).</li>

<li>Blue for <strong>sales of your product/service</strong> (<em>&#8220;revenue&#8221;</em>) and <strong>money you spent</strong> that you&#8217;re not going to get back (<em>&#8220;expenses&#8221;/&#8221;overheads&#8221;</em>). The office chair is green, because you could sell it again if you wanted to, but the bagel is blue, because once you&#8217;ve bought (and eaten) the bagel, that&#8217;s it &#8211; no going back.</li>

<li>Pink for <strong>money from investors</strong> (or yourself) that you got by selling shares (<em>&#8220;capital&#8221;</em>). (If you get a bank loan, that&#8217;s green, not pink, because you owe the bank to pay it back.)</li>
</ul>

<p>Every one of your nodes should fall into exactly one of these categories. If not, something has gone wrong, or you have discovered some bit of the accounting world that I don&#8217;t yet know about.</p>

<p>With these colours set, the profit and loss statement is simply <strong>a list of all the blue nodes</strong>, and the profit or loss of the company is the sum of all of the blue nodes&#8217; balances. The way we&#8217;ve calculated things, a negative value is a profit, and a positive value is a loss. That&#8217;s confusing, so you typically flip the sign when reporting the number (so that a profit is positive).</p>

<p>Written in the standard way, our P&amp;L looks like this:</p>
<table style='margin: 1.5em 0'>
  <tr>
    <th rowspan='2' style='font-variant: small-caps;'>Revenue</th>
    <td style='border-bottom: 1px solid #888;'>Sales</td>
    <td style='border-bottom: 1px solid #888; text-align: right;'>$10,000</td>
  </tr>
  <tr>
    <th style='border-bottom: 1px solid #888;'>Total revenue</th>
    <th style='border-bottom: 1px solid #888; text-align: right;'>$10,000</th>
  </tr>
  <tr style='height: 0.7em' />
  <tr style='margin-top: 1em'>
    <th rowspan='4' style='font-variant: small-caps;'>Expenses</th>
    <td>Payroll</td>
    <td style='text-align: right;'>$8,000</td>
  </tr>
  <tr>
    <td>Depreciation</td>
    <td style='text-align: right;'>$125</td>
  </tr>
  <tr>
    <td style='border-bottom: 1px solid #888;'>Food</td>
    <td style='border-bottom: 1px solid #888; text-align: right;'>$13</td>
  </tr>
  <tr>
    <th style='border-bottom: 1px solid #888;'>Total expenses</th>
    <th style='border-bottom: 1px solid #888; text-align: right;'>$8,138</th>
  </tr>
  <tr style='height: 0.7em' />
  <tr style='margin-top: 1em'>
    <th style='padding-top: 1em; font-variant: small-caps;'>Total</th>
    <th style='border-bottom: 1px solid #888; font-weight: bold;'>Profit/Loss</th>
    <th style='border-bottom: 1px solid #888; font-weight: bold; text-align: right;'>$1,862</th>
    <td>(= total revenue - total expenses)</td>
  </tr>
</table>
<p>The meaning is fairly intuitive. You sold $10,000 worth of stuff, and spent only $8,138 in the process, so you made $1,862 profit.</p>

<p>The profit and loss statement is calculated over a period of time (usually a month, a quarter or a year), and it&#8217;s often interesting to compare two different periods. To calculate it for a period, filter your transactions to only include those which occurred within that period, and add up the account balances for just those transactions.</p>

<p>One thing to watch out for: profit doesn&#8217;t say anything about your bank account. The bank account is a green node, but we&#8217;re only looking at blue nodes here. In this example, you ended up with $23,995 in the bank, even though investors put in $25,000: you made a profit, yet still have less money in the bank than you did before, because Customer 2 hasn&#8217;t yet fully paid. That&#8217;s why it&#8217;s possible for a company to be profitable but still run out of money!</p>

<p><strong>The Balance Sheet</strong></p>

<p>The balance sheet is a bit less intuitive than the P&amp;L, but it&#8217;s quite a powerful document. It summarises what the company currently has and doesn&#8217;t have, and why.</p>

<p>Remember what I said earlier about partitioning the nodes into two disjoint sets, and their summed balances adding to zero? That&#8217;s exactly what happens on the balance sheet. We take all of the nodes in the graph; on the one side we consider all of the green nodes, and on the other side all the blue and pink nodes. The sum of all of the blue and pink nodes&#8217; balances is minus the sum of all of the green nodes&#8217; balances.</p>

<p>Now, by convention, accountants flip the sign on all of the blue and pink nodes&#8217; balances, which means that the two sums end up being equal. And that&#8217;s why it&#8217;s called a balance sheet.</p>

<p>In our example, it looks like this:</p>
<table style='margin: 1.5em 0'>
  <tr>
    <th rowspan='4' style='font-variant: small-caps;'>Assets</th>
    <td>Bank account</td>
    <td style='text-align: right;'>$23,995</td>
  </tr>
  <tr>
    <td>Debtors</td>
    <td style='text-align: right;'>$2,500</td>
  </tr>
  <tr>
    <td style='border-bottom: 1px solid #888;'>Furniture</td>
    <td style='border-bottom: 1px solid #888; text-align: right;'>$375</td>
  </tr>
  <tr>
    <th style='border-bottom: 1px solid #888;'>Total assets</th>
    <th style='border-bottom: 1px solid #888; text-align: right;'>$26,870</th>
  </tr>
  <tr style='height: 0.7em' />
  <tr style='margin-top: 1em'>
    <th rowspan='2' style='font-variant: small-caps;'>Liabilities</th>
    <td style='border-bottom: 1px solid #888;'>Credit card</td>
    <td style='border-bottom: 1px solid #888; text-align: right;'>$8</td>
  </tr>
  <tr>
    <th style='border-bottom: 1px solid #888;'>Total liabilities</th>
    <th style='border-bottom: 1px solid #888; text-align: right;'>$8</th>
  </tr>
  <tr style='height: 0.7em' />
  <tr>
    <th colspan='2' style='border-bottom: 1px solid #888; font-weight: bold;'>Total assets less total liabilities</th>
    <th style='border-bottom: 1px solid #888; font-weight: bold; text-align: right;'>$26,862</th>
  </tr>
  <tr style='height: 0.7em' />
  <tr style='margin-top: 1em'>
    <th rowspan='3' style='padding-top: 1em; font-variant: small-caps;'>Equity</th>
    <td>Profit/Loss</td>
    <td style='text-align: right;'>$1,862</td>
  </tr>
  <tr>
    <td style='border-bottom: 1px solid #888;'>Capital</td>
    <td style='border-bottom: 1px solid #888; text-align: right;'>$25,000</td>
  </tr>
  <tr>
    <th style='border-bottom: 1px solid #888; font-weight: bold;'>Total equity</th>
    <th style='border-bottom: 1px solid #888; font-weight: bold; text-align: right;'>$26,862</th>
  </tr>
</table>
<p>The top block (assets and liabilities) corresponds to the green nodes in the graph, whilst the bottom block contains the pink node (capital) and the sum of all of the blue nodes. We already showed all of the detail for the blue nodes on the Profit and Loss statement above; on the balance sheet we can sum them all up to a single number.</p>

<p>Some more sign-flipping has occurred here: I&#8217;ve written liabilities, equity and P&amp;L with their signs flipped (which usually, but not always, has the effect of making the numbers positive). That doesn&#8217;t change anything fundamental about the graph structure, it just puts things into the conventional schema.</p>

<p>So how can you interpret the balance sheet? There are various things you can read from it. You can see how much money is in the bank, and how much of that money has already been promised to other people (liabilities). You can see how much of the money in the bank came from investors, vs. how much came from sales. And it shows how much money is due to come in soon, from sales that have closed but haven&#8217;t yet been fully paid.</p>

<p>The total of the balance sheet is a lower bound on the value of your company. It&#8217;s a very pessimistic figure &#8212; it assumes that your team, your technology, your brand etc. are all worth precisely nothing; if your company raises money from investors, your valuation will be much higher than the balance sheet figure, since that valuation includes the value of team, technology, brand etc in the form of a wild guess. In established companies you can find &#8220;intangible assets&#8221; on the balance sheet, but since they are very hard to value, I suspect it&#8217;s not worth bothering with unless you know what you are doing.</p>

<p>That&#8217;s the end of our whirlwind tour through the world of accounting. If you&#8217;re a real accountant reading this, please forgive my simplifications; if you spot any mistakes, please let me know. For everyone else, I hope this has been useful. To find out when I write something new, please <a href='http://twitter.com/martinkl'>follow me on Twitter</a> or put your email address in this box:</p>
<form action='http://rapportive.us2.list-manage.com/subscribe/post?u=9a1adaf549282981a96e171d1&amp;id=4543b695f6' class='validate' id='mc-embedded-subscribe-form' method='post' name='mc-embedded-subscribe-form' target='_blank'>
    <fieldset>
        <div class='mc-field-group'>
            <label for='mce-EMAIL'>Email:</label>
            <input class='required email' id='mce-EMAIL' name='EMAIL' type='text' value='' />
            <input class='btn' id='mc-embedded-subscribe' name='subscribe' type='submit' value='Subscribe' />
        </div>
        <div id='mce-responses'>
            <div class='response' id='mce-error-response' style='display:none' />
            <div class='response' id='mce-success-response' style='display:none' />
        </div>
    </fieldset>
</form><p class='disclaimer'>
    I won't give your address to anyone else, won't send you any spam, and you can unsubscribe at any time.
</p>
                ]]></content:encoded>
            </item>
        
            <item>
                <title>Having a launched product is hard</title>
                <link>http://martin.kleppmann.com/2010/12/21/having-a-launched-product-is-hard.html</link>
                <comments>http://martin.kleppmann.com/2010/12/21/having-a-launched-product-is-hard.html#disqus_thread</comments>
                <pubDate>Tue, 21 Dec 2010 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2010/12/21/having-a-launched-product-is-hard.html</guid>
                
                <description><![CDATA[ Over the last 6 months, we have been learning what it means to support a launched product. Some background: Rapportive launched in March 2010, and we joined Y Combinator for the Summer 2010 batch from June to August. Even though it was fantastic to have users and solid growth, it... ]]></description>
                <content:encoded><![CDATA[
                    <p>Over the last 6 months, <a href='http://rapportive.com'>we</a> have been learning what it means to support a launched product.</p>

<p>Some background: Rapportive <a href='http://blog.rapportive.com/the-accidental-launch'>launched</a> in March 2010, and we joined <a href='http://ycombinator.com/'>Y Combinator</a> for the Summer 2010 batch from June to August. Even though it was fantastic to have users and solid growth, it was actually a very frustrating time for us as a team.</p>

<p>We found the downside of having launched, namely that we ended up spending the entire 3 months of Y Combinator (and probably another month either side) doing the following:</p>

<ul>
<li>Answering many, many support emails and tweets</li>

<li>Raising <a href='http://techcrunch.com/2010/08/02/rapportive-funding/'>our seed round</a></li>

<li>Stopping our infrastructure from collapsing under our user growth</li>

<li>Responding to press and bloggers</li>

<li>Reading resumÃ©s and interviewing job candidates</li>

<li>Fixing gnarly bugs in production</li>

<li>Applying for visas, so that we could work in the US</li>

<li>Attending YC dinners and office hours</li>
</ul>

<p>By contrast, the ideal world of Y Combinator involves spending 3 months:</p>

<ul>
<li>Moving your product forward</li>

<li>Attending YC dinners and office hours</li>
</ul>

<p>Although all the things we were doing were valuable &#8212; even spending so much time on support is valuable, because we learnt a lot about our users, and we turned lots of people from angry strangers into enthusiastic supporters &#8212; it was incredibly frustrating. Our product development was almost stalled for months on end. And all the while, our YC batchmates were demoing new features every week, and getting a massive high from the productive flow of developing their products at a rapid pace.</p>

<p>So I am perhaps a bit envious of those who could move ahead rapidly and build their product without having to worry about supporting users or keeping their database alive. On the other hand, I am of course hugely grateful for our users who use and <a href='http://rapportive.com/buzz'>love our product</a> every day. I wouldn&#8217;t have it any other way.</p>
<p style='text-align: center'>
  <a href='http://adam.heroku.com/past/2008/4/23/the_startup_curve/'>
    <img alt='Photo of whiteboard at YC, showing curve of The Process' height='379' src='http://martin.kleppmann.com/2010/12/yc_whiteboard.jpg' width='500' />
  </a>
</p>
<p>Fortunately, we are not the first to experience this. At the YC office there is a whiteboard, now carrying somewhat iconic status, which reminds every single YC founder of <em>&#8220;The Process&#8221;</em>: once a startup has launched, the novelty will wear off, and the team will find itself in the <em>&#8220;Trough of Sorrow&#8221;</em>. We know what the Trough of Sorrow looks like. I have just described it. It&#8217;s not very much fun.</p>

<p>But here is good news: Rapportive now seems to have journeyed on to the next phase, we have made <a href='http://blog.rapportive.com/grow-your-network-with-rapportive'><em>&#8220;Releases of Improvement&#8221;</em></a>, and things are looking hopeful. Are we in those <em>&#8220;Wiggles of False Hope&#8221;</em>? Who knows. But hey, the money is in the bank, the visas are in our passports, the infrastructure has got a lot more robust, and most importantly, our product development is moving again. We have some really cool stuff coming soon. The hope is real. And we seem to have managed to avoid the <em>&#8220;Crash of Ineptitude&#8221;</em>.</p>

<p>So what have we learnt?</p>

<ul>
<li>
<p>Visibly iterating and improving the product is arguably the most important thing a startup should be doing, but sadly, other stuff has an uncanny ability to distract you away from product work. <a href='http://www.paulgraham.com/top.html'>Paul Graham has written</a> about money matters and disputes being particularly bad in this regard. That is true, and I would add server firefighting, recruitment and immigration to the list.</p>
</li>

<li>
<p>In order to get back into a flow of product development, we are now deliberately shunning distractions like recruitment and fundraising. We obviously can&#8217;t ignore these things forever, but for now it&#8217;s best for the business if we stay focussed on the thing we do best: making a <a href='http://www.paulgraham.com/good.html'>product that people want</a>. (We&#8217;re keeping one distraction, namely support, because it is so important. But we are rotating support duties so that most of the team can ignore it at any given moment.)</p>
</li>

<li>
<p>Be grateful if you carry a US passport.</p>
</li>

<li>
<p>Sometimes you just have to wade through a patch of mud, and there&#8217;s no way round it. But as long as you keep your eyes forward and keep moving, you&#8217;ll get through it, and things will brighten up.</p>
</li>
</ul>
<p style='text-align: center'>
  <a href='http://www.flickr.com/photos/performable/4792109640/sizes/l/in/set-72157623602121734/'>
    <img alt='Poster: We have a strategic plan. It&apos;s called doing things.' height='335' src='http://martin.kleppmann.com/2010/12/strategic_plan.jpg' width='500' />
  </a>
</p>
                ]]></content:encoded>
            </item>
        
    </channel>
</rss>
