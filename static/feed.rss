<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
    xmlns:content="http://purl.org/rss/1.0/modules/content/"
    xmlns:dc="http://purl.org/dc/elements/1.1/"
    xmlns:atom="http://www.w3.org/2005/Atom"
    xmlns:sy="http://purl.org/rss/1.0/modules/syndication/">

    <channel>
        <title>Martin Kleppmann's blog</title>
        <atom:link href="http://martin.kleppmann.com/feed.rss" rel="self" type="application/rss+xml" />
        <link>http://martin.kleppmann.com/</link>
        <description></description>
        <lastBuildDate>Mon, 25 Jan 2021 22:34:37 GMT</lastBuildDate>
        <language>en</language>
        <sy:updatePeriod>hourly</sy:updatePeriod>
        <sy:updateFrequency>1</sy:updateFrequency>

        
        
            <item>
                <title>Decentralised content moderation</title>
                <link>http://martin.kleppmann.com/2021/01/13/decentralised-content-moderation.html</link>
                <comments>http://martin.kleppmann.com/2021/01/13/decentralised-content-moderation.html#disqus_thread</comments>
                <pubDate>Wed, 13 Jan 2021 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2021/01/13/decentralised-content-moderation.html</guid>
                
                <description><![CDATA[ Who is doing interesting work on decentralised content moderation? With Donald Trump suspended from Twitter and Facebook, and Parler kicked off AWS, there is renewed discussion about what sort of speech is acceptable online, and how it should be enforced. Let me say up front that I believe that these... ]]></description>
                <content:encoded><![CDATA[
                    <p><strong>Who is doing interesting work on decentralised content moderation?</strong></p>

<p>With Donald Trump suspended from Twitter and Facebook, and
<a href="https://en.wikipedia.org/wiki/Parler">Parler</a> kicked off AWS, there is renewed discussion about
what sort of speech is acceptable online, and how it should be enforced. Let me say up front that
I believe that these bans were justified. However, they do raise questions that need to be
discussed, especially within the technology community.</p>

<p>As many have already pointed out, Twitter, Facebook and Amazon are corporations that are free to
enforce their terms of service in whatever way they see fit, within the bounds of applicable law
(e.g. anti-discrimination legislation). However, we should also realise that <em>almost all</em> social
media, the public spaces of the digital realm, are in fact privately owned spaces subject to
a corporation‚Äôs terms of service. There is currently no viable, non-corporate alternative space that
we could all move to. For better or for worse, Mark Zuckerberg, Jack Dorsey, and Jeff Bezos (and
their underlings) are, for now, the arbiters of what can and cannot be said online.</p>

<p>This situation draws attention to the <a href="https://redecentralize.org/">decentralised web community</a>,
a catch-all for a broad set of projects that are aiming to reduce the degree of centralised
corporate control in the digital sphere. This includes self-hosted/federated social networks such as
<a href="https://joinmastodon.org/">Mastodon</a> and <a href="https://diasporafoundation.org/">Diaspora</a>, peer-to-peer
social networks such as <a href="https://scuttlebutt.nz/">Scuttlebutt</a>, and miscellaneous blockchain
projects. The exact aims and technicalities of those projects are not important for this post.
I will start by focussing on one particular design goal that is mentioned by many decentralised web
projects, and that is <em>censorship resistance</em>.</p>

<h2 id="censorship-resistance">Censorship resistance</h2>

<p>When we think of censorship, we think of totalitarian states exercising violent control over their
population, crushing dissent and stifling the press. Against such an adversary, technologies that
provide censorship resistance seem like a positive step forward, since they promote individual
liberty and human rights.</p>

<p>However, often the adversary is not a totalitarian state, but other users. Censorship resistance
means that anybody can say anything, without suffering consequences. And unfortunately there are
a lot of people out there who say and do rather horrible things. Thus, as soon as
a censorship-resistant social network becomes sufficiently popular, I expect that it will be filled
with messages from spammers, neo-nazis, and child pornographers (or any other type of content that
you consider despicable). One person‚Äôs freedom from violence is another person‚Äôs censorship, and
thus, a system that emphasises censorship resistance will inevitably invite violence against some
people.</p>

<p>I fear that many decentralised web projects are designed for censorship resistance not so much
because they deliberately want to become hubs for neo-nazis, but rather out of a kind of naive
utopian belief that more speech is always better. But I think we have learnt in the last decade that
this is not the case. If we want technologies to help build the type of society that we want to live
in, then certain abusive types of behaviour must be restricted. Thus, content moderation is needed.</p>

<h2 id="the-difficulty-of-content-moderation">The difficulty of content moderation</h2>

<p>If we want to declare some types of content as unacceptable, we need a process for distinguishing
between acceptable and unacceptable material. But this is difficult. Where do you draw the line
between healthy scepticism and harmful conspiracy theory? Where do you draw the line between healthy
satire, using exaggeration for comic effect, and harmful misinformation? Between legitimate
disagreement and harassment? Between honest misunderstanding and malicious misrepresentation?</p>

<p>With all of these, some cases will be very clearly on one side or the other of the dividing line,
but there will always be a large grey area of cases that are unclear and a matter of subjective
interpretation. ‚Äú<a href="https://en.wikipedia.org/wiki/I_know_it_when_I_see_it">I know it when I see it</a>‚Äù
is difficult to generalise into a rule that can be applied objectively and consistently; and without
objectivity and consistency, moderation can easily degenerate into a situation where one group of
people forces their opinions on everyone else, like them or not.</p>

<p>In a service that is used around the world, there will be cultural differences on what is considered
acceptable or not. Maybe one culture is sensitive about nudity and tolerant of depictions of
violence, while another culture is liberal about nudity and sensitive about violence. One person‚Äôs
terrorist is another person‚Äôs freedom fighter. There is no single, globally agreed standard of what
is or is not considered acceptable.</p>

<p>Nevertheless, it is possible to come to agreement. For example, Wikipedia editors successfully
manage to agree on what should and should not be included in Wikipedia articles, even those on
contentious subjects. I won‚Äôt say that this process is perfect: Wikipedia editors are predominantly
white, male, and from the Anglo-American cultural sphere, so there is bound to be bias in their
editorial decisions. I haven‚Äôt participated in this community, but I assume the process of coming to
agreement is sometimes messy and will not make everybody happy.</p>

<p>Moreover, being an encyclopaedia, Wikipedia is focussed on widely accepted facts backed by evidence.
Attempting to moderate social media in the same way as Wikipedia would make it joyless, with no room
for satire, comedy, experimental art, or many of the other things that make it interesting and
humane. Nevertheless, Wikipedia is an interesting example of decentralised content moderation that
is not controlled by a private entity.</p>

<p>Another example is federated social networks such as Mastodon or Diaspora. Here, each individual
server administrator has the authority to
<a href="https://docs.joinmastodon.org/admin/moderation/">set the rules for the users of their server</a>, but
they have no control over activity on other servers (other than to block another server entirely).
Despite the decentralised architecture, there is a
<a href="https://arxiv.org/pdf/1909.05801.pdf">trend towards centralisation</a> (10% of Mastodon instances
account for almost half the users), leaving a lot of power in the hand of a small number of server
administrators. If these social networks are to go more mainstream, I expect these effects to be
amplified.</p>

<h2 id="filter-bubbles">Filter bubbles</h2>

<p>One form of social media is private chat for small groups, as provided e.g. by WhatsApp, Signal, or
even email. Here, when you post a message to a group, the only people who can see it are members of
that group. In this setting, not much content moderation is needed: group members can kick out other
members if they say things considered unacceptable. If one group says things that another group
considers objectionable, that‚Äôs no problem, because the two groups can‚Äôt see each other‚Äôs
conversations anyway. If one user is harassing another, the victim can block the harasser. Thus,
private groups are comparatively easy to deal with.</p>

<p>The situation is harder with social media that is public (anyone can read) and open (anyone can join
a conversation), or when the groups are very large. Twitter is an example of this model (and
Facebook to some degree, depending on your privacy settings). When anybody can write a message that
you will see (e.g. a reply to something you posted publicly), the door is opened to harassment and
abuse.</p>

<p>One response might be to retreat into our filter bubbles. For example, we could say that you see
only messages posted by your immediate friends and friends-of-friends. I am pretty sure that there
are no neo-nazis among my direct friends, and probably also among my second-degree network, so such
a rule would shield me from extremist content of one sort, at least.</p>

<p>It is also possible for users to collaborate on creating filters. For example,
<a href="https://github.com/freebsdgirl/ggautoblocker">ggautoblocker</a> was a tool to block abusive Twitter
accounts during <a href="https://en.wikipedia.org/wiki/Gamergate_controversy">GamerGate</a>, a 2014
misogynistic harassment campaign that
<a href="https://www.theguardian.com/technology/2016/dec/01/gamergate-alt-right-hate-trump">foreshadowed</a>
the rise of the alt-right and Trumpism. In the absence of central moderation by Twitter, victims of
this harassment could use this tool to automatically block a large number of harmful users so that
they wouldn‚Äôt have to see the abusive messages.</p>

<p>Of course, even though such filtering saves you from having to see things you don‚Äôt like, it doesn‚Äôt
stop the objectionable content from existing. Moreover, other people may have the opposite sort of
filter bubble in which they see <em>lots</em> of extremist content, causing them to become radicalised.
Personalised filters also stop us from seeing alternative (valid) opinions that would help broaden
our worldview and enable better mutual understanding of different groups in society.</p>

<p>Thus, subjective filtering of who sees what, such as blocking users, is an important part of
reducing harm on social media, but by itself it is not sufficient. It is also necessary to uphold
minimum standards on what can be posted at all, for example by requiring a baseline of civility and
truthfulness.</p>

<h2 id="democratic-content-moderation">Democratic content moderation</h2>

<p>I previously argued that there is no universally agreed standard of acceptability of content; and
yet, we must somehow keep the standard of discourse high enough that it does not become intolerable
for those involved, and to minimise the harms e.g. from harassment, radicalisation, and incitement
of violence. How do we solve this contradiction? Leaving the power in the hands of a small number of
tech company CEOs, or any other small and unelected group of people, does not seem like a good
long-term solution.</p>

<p>A purely technical solution does not exist either, since code cannot make value judgements about
what sort of behaviour is acceptable. It seems like some kind of democratic process is the only
viable long-term solution here, perhaps supported by some technological mechanisms, such as
AI/machine learning to flag potentially abusive material. But what might this democratic process
look like?</p>

<p>Moderation should not be so heavy-handed that it drowns out legitimate disagreement. Disagreement
need not always be polite; indeed,
<a href="https://everydayfeminism.com/2015/12/tone-policing-and-privilege/">tone policing</a> should not be
a means of silencing legitimate complaints. On the other hand, aggressive criticism may quickly flip
into the realm of harassment, and it may be unclear when exactly this line has been crossed.
Sometimes it may be appropriate to take into account the power relationships between the people
involved, and hold the privileged and powerful to a higher standard than the oppressed and
disadvantaged, since otherwise the system may end up reinforcing existing imbalances. But there are
no hard and fast rules here, and much depends on the context and background of the people involved.</p>

<p>This example indicates that the moderation process needs to embed ethical principles and values. One
way of doing this would be to have a board of moderation overseers that is elected by the user base.
In their manifesto, candidates for this board can articulate the principles and values that they
will bring to the job. Different candidates may choose to represent people with different world
views, such as conservatives and liberals. Having a diverse set of opinions and cultures represented
on such a board would both legitimise its authority and improve the quality of its decision-making.
In time, maybe even parties and factions may emerge, which I would regard as a democratic success.</p>

<p>Facebook employs
<a href="https://bhr.stern.nyu.edu/tech-content-moderation-june-2020">around 15,000 content moderators</a>, and
on all accounts it‚Äôs
<a href="https://www.theverge.com/2019/2/25/18229714/cognizant-facebook-content-moderator-interviews-trauma-working-conditions-arizona">a horrible job.</a>
Who would want to do it? On the other hand, 15,000 is a tiny number compared to Facebook‚Äôs user
count. Rather than concentrating all the content moderation work on a comparatively small number of
moderators, maybe every user should have to do a stint at moderation from time to time as part of
their conditions for using a service? Precedents for this sort of thing exist: in a number of
countries, individuals may be called to jury duty to help decide criminal cases; and researchers are
regularly asked to review articles written by their peers. These things are not great fun either,
but we do them for the sake of the civic system that we all benefit from.</p>

<p>Moderators with differing political views may disagree on whether a certain piece of content is
acceptable or not. In cases of such disagreement, additional people can be brought in, hopefully
allowing the question to be settled through debate. If no agreement can be found, the matter can be
escalated to the elected board, which has the final say and which uses the experience to set
guidelines for future moderation.</p>

<h2 id="implications-for-decentralised-technologies">Implications for decentralised technologies</h2>

<p>In decentralised social media, I believe that ultimately it should be the users themselves who
decide what is acceptable or not. This governance will have to take place through some human process
of debate and deliberation, although technical tools and some degree of automation may be able to
support the process and make it more efficient. Rather than simplistic censorship resistance, or
giving administrators dictatorial powers, we should work towards ethical principles, democratic
control, and accountability.</p>

<p>I realise that my proposals are probably naive and smack of ‚Äúcomputer scientist finally discovers
why the humanities are important‚Äù. Therefore, if you know of any work that is relevant to this topic
and can help technological systems learn from centuries of experience in democracy in the civil
society, please send it to me ‚Äî I am keen to learn more. Moreover, if there is existing work in the
decentralised web community on enabling this kind of grassroots democracy, I would love to hear
about it too.</p>

<p>You can find me on Twitter <a href="https://twitter.com/martinkl">@martinkl</a>, or contact me by email
(firstname at lastname dot com). I will update this post with interesting things that are sent to
me.</p>

<h2 id="updates-related-work">Updates: related work</h2>

<p>Here are some related projects that have been pointed out to me since this post was published. I
have not vetted them, so don‚Äôt take this as an endorsement.</p>

<ul>
  <li>The <a href="https://oversightboard.com/">Facebook/Instagram Oversight Board</a> is quite close to what
I have in mind, and I am looking forward to seeing how it works out.</li>
  <li>The recently launched
<a href="https://news.mit.edu/2021/center-constructive-communication-0113">MIT Center for Constructive Communication</a>
is an ambitious effort in this area.</li>
  <li>‚Äú<a href="https://foundation.mozilla.org/en/blog/fellow-research-decentralized-web-hate/">The Decentralized Web of Hate</a>‚Äù
is a detailed report by <a href="http://emmibevensee.com/">Emmi Bevensee</a> on use of decentralised
technologies by extremists.</li>
  <li><a href="https://homes.cs.washington.edu/~axz/publications.html">Amy X. Zhang</a> and her collaborators have
done a lot of research on moderation.</li>
  <li><a href="https://twitter.com/arcalinea">Jay Graber</a> recently published a comprehensive
<a href="https://twitter.com/arcalinea/status/1352316972654944257">report comparing decentralised social protocols</a>, and a
<a href="https://jaygraber.medium.com/designing-decentralized-moderation-a76430a8eab">blog post</a>
on decentralised content moderation.</li>
  <li>A few <a href="https://twitter.com/xmal/status/1349413781953273857">people</a>
<a href="https://twitter.com/weschow/status/1349417270179737604">mentioned</a> Slashdot, Reddit, and Stack Overflow
as successful examples of community-run moderation.</li>
  <li><a href="https://cblgh.org/articles/trustnet.html">Trustnet</a> is a way of computing numerical scores for
the degree of trust in indvidual users, based on the social graph.</li>
  <li><a href="https://matrix.org/">Matrix</a>, a federated messaging system, is
<a href="https://matrix.org/blog/2020/10/19/combating-abuse-in-matrix-without-backdoors">working on</a> a
decentralised, subjective reputation system.</li>
  <li><a href="https://github.com/Freechains/README">Freechains</a> is a peer-to-peer content distribution
protocol with an embedded user reputation system.</li>
  <li><a href="https://cabal.chat/">Cabal</a> allows users to
<a href="https://twitter.com/substack/status/1349471659653124098">subscribe</a> to other users‚Äô moderation
actions, such as blocking and hiding posts.</li>
  <li>An app called <a href="https://kc-fantastic-app.medium.com/decentralized-content-moderation-on-fantastic-app-3768989ced19">Fantastic</a>
is exploring mechanisms for moderation.</li>
  <li>Felix Dietze‚Äôs <a href="https://github.com/fdietze/notes/blob/master/felix_dietze_master_thesis_2015.pdf">2015 master‚Äôs thesis</a>
explores community-run moderation. He is also working on
<a href="https://felix.unote.io/hacker-news-scores">ranking</a>
<a href="https://github.com/fdietze/downvote-scoring">algorithms</a>
for news aggregators.</li>
</ul>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Using Bloom filters to efficiently synchronise hash graphs</title>
                <link>http://martin.kleppmann.com/2020/12/02/bloom-filter-hash-graph-sync.html</link>
                <comments>http://martin.kleppmann.com/2020/12/02/bloom-filter-hash-graph-sync.html#disqus_thread</comments>
                <pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2020/12/02/bloom-filter-hash-graph-sync.html</guid>
                
                <description><![CDATA[ This blog post uses MathJax to render mathematics. You need JavaScript enabled for MathJax to work. In some recent research, Heidi and I needed to solve the following problem. Say you want to sync a hash graph, such as a Git repository, between two nodes. In Git, each commit is... ]]></description>
                <content:encoded><![CDATA[
                    <p><em>This blog post uses <a href="https://www.mathjax.org/">MathJax</a> to render mathematics. You need JavaScript enabled for MathJax to work.</em></p>

<p>In some recent research, <a href="http://heidihoward.co.uk/">Heidi</a> and I needed to solve the following problem.
Say you want to sync a hash graph, such as a Git repository, between two nodes.
In Git, each commit is identified by its hash, and a commit may include the hashes of predecessor commits (a commit may include more than one hash if it‚Äôs a merge commit).
We want to figure out the minimal set of commits that the two nodes need to send to each other in order to make their graphs the same.</p>

<p>You might wonder: isn‚Äôt this a solved problem?
Git has to do this every time you do <code>git pull</code> or <code>git push</code>!
You‚Äôre right, and some cases are easy, but other cases are a bit trickier.
What‚Äôs more, the algorithm used by Git is not particularly well-documented, and in any case we think that we can do better.</p>

<p>For example, say we have two nodes, and each has one of the following two hash graphs (circles are commits, arrows indicate one commit referencing the hash of another).
The blue part (commit A and those to the left of it) is shared between the two graphs, while the dark grey and light grey parts exist in only one of the two graphs.</p>

<p><a href="/2020/12/hash-dag.png"><img src="/2020/12/hash-dag.png" width="550" height="258" alt="Illustration of two hash graphs" /></a></p>

<p>We want to reconcile the two nodes‚Äô states so that one node sends all of the dark-grey-coloured commits, the other sends all of the light-grey-coloured commits, and both end up with the following graph:</p>

<p><a href="/2020/12/hash-dag2.png"><img src="/2020/12/hash-dag2.png" width="550" height="143" alt="Hash graph after reconciliation" /></a></p>

<p>How do we efficiently figure out which commits the two nodes need to send to each other?</p>

<h2 id="traversing-the-graph">Traversing the graph</h2>

<p>First, some terminology.
Let‚Äôs say commit A is a <em>predecessor</em> of commit B if B references the hash of A, or if there is some chain of hash references from B leading to A.
If A is a predecessor of B, then B is a <em>successor</em> of A.
Finally, define the <em>heads</em> of the graph to be those commits that have no successors.
In the example above, the heads are B, C, and D.
(This is slightly different from how Git defines <code>HEAD</code>.)</p>

<p>The reconciliation algorithm is easy if it‚Äôs a ‚Äúfast-forward‚Äù situation: that is, if one node‚Äôs heads are commits that the other node already has.
In that case, one node sends the other the hashes of its heads, and the other node replies with all commits that are successors of the first node‚Äôs heads.
However, the situation is tricker in the example above, where one node‚Äôs heads B and C are unknown to the other node, and likewise head D is unknown to the first node.</p>

<p>In order to reconcile the two graphs, we want to figure out which commits are the latest common predecessors of both graphs‚Äô heads (also known as <em>common ancestors</em>, marked A in the example), and then the nodes can send each other all commits that are successors of the common predecessors.</p>

<p>As a first attempt, we can try this: the two nodes send each other their heads; if those contain any unknown predecessor hashes, they request those, and repeat until all hashes resolve to known commits.
Thus, the nodes gradually work their way from the heads towards the common predecessors.
This works, but it is slow if your graph contains long chains of commits, since the number of round trips required equals the length of the longest path from a head to a common predecessor.</p>

<p>The ‚Äúsmart‚Äù transfer protocol used by Git essentially <a href="https://www.git-scm.com/docs/http-protocol">works like this</a>, except that it sends 32 hashes at a time in order to reduce the number of round trips.
Why 32? Who knows.
It‚Äôs a trade-off: send more hashes to reduce the number of round trips, but each request/response is bigger.
Presumably they decided that 32 was a reasonable compromise between latency and bandwidth.</p>

<p>Recent versions of Git also support an experimental <a href="https://github.com/git/git/commit/42cc7485a2ec49ecc440c921d2eb0cae4da80549">‚Äúskipping‚Äù algorithm</a>, which can be enabled using the <a href="https://git-scm.com/docs/git-config#Documentation/git-config.txt-fetchnegotiationAlgorithm"><code>fetch.negotiationAlgorithm</code> config option</a>.
Rather than moving forward by a fixed number of predecessors in each round trip, this algorithm allows some commits to be skipped, so that it reaches the common predecessors faster.
The skip size grows similarly to the Fibonacci sequence (i.e. exponentially) with each round trip.
This reduces the number of round trips to \(O(\log n)\), but you can end up overshooting the common predecessors, and thus the protocol may end up unnecessarily transmitting commits that the other node already has.</p>

<h2 id="bloom-filters-to-the-rescue">Bloom filters to the rescue</h2>

<p>In our new paper draft, which we are <a href="https://arxiv.org/abs/2012.00472">making available on arXiv today</a>, Heidi and I propose a different algorithm for performing this kind of reconciliation.
It is quite simple if you know how <a href="https://en.wikipedia.org/wiki/Bloom_filter">Bloom filters</a> work.</p>

<p>In addition to sending the hashes of their heads, each node constructs a Bloom filter containing the hashes of the commits that it knows about.
In our prototype, we allocate 10 bits (1.25 bytes) per commit.
This number can be adjusted, but note that it is a lot more compact than sending the full 16-byte (for SHA-1, used by Git) or 32-byte (for SHA-256, which is more secure) hash for each commit.
Moreover, we keep track of the heads from the last time we reconciled our state with a particular node, and then the Bloom filter only needs to include commits that were added since the last reconciliation.</p>

<p>When a node receives such a Bloom filter, it checks its own commit hashes to see whether they appear in the filter.
Any commits whose hash does not appear in the Bloom filter, and its successors, can immediately be sent to the other node, since we can be sure that the other node does not know about those commits.
For any commits whose hash does appear in the Bloom filter, it is likely that the other node knows about that commit, but due to false positives it is possible that the other node actually does not know about those commits.</p>

<p>After receiving all the commits that did not appear in the Bloom filter, we check whether we know all of their predecessor hashes.
If any are missing, we request them in a separate round trip using the same graph traversal algrorithm as before.
Due to the way the false positive probabilities work, the probability of requiring n round trips decreases exponentially as n grows.
For example, you might have a 1% chance of requiring two round trips, a 0.01% chance of requiring three round trips, a 0.0001% chance of requiring four round trips, and so on.
Almost all reconciliations complete in one round trip.</p>

<p>Unlike the skipping algorithm used by Git, our algorithm never unnecessarily sends any commits that the other side already has, and the Bloom filters are very compact, even for large commit histories.</p>

<h2 id="practical-relevance">Practical relevance</h2>

<p>In the paper we also prove that this algorithm allows nodes to sync their state even in the presence of arbitrarily many malicious nodes, making it immune to <a href="https://en.wikipedia.org/wiki/Sybil_attack">Sybil attacks</a>.
We then go on to prove a theorem that shows which types of applications can and cannot be implemented in this Sybil-immune way, without requiring any Sybil countermeasures such as <a href="https://en.wikipedia.org/wiki/Proof_of_work">proof-of-work</a> or the centralised control of <a href="https://arxiv.org/pdf/1711.03936.pdf">permissioned blockchains</a>.</p>

<p>All of this is directly relevant for <a href="https://www.inkandswitch.com/local-first.html">local-first</a> peer-to-peer applications in which apps running on different devices need to sync up their state without necessarily trusting each other or relying on any trusted servers.
I assume it‚Äôs also relevant for <a href="https://www.swirlds.com/downloads/SWIRLDS-TR-2016-01.pdf">blockchains that use hash graphs</a>, but I don‚Äôt know much about them.
So, syncing a Git commit history is just one of many possible use cases ‚Äì I just used it because most developers will be at least roughly familiar with it!</p>

<p>The details of the algorithm and the theorems are in the <a href="https://arxiv.org/abs/2012.00472">paper</a>, so I won‚Äôt repeat them here.
Instead, I will briefly mention a few interesting things that didn‚Äôt make it into the paper.</p>

<h2 id="why-bloom-filters">Why Bloom filters?</h2>

<p>One thing you might be wondering: rather than creating a Bloom filters with 10 bits per commit, can we not just truncate the commit hashes to 10 bits and send those instead?
That would use the same amount of network bandwidth, and intuitively it may seem like it should be equivalent.</p>

<p>However, that is not the case: Bloom filters perform vastly better than truncated hashes.
I will use a small amount of probability theory to explain why.</p>

<p>Say we have a hash graph containing \(n\) distinct items, and we want to use \(b\) bits per item (so the total size of the data structure is \(m=bn\) bits).
If we are using truncated hashes, there are \(2^b\) possible values for each \(b\)-bit hash.
Thus, given two independently chosen, uniformly distributed hashes, the probability that they are the same is \(2^{-b}\).</p>

<p>If we have \(n\) uniformly distributed hashes, the probability that they are all different from a given \(b\)-bit hash is \((1-2^{-b})^n\).
The false positive probability is therefore the probability that a given \(b\)-bit hash equals one or more of the \(n\) hashes:</p>

<p>\[ P(\text{false positive in truncated hashes}) = 1 - (1 - 2^{-b})^n \]</p>

<p>On the other hand, with a Bloom filter, we start out with all \(m\) bits set to zero, and then for each item, we set \(k\) bits to one.
After one uniformly distributed bit-setting operation, the probability that a given bit is zero is \(1 - 1/m\).
Thus, after \(kn\) bit-setting operations, the probability that a given bit is still zero is \((1 - 1/m)^{kn}\).</p>

<p>A Bloom filter has a false positive when we check \(k\) bits for some item and they are all one, even though that item was not in the set.
The probability of this happening is</p>

<p>\[ P(\text{false positive in Bloom filter}) = (1 - (1 - 1/m)^{kn})^k \]</p>

<p>It‚Äôs not obvious from those expressions which of the two is better, so I plotted the false positive probabilities of truncated hashes and Bloom filters for varying numbers of items \(n\), and with parameters \(b=10\), \(k=7\), \(m=bn\):</p>

<p><a href="/2020/12/false-pos.png"><img src="/2020/12/false-pos.png" width="550" height="200" alt="Plot of false positive probability for truncated hashes and Bloom filters" /></a></p>

<p>For a Bloom filter, as long as we grow the size of the filter proportionally to the number of items (here we have 10 bits per item), the false positive probability remains pretty much constant at about 0.8%.
But truncated hashes of the same size behave much worse, and with more than about 1,000 items the false positive probability exceeds 50%.</p>

<p>The reason for this: with 10-bit truncated hashes there are only 1,024 possible hash values, and if we have 1,000 different items, then most of those 1,024 possible values are already taken.
With truncated hashes, if we wanted to keep the false positive probability constant, we would have to use more bits per item as the number of items grows, so the total size of the data structure would grow faster than linearly in the number of items.</p>

<p>Viewing it like this, it is quite remarkable that Bloom filters work as well as they do, using only a constant number of bits per item!</p>

<h2 id="further-details">Further details</h2>

<p>The Bloom filter false positive formula given above is the one that is commonly quoted, but it‚Äôs actually not quite correct.
To be precise, it is a <a href="https://www.sciencedirect.com/science/article/abs/pii/S0020019008001579">lower bound</a> on the exact false positive probability (<a href="https://git.gnunet.org/bibliography.git/plain/docs/FalsepositiverateBloomFilter2008Bose.pdf">open access paper</a>).</p>

<p>Out of curiosity I wrote a <a href="https://gist.github.com/ept/83b91aa07e2495c86ddd8c364a8cfbc7">little Python script</a> that calculates the false positive probability for truncated hashes, Bloom filters using the approximate formula, and Bloom filters using the exact formula.
Fortunately, for the parameter values we are interested in, the difference between approximate and exact probability is very small.
The <a href="https://gist.github.com/ept/83b91aa07e2495c86ddd8c364a8cfbc7">gist</a> also contains a <a href="http://www.gnuplot.info/">Gnuplot</a> script to produce the graph above.</p>

<p><a href="https://twitter.com/pvh">Peter</a> suggested that a <a href="https://en.wikipedia.org/wiki/Cuckoo_filter">Cockoo filter</a> may perform even better than a Bloom filter, but we haven‚Äôt looked into that yet.
To be honest, the Bloom filter approach already works so well, and it‚Äôs so simple, that I‚Äôm not sure the added complexity of a more sophisticated data structure would really be worth it.</p>

<p>That‚Äôs all for today.
Our paper is at <a href="https://arxiv.org/abs/2012.00472">arxiv.org/abs/2012.00472</a>.
Hope you found this interesting, and please let us know if you end up using the algorithm!</p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>New courses on distributed systems and elliptic curve cryptography</title>
                <link>http://martin.kleppmann.com/2020/11/18/distributed-systems-and-elliptic-curves.html</link>
                <comments>http://martin.kleppmann.com/2020/11/18/distributed-systems-and-elliptic-curves.html#disqus_thread</comments>
                <pubDate>Wed, 18 Nov 2020 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2020/11/18/distributed-systems-and-elliptic-curves.html</guid>
                
                <description><![CDATA[ I have just published new educational materials that might be of interest to computing people: a new 8-lecture course on distributed systems, and a tutorial on elliptic curve cryptography. Distributed Systems Since last year I have been delivering an 8-lecture undergraduate course on distributed systems at the University of Cambridge.... ]]></description>
                <content:encoded><![CDATA[
                    <p>I have just published new educational materials that might be of interest to computing people:
a new 8-lecture course on distributed systems, and a tutorial on elliptic curve cryptography.</p>

<h2 id="distributed-systems">Distributed Systems</h2>

<p>Since last year I have been delivering an 8-lecture undergraduate course on distributed systems at the University of Cambridge.
The first time I delivered it, I inherited the slides and exercises from the people who lectured it in previous years (Richard Mortier, Anil Madhavapeddy, Robert Watson, Jean Bacon, and Steven Hand), and I just used those materials with minor modifications.
It was a good course, but it was getting quite dated (e.g. lots of material on <a href="https://en.wikipedia.org/wiki/Common_Object_Request_Broker_Architecture">CORBA</a>, which is now of mostly historical interest).</p>

<p>Therefore, this year I decided to do a thorough refresh of the course content, and wrote a brand new set of slides and lecture notes.
Also, due to the pandemic we are not having any in-person lectures, so I recorded videos for all of the lectures.
I decided to make all of this available publicly under a <a href="https://creativecommons.org/licenses/by-sa/4.0/">creative commons CC BY-SA license</a>, which means that you‚Äôre welcome to use it freely (including incorporating it into your own work), provided that you give credit to me, and that you share your derived work under the same license.</p>

<p>The result is here:</p>

<ul>
  <li><a href="https://www.cl.cam.ac.uk/teaching/2021/ConcDisSys/dist-sys-notes.pdf">Lecture notes (PDF)</a> (including exercises)</li>
  <li>Slides: <a href="https://www.cl.cam.ac.uk/teaching/2021/ConcDisSys/dist-sys-slides.pdf">slideshow</a> and <a href="https://www.cl.cam.ac.uk/teaching/2021/ConcDisSys/dist-sys-handout.pdf">printable</a> (PDF)</li>
  <li><a href="https://www.youtube.com/playlist?list=PLeKd45zvjcDFUEv_ohr_HdUFe97RItdiB">Lecture videos (YouTube)</a></li>
  <li><a href="https://www.cst.cam.ac.uk/teaching/2021/ConcDisSys">Course web page</a></li>
  <li>Solution notes for the exercises are available on demand (<a href="/contact.html">email me</a> and convince me that you‚Äôre not a student trying to cheat).
Cambridge supervisors can <a href="https://www.cl.cam.ac.uk/teaching/2021/ConcDisSys/supervisors/dist-sys-solutions.pdf">download the solution notes directly</a> (Raven login required).</li>
</ul>

<p>The course is primarily designed for Cambridge undergraduate students, and it includes some cross-references to other courses.
Many other courses also make their notes or slides publicly available, so you can still look them up if you‚Äôre not at Cambridge by going to the <a href="https://www.cl.cam.ac.uk/teaching/2021/part1b-75.html">course web pages</a>.
(Many lecturers restrict their video recordings to Cambridge users only, so those might not be publicly available.)</p>

<p>The distributed systems course comprises about 7 hours of video and 87 pages of lecture notes.
It covers the following topics:</p>

<ol>
  <li>Introduction: distributed systems, computer networks, and RPC</li>
  <li>System models: network faults, crash and Byzantine faults, synchrony assumptions</li>
  <li>Physical clocks, clock synchronisation, and causality</li>
  <li>Logical time, broadcast protocols (reliable, FIFO, causal, total order)</li>
  <li>Replication, quorum protocols, state machine replication</li>
  <li>Consensus, details on the Raft consensus algorithm</li>
  <li>Replica consistency, two-phase commit, linearizability, eventual consistency</li>
  <li>Case studies: collaboration software, Google‚Äôs Spanner</li>
</ol>

<p>The main focus of this course is on understanding the algorithms and the principles that allow us to build robust and reliable distributed systems.
It uses examples of practical systems as motivation, and the videos include a few live demos of real distributed systems in action.
The aim is to convey the fundamentals without being excessively theoretical; there are a few mathematical proofs in the exercises, but most of the discussion is informal and example-based.</p>

<p>The level of this course is intended for second-year undergraduates.
Our students at this level have reasonable fluency with mathematical notation, and some background in programming languages and operating systems, so that‚Äôs what this course assumes.</p>

<h2 id="elliptic-curve-cryptography">Elliptic Curve Cryptography</h2>

<p>Another document I‚Äôm releasing today is called
<a href="https://martin.kleppmann.com/papers/curve25519.pdf">Implementing Curve25519/X25519: A Tutorial on Elliptic Curve Cryptography</a>.
There‚Äôs no video for this one, just a 30-page PDF.</p>

<p>Many textbooks cover the concepts behind Elliptic Curve Cryptography (ECC), but few explain how to go from the equations to a working, fast, and secure implementation.
On the other hand, while the code of many cryptographic libraries is available as open source, it can be <a href="https://github.com/jedisct1/libsodium/blob/master/src/libsodium/crypto_scalarmult/curve25519/ref10/x25519_ref10.c#L91-L132">rather opaque to the untrained eye</a>, and it is rarely accompanied by detailed documentation explaining how the code came about and why it is correct.</p>

<p>This tutorial bridges the gap between the mathematics and implementation of elliptic curve cryptography.
It is written for readers who are new to cryptography, and it assumes no more mathematical background than most undergraduate computer science courses.
Starting from first principles, this document shows how to derive every line of code in an implementation of the <a href="https://tools.ietf.org/html/rfc7748">X25519</a> Diffie-Hellman key agreement scheme, based on the <a href="https://ianix.com/pub/curve25519-deployment.html">widely-used Curve25519 elliptic curve</a>.
The implementation is based on Dan Bernstein et al.‚Äôs <a href="https://tweetnacl.cr.yp.to/">TweetNaCl</a>.
It is fast and secure; in particular, it uses constant-time algorithms to prevent side-channel attacks.</p>

<p>I wrote this because I wanted to learn how real implementations of ECC work, but I couldn‚Äôt find good resources that explained it, so I wrote the document as I figured it out step-by-step from a number of sources (and by doing a lot of the calculations myself).
I hope others will also find it useful.</p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Writing a book: is it worth it?</title>
                <link>http://martin.kleppmann.com/2020/09/29/is-book-writing-worth-it.html</link>
                <comments>http://martin.kleppmann.com/2020/09/29/is-book-writing-worth-it.html#disqus_thread</comments>
                <pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2020/09/29/is-book-writing-worth-it.html</guid>
                
                <description><![CDATA[ My book, Designing Data-Intensive Applications, recently passed the milestone of 100,000 copies sold. Last year, it was the second-best-selling book in O‚ÄôReilly‚Äôs entire catalogue, second only to Aur√©lien G√©ron‚Äôs machine learning book. Machine learning is obviously a hot topic, so I am quite content with coming second to it! üòÑ... ]]></description>
                <content:encoded><![CDATA[
                    <p>My book, <a href="https://dataintensive.net/">Designing Data-Intensive Applications</a>, recently passed the
milestone of 100,000 copies sold. Last year, it was the second-best-selling book in O‚ÄôReilly‚Äôs
entire catalogue, second only to
<a href="https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646/ref=as_li_ss_tl?ie=UTF8&amp;linkCode=ll1&amp;tag=dataintensive-20&amp;linkId=7d47ed0da85dc67659afbfbbad99f6ec&amp;language=en_US">Aur√©lien G√©ron‚Äôs machine learning book</a>.
Machine learning is obviously a hot topic, so I am quite content with coming second to it! üòÑ</p>

<p>To me, the success of this book was totally unexpected: while I was writing it, I thought that it
was going to be a bit niche, and I set myself the goal of selling 10,000 copies over the lifetime
of the book. Having passed that goal tenfold, this seems like a good opportunity to look back and
reflect on the process. I don‚Äôt want to make this post too self-congratulatory, but rather I will
try to share some insights into the business of book-writing.</p>

<h2 id="is-it-financially-worth-it">Is it financially worth it?</h2>

<p>Most books make very little money for both authors and publishers, but then occasionally something
like <em>Harry Potter</em> comes along. If you are considering writing a book, I strongly recommend that
you estimate the value of your future royalties to be close to zero. Like starting a band with
friends and hoping to become rock stars, it‚Äôs difficult to predict in advance what will be a hit and
what will flop. Maybe this applies less to technical books than to fiction and music, but I suspect
that even with technical books, there are a small number of hits, and most books sell quite modest
numbers.</p>

<p>That said, in my case, I am happy to report that writing this book has in retrospect turned out to
be a financially sound decision. These graphs show the royalties I have been paid since the book
first went on sale:</p>

<p><img src="/2020/09/royalties-cumulative.png" width="550" height="311" alt="Cumulative royalties chart" /></p>

<p><img src="/2020/09/royalties-monthly.png" width="550" height="311" alt="Monthly royalties chart" /></p>

<p>For the first 2¬Ω years the book was in ‚Äúearly release‚Äù: during this period I was still writing, and
we released it in unedited form, one chapter at a time, as ebook only. Then in March 2017 the book
was officially published, and the print edition went on sale. Since then, the sales have fluctuated
from month to month, but on average they have stayed remarkably constant. At some point I would
expect the market to become saturated (i.e. most people who were going to buy the book have already
bought it), but that does not seem to have happened yet: indeed, sales noticeably increased in late
2018 (I don‚Äôt know why). The x axis ends in July 2020 because from the time of sale, it takes
a couple of months for the money to trickle through the system.</p>

<p>My contract with the publisher specifies that I get 25% of publisher revenue from ebooks, online
access, and licensing, 10% of revenue from print sales, and 5% of revenue from translations. That‚Äôs
a percentage of the wholesale price that retailers/distributors pay to the publisher, so it doesn‚Äôt
include the retailers‚Äô markup. The figures in this section are the royalties I was paid, after the
retailer and publisher have taken their cut, but before tax.</p>

<p>The total sales since the beginning have been (in US dollars):</p>

<ul>
  <li>Print: 68,763 copies, $161,549 royalties ($2.35/book)</li>
  <li>Ebook: 33,420	copies, $169,350 royalties ($5.07/book)</li>
  <li>O‚ÄôReilly online access (formerly called Safari Books Online): $110,069 royalties
(I don‚Äôt get readership numbers for this channel)</li>
  <li><a href="http://dataintensive.net/translations.html">Translations</a>: 5,896 copies, $8,278 royalties ($1.40/book)</li>
  <li>Other licensing and sponsorship: $34,600 royalties</li>
  <li>Total: 108,079 copies, $477,916</li>
</ul>

<p>A lot of money, but I also put a lot of time into it! I estimate that I spent about 2.5 years of
full-time equivalent work researching and writing the book, spread out over the course of 4 years.
Of that time, I spent one year (2014‚Äì15) working full-time on the book without income, while the
rest of the time I worked on the book part-time alongside part-time employment.</p>

<p>Now, in retrospect, it turns out that those 2.5 years were a good investment, because the income
that this work has generated is in the same ballpark as the Silicon Valley software engineering
salary (including stock and benefits) I could have received in the same time if I hadn‚Äôt quit
LinkedIn in 2014 to work on the book. But of course I didn‚Äôt know that at the time! The royalties
could easily have turned out to be a factor of 10 lower, in which case it would have been
a financially much less compelling proposition.</p>

<h2 id="beyond-the-royalties">Beyond the royalties</h2>

<p>Part of the success of my book might also be explained by the fact that I put a lot of effort into
promoting it. Since the book went into early release I have given <a href="/talks.html">almost 50 talks</a> at
major conferences, plus a bunch of additional invited talks at companies and universities. Every
single talk contained at least a small advertisement for my book. Like a band going on tour to
promote their latest album, I suspect these talks contributed to the book being widely known.
A couple of my blog posts have also been quite popular, and these may also have brought the book to
potential readers‚Äô attention. I have now significantly dialled back my speaking commitments, so
I assume it is mostly spreading via word of mouth
(<a href="https://twitter.com/intensivedata/likes">social media</a>, and readers recommending it to their
colleagues).</p>

<p>The combination of talks and the book have allowed me to establish a significant public presence and
reputation in this field. I now get far more invitations to speak at conferences than I can
realistically accept. Conference talks don‚Äôt generate income <em>per se</em> (good industry conferences
generally pay for speakers‚Äô travel and accommodation, but they rarely pay speaking fees), but this
kind of reputation is helpful for getting consulting gigs.</p>

<p>I have only done a bit of consulting (and I now regularly turn down consulting requests from
companies because I‚Äôm focussing on my research), but I suspect that in my current position it would
be fairly easy to establish a lucrative consulting and training business, going into companies and
helping them with their data infrastructure problems. That is further financial value that writing
a book can bring: you become recognised as an expert and an authority in an area, and companies will
pay good money to get advice from such experts.</p>

<p>I have focussed a lot on the financial viability of writing a book because I believe that books are
an extremely valuable educational resource (more on this below). I want more people to write books,
and that requires book-writing to be a sustainable activity.</p>

<p>I was able to spend a great deal of time doing background research for my book because I was able to
afford to live without a salary for a year, but many people will not be able to do that. If people
can get <a href="https://jvns.ca/blog/2018/09/01/who-pays-to-educate-developers-/">paid fairly for creating educational materials</a>,
we will get more and better educational materials.</p>

<p>The economics of book-writing are challenging, and I reiterate that the success of my book is
atypical. However, I also find it heartening that it is <em>possible</em> to make a decent living from
technical writing. Not guaranteed, but possible, and that gives me hope.</p>

<h2 id="a-book-is-accessible-education">A book is accessible education</h2>

<p>Besides financial value to the author, there are lots of other good things about writing books.</p>

<p>A book is universally <strong>accessible</strong>: it is affordable to almost everyone, anywhere in the world. It
is vastly cheaper than a university course or corporate training, and you don‚Äôt have to move to
another city to take advantage of it. People in rural areas and developing countries can benefit
equally to those living in the global centres of tech. You can skim it or read it carefully cover to
cover, as you please. You don‚Äôt even need an internet connection to use it. Of course it doesn‚Äôt
confer all of the benefits of a university education (such as individual feedback, credentials,
professional network, social life), but as a medium for communicating knowledge, a book is almost
unbeatably efficient.</p>

<p>Of course there are also plenty of free resources online: Wikipedia, blog posts, videos, Stack
Overflow, API documentation, research papers, and so on. These are good as reference material for
answering a concrete question that you have (such as ‚Äúwhat are the parameters of the function
foo?‚Äù), but they are piecemeal fragments that are difficult to assemble into a coherent education.
On the other hand, a good book provides a carefully selected and designed programme of study, and
a narrative that is particularly valuable when you are trying to make sense of a complex topic for
the first time.</p>

<p>Compared to teaching people in person, a book is vastly more scalable. Even if I lecture in my
university‚Äôs largest lecture theatre for the rest of my career, I will not get anywhere near
teaching 100,000 people. For individual and small-group teaching, the disparity is greater still.
Yet a book is able to reach such large numbers of people routinely.</p>

<h2 id="creating-more-value-than-you-capture">Creating more value than you capture</h2>

<p>Writing a book is an activity that
<a href="http://radar.oreilly.com/2009/01/work-on-stuff-that-matters-fir.html">creates more value than it captures</a>.
What I mean with this is that the benefits that readers get from it are greater than the price they
paid for the book. To back this up, let‚Äôs try roughly estimating the value created by my book.</p>

<p>Of the 100,000 people who have bought my book so far, let‚Äôs say that two thirds of them intend to
read it but actually haven‚Äôt got round to it yet. Of those who have read it, let‚Äôs say that one
third were able to actually apply some of the ideas in the book, and two thirds read it purely out
of interest. So let‚Äôs say conservatively that 10% of people who bought the book, that is 10,000
people, have applied it for some useful purpose.</p>

<p>What might such a useful purpose look like? In the case of my book, much of it is about making
architectural decisions regarding data storage. If you get them right, you can build some amazing
systems; if you get them wrong, you have to spend ages painfully digging yourself out of a mess that
you got yourself into.</p>

<p>It‚Äôs hard to quantify that, but let‚Äôs say that the people who applied ideas from the book avoided
a bad decision that would have taken them one month of engineering time to rectify. (I‚Äôd actually
love to claim that the time saving is much higher, but let‚Äôs be conservative in our estimates.)
Thus, the 10,000 readers who applied the knowledge freed up an estimated 10,000 months, or 833
years, of engineering time to spend on things that are more useful than digging yourself out of
a mess.</p>

<p>If I spend 2.5 years writing a book, and it saves other people 833 years of time in aggregate, that
is over 300x leverage. If we assume an average engineering salary on the order of $100k, that‚Äôs $80m
of value created. Readers have spent approximately $4m buying those 100,000 books, so the value
created is about 20 times greater than the value captured. And this is based on some very
conservative estimates.</p>

<p>There are further ways in which the book creates value. For example, lots of readers have sent me
emails and tweets saying that because they read my book, they did well in a job interview, landing
them their dream job and providing financial security for their family. I don‚Äôt know how to measure
that sort of value created, but I think it‚Äôs tremendous.</p>

<p>How to be a 10x engineer:
<a href="https://twitter.com/peterseibel/status/512615519934230528">help ten other engineers be twice as good</a>.
Producing high-quality educational materials enables you to be a 300x engineer.</p>

<h2 id="conclusions">Conclusions</h2>

<p>Writing a technical book is not easy, but it is:</p>

<ul>
  <li>valuable (it helps people be better at their job),</li>
  <li>scalable (large numbers of people can benefit from it),</li>
  <li>accessible (it doesn‚Äôt discriminate who can benefit), and</li>
  <li>economically viable (it is possible to generate a reasonable level of income from it).</li>
</ul>

<p>It would be interesting to compare it to working on open source software, another activity that can
have significant positive impact but is
<a href="https://www.amazon.com/Working-Public-Making-Maintenance-Software/dp/0578675862/ref=as_li_ss_tl?ref_=nav_signin&amp;&amp;linkCode=ll1&amp;tag=dataintensive-20&amp;linkId=a152ade47ab23a9351d3e24950a89515&amp;language=en_US">difficult to get paid for</a>.
I don‚Äôt have a strong opinion on this at the moment.</p>

<p>On the downside, writing a book is really hard, at least if you want to do it well. For me it was
about the same level of difficulty as building and selling a
<a href="https://www.crunchbase.com/organization/rapportive">startup</a> (YMMV), that is to say, involving
multiple existential crises. The writing process was not good for my mental health. For that reason
I haven‚Äôt rushed into writing another book: the scars from writing the first one are still too
fresh. But the scars gradually do fade, and I‚Äôm hoping (perhaps naively) that it might be easier
next time.</p>

<p>On balance, I do think that writing a technical book is worth it. The feeling of knowing that you
have helped a lot of people is gratifying.  The personal growth that comes from taking on such
a challenge is also considerable. And there is no better way to learn something in depth than by
explaining it to others.</p>

<p>In my next post I will provide some advice on writing and publishing from my experience so far.</p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>A brief Brexit lament</title>
                <link>http://martin.kleppmann.com/2020/01/31/brief-brexit-lament.html</link>
                <comments>http://martin.kleppmann.com/2020/01/31/brief-brexit-lament.html#disqus_thread</comments>
                <pubDate>Fri, 31 Jan 2020 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2020/01/31/brief-brexit-lament.html</guid>
                
                <description><![CDATA[ It‚Äôs Brexit day, and I am sad. For me, something precious is lost today, and I would like to attempt to explain why. My father is German and my mother British. I have dual citizenship, grew up bilingually in Germany, and then moved to the UK over 16 years ago.... ]]></description>
                <content:encoded><![CDATA[
                    <p>It‚Äôs Brexit day, and I am sad.
For me, something precious is lost today, and I would like to attempt to explain why.</p>

<p>My father is German and my mother British.
I have dual citizenship, grew up bilingually in Germany, and then moved to the UK over 16 years ago.
In my grandparents‚Äô generation, our two countries fought a terrible war against each other, and yet, in my generation, many people like myself are the children of European love.</p>

<p>Comparing Germany and the UK, I have noticed how Europe seems to be perceived very differently in each country.
In the UK, the relationship with Europe is regarded mostly in economic terms, as a free-trade zone.
In Germany, it is regarded in the first instance as a peace project.</p>

<p>While the economic aspects are obviously important, the difference in perspective has profound implications.
The European project is not a marriage of mere economic convenience, to be divorced again as soon as we believe to have found a better ‚Äúdeal‚Äù elsewhere (ignoring the question of whether such a deal may or may not actually materialise).
If you regard Europe as a peace project, leaving it seems ridiculous.
Why would you <em>not</em> want to be part of a peace project?</p>

<p>In Europe, we learn each others‚Äô languages, we visit our twinned cities for school exchanges, concerts etc., we study abroad for a year through the Erasmus programme, we set up funding so that the richer regions in Europe support the development in poorer regions, and so on. 
Basically, the thinking goes: if the people of Europe make friends and find their spouses in other European countries, if they understand each other, trade with each other and support each other, then it is less likely that a demagogue will be able to lure them into fighting with each other again.</p>

<p>Although many British fought and fell in the world wars, and even the smallest village has a war memorial, the memory seems to be different.
I get the impression that British remembrance is often framed in a context of ‚Äúdefeating evil‚Äù, while German memories are around the horrors of brutal dictatorship, senseless destruction, pointless death, and the terrible trauma of the Holocaust.
There is nothing glorious in war, only suffering.
No winners, only losers.</p>

<p>I‚Äôm not claiming the EU is perfect; every human institution has flaws.
However, the way to improve it is not by throwing a screaming tantrum, running out of the door and banging it shut.
The way to improve an institution is to work from the inside to reform it and shape it to be what we want.
And so I mourn that my home country of choice, which I like in so many other ways, has decided to turn up its nose at this project.</p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Research update for 2019</title>
                <link>http://martin.kleppmann.com/2019/10/30/research-update.html</link>
                <comments>http://martin.kleppmann.com/2019/10/30/research-update.html#disqus_thread</comments>
                <pubDate>Wed, 30 Oct 2019 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2019/10/30/research-update.html</guid>
                
                <description><![CDATA[ It has now been four years since I moved from industry into academic research, and a lot has happened in this time. In 2015 I posted a year in review blog post, and in 2016 I announced the TRVE DATA project (which I am still working on), but I haven‚Äôt... ]]></description>
                <content:encoded><![CDATA[
                    <p>It has now been four years since I moved from industry into academic research, and
a lot has happened in this time.
In 2015 I posted a <a href="/2015/12/28/year-2015-review.html">year in review</a> blog post, and
in 2016 I announced the <a href="/2016/04/15/announcing-trve-data.html">TRVE DATA</a> project
(which I am still working on), but I haven‚Äôt posted an update on my work since.
So here goes!</p>

<p>In academic terms, things have been going well. Last year I
<a href="https://twitter.com/martinkl/status/1066748982129504256">got my PhD</a>, 
and as of October 2019 I got another few upgrades to my titles:</p>

<ul>
  <li>I am now an Early Career Fellow of the <a href="https://www.leverhulme.ac.uk/early-career-fellowships">Leverhulme Trust</a>,
with a matching fellowship from the <a href="https://www.newtontrust.cam.ac.uk">Isaac Newton Trust</a>.</li>
  <li>In the department I am now a ‚ÄúSenior Research Associate‚Äù (SRA, up from ‚ÄúResearch Associate‚Äù previously).</li>
  <li>At <a href="https://www.corpus.cam.ac.uk">Corpus Christi College</a> I am now a
<a href="https://www.corpus.cam.ac.uk/people/dr-martin-kleppmann">Fellow</a> and Director of Studies
(which means that I look after the computer science students in the college).</li>
</ul>

<p>The fellowships from the Leverhulme Trust and Isaac Newton Trust are paying my salary
for the next three years, along with additional support from the
<a href="https://mobicentre.cst.cam.ac.uk">Cambridge Centre for Mobile, Wearable Systems and Augmented Intelligence</a>.
The college fellowship mainly involves a very generous dining allowance, and being part of
a community of academics across a broad range of subjects (it‚Äôs nice sometimes to talk to people who
are not computer scientists).</p>

<p>My job title of ‚ÄúSenior Research Associate‚Äù reflects my status as an independent researcher: that is,
I set my own research agenda. However, to be clear, <em>independent</em> does not mean <em>alone</em>! My experience
of research has been a very sociable one, and all of my work has been in collaboration with others.
If you want to go far,
<a href="https://www.npr.org/sections/goatsandsoda/2016/07/30/487925796/it-takes-a-village-to-determine-the-origins-of-an-african-proverb">go together</a>.
On that note, shout-out to my fine collaborators of the last few years (in alphabetic order),
<a href="https://www.cl.cam.ac.uk/~arb33/">Alastair Beresford</a>,
<a href="https://www.cl.cam.ac.uk/~vb358/">Victor Gomes</a>,
<a href="https://www.cl.cam.ac.uk/~sak70/">Stephan Kollmann</a>,
<a href="http://dominic-mulligan.co.uk/">Dominic Mulligan</a>,
<a href="https://personal.cis.strath.ac.uk/d.thomas/">Daniel Thomas</a>,
<a href="https://twitter.com/pvh">Peter van Hardenberg</a>,
<a href="https://www.cl.cam.ac.uk/~dac53/">Diana Vasile</a>,
<a href="http://about.adamwiggins.com/">Adam Wiggins</a>,
and several more people from the <a href="https://www.inkandswitch.com">Ink &amp; Switch</a> research lab!</p>

<p>Huge thanks to <a href="http://www.boeing.com">The Boeing Company</a> for funding my work for the last four years.
Huge thanks also to <a href="https://www.cl.cam.ac.uk/~arb33/">Prof. Alastair Beresford</a>, my excellent
adviser, mentor, collaborator, and PI (that‚Äôs academic-speak for ‚Äúboss‚Äù) over the last four years.</p>

<h2 id="research-funding">Research funding</h2>

<p>The Leverhulme Trust and Isaac Newton Trust, which are funding my work, are UK charities that support
research across many subjects and disciplines, including humanities and social sciences. They fund about
<a href="https://www.leverhulme.ac.uk/early-career-fellowships-2019">140 early career fellowships</a> per year
across all subjects; only one or two per year of these are in computer science. So it looks like
I‚Äôm the computer scientist for this year!</p>

<p>A great aspect of this charity funding is that I am free to publish all my work as open source and
open access, with no restrictions. All the code I write is in a public repository by default. This
is very important to me, because the goal behind the things I‚Äôm working on (see below) is to
maximise the public benefit of these technologies through open source and open standards.</p>

<p>A downside is that all my positions are for a fixed three-year term (they are not tenure-track),
and I don‚Äôt know what comes afterwards. But for now I am going to concentrate on making the most I
can out of those three years.</p>

<h2 id="background-to-my-research">Background to my research</h2>

<p>Nowadays, we increasingly depend on Internet services for communication and collaboration: for
example, we use Google Docs to collaborate on documents, spreadsheets and presentations; we copy
files between devices using Dropbox; we communicate with colleagues using Slack; and we use many
other online services for task tracking, note taking, project planning, knowledge management, and
more.</p>

<p>These services are very valuable and convenient, but their use is also risky because they are
provided through a centralised server infrastructure. If the company providing the service goes out
of business, or decides to discontinue a product, the servers are shut down, the software stops
working, and users are locked out of the documents and data created with that software.</p>

<p>Moreover, since those servers typically process user data in unencrypted form, a rogue employee, or
an adversary who gains access to the servers, can read and tamper with vast amounts of sensitive
data. The provider may also use the data in arbitrary ways, e.g. to train their machine learning
systems and target you with ads.</p>

<p>When these risks are unacceptable, we can fall back to what we might call ‚Äúold-fashioned‚Äù
collaboration methods: for example, one person creates a spreadsheet with Excel and emails it to
their collaborator, who makes changes and then sends the modified file back again by email. This
approach has merits: it does not rely on any external services that might go away (besides the email
infrastructure), and the file can easily be encrypted. However, it quickly becomes messy if the
file is modified by more than one person at a time.</p>

<h2 id="research-goals">Research goals</h2>

<p>Together with my collaborators I am developing the foundations of a new kind of collaboration
software, which we are calling <a href="https://www.inkandswitch.com/local-first.html">local-first software</a>.
It aims to achieve the best of both worlds: allowing the user-friendly real-time collaboration of
applications like Google Docs, in which several people can make changes simultaneously without
suffering conflicts, but without relying on trusted, centralised servers.</p>

<p>While most of today‚Äôs Internet services keep the primary copy of any shared data on a server, the
local-first approach stores primary copies of the data as files on the collaborators‚Äô devices, like
in ‚Äúold-fashioned‚Äù collaboration. Servers may still be used, but rather than being a linchpin, they
become an optional enabling component. Because all the data is local, the software continues
working, even when the device has no Internet access or the servers are unavailable. When a user
modifies a document, local-first software automatically sends the changes to collaborators whenever
a network connection is available, so there is no need to email files back and forth.</p>

<p>Local-first software allows multiple users to make changes to the same document concurrently, even
while users are offline, and ensures that all of the changes are automatically merged into
a consistent result. We do this using <a href="https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type">CRDTs</a>.
In this regard our approach differs from version control systems such as SVN or Git, which require
conflicts to be resolved manually, and only offer merging of plain text files. By contrast, we can
perform all merges automatically, and support arbitrarily complex file formats such as spreadsheets,
CAD drawings, or databases with various data models.</p>

<p>Collaborators‚Äô devices can either communicate directly, using fast local networks in a
<em>peer-to-peer</em> manner, or indirectly via servers. To protect the confidentiality and integrity of
the communication between collaborators we plan to use <em>end-to-end encryption</em>. In this approach, if
servers are used, they only ever handle encrypted data that they cannot decrypt. Thus, even if
communication networks or servers are compromised by an attacker, user privacy and data
confidentiality are protected.</p>

<p>This approach is particularly suitable for sensitive data such as a university‚Äôs student records,
a hospital‚Äôs patient records, legally privileged communication, journalistic investigations, law
enforcement, diplomatic correspondence, and many other settings where regulations and
confidentiality obligations prohibit the sharing of unencrypted data with third parties.</p>

<h2 id="research-outputs">Research outputs</h2>

<p>The results of our research will be published in two forms: as traditional research papers in
academic venues, and in the form of open source software.</p>

<p>Research papers are important because they document the thought process and reasoning behind our
designs, and help others build upon our work in the future. We have already written a series of
<a href="/#publications">publications</a> about our research in the last four years, and there is lots more
interesting material still to come.</p>

<p>Software releases are traditionally regarded as less important in academia (and, to be honest, a lot
of code written by researchers is not very good). However, I regard open source software as a
crucial output for this project. <a href="https://github.com/automerge/automerge">Automerge</a> is the main
CRDT implementation I am working on, and it is designed from the start to be production-quality (in
terms of reliability, test coverage, stability, API design, documentation, community, and so on).</p>

<p>Automerge is not yet perfect, especially in terms of its performance: if it was, our research would
already be done! But I have a plan that should lead to big improvements in the coming year, and the
goal is that Automerge will soon be suitable for building ambitious, large-scale, local-first
applications.</p>

<p>In order to maximise the number of projects that can benefit from this work, the code is licensed
under the liberal <a href="https://opensource.org/licenses/MIT">MIT license</a>. Moreover, as the data format
(for representing documents on disk and for network communication) becomes stable, I think it will
make sense to formalise it as an open standard, with interoperable implementations in several
different programming languages and platforms.</p>

<p>And finally, we are starting to see an emerging community of users and contributors around Automerge.
New community members are regularly popping up in the
<a href="https://communityinviter.com/apps/automerge/automerge">Automerge Slack</a>, users are helpfully
reporting bugs, and a steadily growing
<a href="https://github.com/automerge/automerge/graphs/contributors">set of contributors</a> have had their
pull requests merged. It‚Äôs exciting to see this growing community engagement.</p>

<h2 id="interviews">Interviews</h2>

<p>If you want to hear more, I‚Äôve done a bunch of interviews with various podcasts and blogs over the
last few years:</p>

<ul>
  <li><a href="https://softwareengineeringdaily.com/2017/05/02/data-intensive-applications-with-martin-kleppmann/">With Software Engineering Daily about my book</a>,
‚ÄúDesigning Data-Intensive Applications‚Äù (May 2017)</li>
  <li><a href="https://softwareengineeringdaily.com/2017/12/08/decentralized-objects-with-martin-kleppman/">With Software Engineering Daily again</a>,
this time about my research on CRDTs (December 2017)</li>
  <li><a href="https://advancetechmedia.org/episode-008-martin-kleppmann/">With the Advance Tech Podcast</a>
about a wide range of topics that I find interesting (October 2017)</li>
  <li><a href="https://www.investedinvestor.com/articles/2018/1/23/martin-kleppmann">With the Invested Investor podcast</a>
about my startup career before I switched towards research (January 2018)</li>
  <li><a href="/2019/06/27/hydra-interview.html">With the Hydra conference</a> about distributed systems in general,
and my work specifically (June 2019)</li>
  <li><a href="https://medium.com/csr-tales/csrtale-13-formal-verification-of-strong-eventual-consistency-1cc0af942e64">With Computer Science Research (CSR) Tales</a>
(July 2019) about the background story behind our paper
‚Äú<a href="/2017/10/25/verifying-crdt-isabelle.html">Verifying Strong Eventual Consistency in Distributed Systems</a>‚Äù,
which won the Distinguished Paper Award and Distinguished Artifact Award at the OOPSLA 2017 conference.</li>
</ul>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Figuring out the future of distributed data systems (interview)</title>
                <link>http://martin.kleppmann.com/2019/06/27/hydra-interview.html</link>
                <comments>http://martin.kleppmann.com/2019/06/27/hydra-interview.html#disqus_thread</comments>
                <pubDate>Thu, 27 Jun 2019 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2019/06/27/hydra-interview.html</guid>
                
                <description><![CDATA[ I will be speaking at the Hydra distributed computing conference in St. Petersburg in July 2019. In the run-up to the conference, I did a long interview with Vadim Tsesko, a lead software engineer at Odnoklassniki. We covered lots of interesting topics in this interview, and so I requested permission... ]]></description>
                <content:encoded><![CDATA[
                    <p><em>I will be <a href="/2019/07/11/hydra-distributed.html">speaking</a> at the
<a href="https://hydraconf.com">Hydra distributed computing conference</a> in St. Petersburg in July 2019.
In the run-up to the conference, I did a long interview with
<a href="http://twitter.com/incubos">Vadim Tsesko</a>, a lead software engineer at
<a href="https://ok.ru">Odnoklassniki</a>.
We covered lots of interesting topics in this interview, and so I requested permission to
re-publish it on my own blog as well.</em></p>

<p><em>This interview was <a href="https://medium.com/@hydraconference/the-big-interview-with-martin-kleppmann-figuring-out-the-future-of-distributed-data-systems-28a680d99ae6">originally published by Hydra</a>.
It has also been <a href="https://habr.com/ru/company/jugru/blog/457736/">translated into Russian</a>.</em></p>

<h2 id="contents">Contents:</h2>

<ul>
  <li><a href="#moving-from-business-to-academic-research">Moving from business to academic research</a></li>
  <li><a href="#designing-data-intensive-applications">Designing Data-Intensive Applications</a></li>
  <li><a href="#common-sense-against-artificial-hype-and-aggressive-marketing">Common sense against artificial hype and aggressive marketing</a></li>
  <li><a href="#pitfalls-of-cap-theorem-and-other-industry-mistakes">Pitfalls of CAP theorem and other industry mistakes</a></li>
  <li><a href="#benefits-of-decentralization">Benefits of decentralization</a></li>
  <li><a href="#blockchains-dat-ipfs-filecoin-webrtc">Blockchains, Dat, IPFS, Filecoin, WebRTC</a></li>
  <li><a href="#new-crdts-formal-verification-with-isabelle">New CRDTs. Formal verification with Isabelle</a></li>
  <li><a href="#event-sourcing-and-apache-kafka">Event sourcing and Apache Kafka</a></li>
  <li><a href="#integrating-storage-systems-postgresql-memcached-redis-elasticsearch">Integrating storage systems: PostgreSQL, Memcached, Redis, Elasticsearch</a></li>
  <li><a href="#distributed-transactions-and-recovering-from-bugs">Distributed transactions and recovering from bugs</a></li>
  <li><a href="#professional-growth-and-development">Professional growth and development</a></li>
</ul>

<h2 id="moving-from-business-to-academic-research">Moving from business to academic research</h2>

<p><strong>Vadim</strong>: The first question I would like to ask you is really important for me. You founded Go
Test It and Rapportive, and you had been designing and engineering large-scale systems at LinkedIn
for a while. Then you decided to switch from industrial engineering to academia. Could you please
explain the motivation for that decision? What have you gained and what have you had to sacrifice?</p>

<p><strong>Martin</strong>: It‚Äôs been a very interesting process. As you seem to be hinting at, not many people make
the switch in that direction. A lot of people go from academia to industry, but not so many back.
Which is understandable, because I had to take quite a large pay cut in order to go back to
academia. But what I really love about research is the freedom to work on topics that I find
interesting and that I think are important, even if those topics don‚Äôt immediately lead to
a commercially viable product within the next 6 months or so. Of course, at a company the stuff you
build needs to turn into a product that can be sold in some form or another.</p>

<p>On the other hand, the things I‚Äôm now working on are topics that are really important for the future
of how we build software and how the internet works. But we don‚Äôt really understand these topics
well enough yet to go and start building commercial products: we are still at the level of trying to
figure out, fundamentally, what these technologies need to look like. And since this is fundamental
research I realized it‚Äôs better to do this at a university than to try to do it at a company,
because at a university I‚Äôm free to work on things that might not become commercially viable for
another ten years, and that is OK. It‚Äôs OK to work with a much longer time horizon when you‚Äôre in
research.</p>

<h2 id="designing-data-intensive-applications">Designing Data-Intensive Applications</h2>

<p><strong>Vadim</strong>: We‚Äôll definitely get back to your current research interests. Meanwhile let‚Äôs talk about
your latest book <a href="https://dataintensive.net">Designing Data-Intensive Applications</a>. I‚Äôm a big fan
of your book and I believe it‚Äôs one of the best guides for building modern distributed systems.
You‚Äôve covered almost all the notable achievements up to date.</p>

<p><strong>Martin</strong>: Thank you, I‚Äôm glad you find it useful.</p>

<p><strong>Vadim</strong>: Just for those unlucky readers who haven‚Äôt read your book yet, could you please name
several major achievements in the field of distributed systems nowadays?</p>

<p><strong>Martin</strong>: Well, the goal of the book is not so much to explain one particular technology; the goal
is rather to give you a guide to the entire landscape of different systems that are used for storing
and processing data. There are so many different databases, stream processors, batch processing
tools, all sorts of replication tools and so on, and it‚Äôs really hard to get an overview. If you‚Äôre
trying to build a particular application it‚Äôs really hard to know which database you should use, and
which tools are the most appropriate ones for the problem you‚Äôre trying to solve.</p>

<p>A lot of existing computing books simply didn‚Äôt answer that problem in a satisfactory way. I found
that if you‚Äôre reading a book on Cassandra for example, it would tell you why Cassandra is
wonderful, but it generally wouldn‚Äôt tell you about things for which it‚Äôs not a good fit. So what
I really wanted to do in this book was to identify the main questions that you need to ask yourself
if you‚Äôre trying to build some kind of large-scale system. And through answering those questions you
can then help figure out which technologies are appropriate and which are less appropriate for the
particular problem you‚Äôre trying to solve ‚Äî because, in general, there‚Äôs no one technology that is
perfect for everything. And so, the book is trying to help you figure out the pros and cons of
different technologies in different settings.</p>

<h2 id="common-sense-against-artificial-hype-and-aggressive-marketing">Common sense against artificial hype and aggressive marketing</h2>

<p><strong>Vadim</strong>: Indeed, often ‚Äî if not always ‚Äî there are many technologies with overlapping functions,
features and data models. And you can‚Äôt believe all those marketing buzzwords. You need to read the
white papers to learn the internals, and even try to read the source code to understand how it works
exactly.</p>

<p><strong>Martin</strong>: And I found that you often have to read between the lines because often the
documentation doesn‚Äôt really tell you for which things a particular database sucks. The truth is
that every database sucks at some kind of workload, the question is just to know which ones they
are. So yes, sometimes you have to read the deployment guidelines for ops people and try to
reverse-engineer from that what is actually going on in the system.</p>

<p><strong>Vadim</strong>: Don‚Äôt you feel that the industry lacks the common vocabulary or a set of criteria to
compare different solutions for the same problem? Similar things are called by different names, some
things are omitted which should always be clear and stated explicitly, like transaction guarantees.
What do you think?</p>

<p><strong>Martin</strong>: Yeah, I think a problem that our industry has is that often, when people talk about a
particular tool, there‚Äôs a lot of hype about everything. Which is understandable, because the tools
are made by various companies, and obviously those companies want to promote their products, and so
those companies will send people to conferences to speak about how wonderful their product is,
essentially. It will be disguised as a tech talk, but essentially it‚Äôs still a sales activity. As an
industry, we really could do with more honesty about the advantages and disadvantages of some
product. And part of that requires a common terminology, because otherwise you simply can‚Äôt compare
things on an equal footing. But beyond a shared terminology we need ways of reasoning about things
that certain technologies are good or bad at.</p>

<h2 id="pitfalls-of-cap-theorem-and-other-industry-mistakes">Pitfalls of CAP theorem and other industry mistakes</h2>

<p><strong>Vadim</strong>: My next question is quite a controversial one. Could you please name any major mistakes
in the industry you have stumbled upon during your career? Maybe overvalued technologies or
widely-practiced solutions we should have got rid of a long time ago? It might be a bad example, but
compare JSON over HTTP/1.1 vs the much more efficient gRPC over HTTP/2. Or is there an alternative
point of view?</p>

<p><strong>Martin</strong>: I think in many cases there are very good reasons for why a technology does one thing
and not another. So I‚Äôm very hesitant to call things mistakes, because in most cases it‚Äôs a question
of trade-offs. In your example of JSON over HTTP/1.1 versus Protocol Buffers over HTTP/2, I think
there are actually quite reasonable arguments for both sides there. For example, if you want to use
Protocol Buffers, you have to define your schema, and a schema can be a wonderful thing because it
helps document exactly what communication is going on. But some people find schemas annoying,
especially if they‚Äôre at early stages of development and they‚Äôre changing data formats very
frequently. So there you have it, there‚Äôs a question of trade-offs; in some situations one is
better, in others the other is better.</p>

<p>In terms of actual mistakes that I feel are simply bad, there‚Äôs only a fairly small number of
things. One opinion that I have is that the CAP Theorem is fundamentally bad and just not useful.
Whenever people use the CAP Theorem to justify design decisions, I think often they are either
misinterpreting what CAP is actually saying, or stating the obvious in a way. CAP as a theorem has a
problem that it is really just stating the obvious. Moreover, it talks about just one very narrowly
defined consistency model, namely linearizability, and one very narrowly defined availability model,
which is: you want every replica to be fully available for reads and writes, even if it cannot
communicate with any other replicas. These are reasonable definitions, but they are very narrow, and
many applications simply do not fall into the case of needing precisely that definition of
consistency or precisely that definition of availability. And for all the applications that use a
different definition of those words, the CAP Theorem doesn‚Äôt tell you anything at all. It‚Äôs simply
an empty statement. So that, I feel, is a mistake.</p>

<p>And while we‚Äôre ranting, if you‚Äôre asking me to name mistakes, another big mistake that I see in the
tech industry is the mining of cryptocurrencies, which I think is such an egregious waste of
electricity. I just cannot fathom why people think that is a good idea.</p>

<p><strong>Vadim</strong>: Talking about the CAP Theorem, many storage technologies are actually tunable, in terms
of things like AP or CP. You can choose the mode they operate in.</p>

<p><strong>Martin</strong>: Yes. Moreover, there are many technologies which are neither consistent nor available
under the strict definition of the CAP Theorem. They are literally just P! Not CP, not CA, not AP,
just P. Nobody says that, because that would look bad, but honestly, this could be a perfectly
reasonable design decision to make. There are many systems for which that is actually totally fine.
This is actually one of the reasons why I think that CAP is such an unhelpful way of talking about
things: because there is a huge part of the design space that it simply does not capture, where
there are perfectly reasonable good designs for software that it simply doesn‚Äôt allow you to talk
about.</p>

<h2 id="benefits-of-decentralization">Benefits of decentralization</h2>

<p><strong>Vadim</strong>: Talking about data-intensive applications today, what other major challenges, unsolved
problems or hot research topics can you name? As far as I know, you‚Äôre a major proponent of
decentralized computation and storage.</p>

<p><strong>Martin</strong>: Yes. One of the theses behind my research is that at the moment we rely too much on
servers and centralization. If you think about how the Internet was originally designed back in the
day when it evolved from ARPANET, it was intended as a very resilient network where packets could be
sent via several different routes, and they would still get to the destination. And if a nuclear
bomb hit a particular American city, the rest of the network would still work because it would just
route around the failed parts of the system. This was a Cold War design.</p>

<p>And then we decided to put everything in the cloud, and now basically everything has to go via one
of AWS‚Äôs datacenters, such as us-east-1 somewhere in Virginia. We‚Äôve taken away this ideal of being
able to decentrally use various different parts of the network, and we‚Äôve put in these servers that
everything relies on, and now it‚Äôs extremely centralized. So I‚Äôm interested in decentralization, in
the sense of moving some of the power and control over data away from those servers and back to the
end users.</p>

<p>One thing I want to add in this context is that a lot of people talking about decentralization are
talking about things like cryptocurrencies, because they are also attempting a form of
decentralization whereby control is moved away from a central authority like a bank and into a
network of cooperating nodes. But that‚Äôs not really the sort of decentralization that I‚Äôm interested
in: I find that these cryptocurrencies are actually still extremely centralized, in the sense that
if you want to make a Bitcoin transaction, you have to make it on the Bitcoin network ‚Äî you have to
use the network of Bitcoin, so everything is centralized on that particular network. The way it‚Äôs
built is decentralized in the sense that it doesn‚Äôt have a single controlling node, but the network
as a whole is extremely centralized in that any transaction you have to make you have to do through
this network. You can‚Äôt do it in some other way. I feel that it‚Äôs still a form of centralization.</p>

<p>In the case of a cryptocurrency this centralization might be inevitable, because you need to do
stuff like avoid double spending, and doing that is difficult without a network that achieves
consensus about exactly which transactions have happened and which have not. And this is exactly
what the Bitcoin network does. But there are many applications that do not require something like a
blockchain, which can actually cope with a much more flexible model of data flowing around the
system. And that‚Äôs the type of decentralized system that I‚Äôm most interested in.</p>

<h2 id="blockchains-dat-ipfs-filecoin-webrtc">Blockchains, Dat, IPFS, Filecoin, WebRTC</h2>

<p><strong>Vadim</strong>: Could you please name any promising or undervalued technologies in the field of
decentralized systems apart from blockchain? I have been using IPFS for a while.</p>

<p><strong>Martin</strong>: For IPFS, I have looked into it a bit though I haven‚Äôt actually used it myself. We‚Äôve
done some work with the <a href="https://dat.foundation/">Dat</a> project, which is somewhat similar to
<a href="https://ipfs.io/">IPFS</a> in the sense that it is also a decentralized storage technology. The
difference is that IPFS has <a href="https://filecoin.io/">Filecoin</a>, a cryptocurrency, attached to it as a
way of paying for storage resources, whereas Dat does not have any blockchain attached to it ‚Äî it is
purely a way of replicating data across multiple machines in a P2P manner.</p>

<p>For the project that I‚Äôve been working on, Dat has been quite a good fit, because we wanted to build
collaboration software in which several different users could each edit some document or database,
and any changes to that data would get sent to anyone else who needs to have a copy of this data. We
can use Dat to do this replication in a P2P manner, and Dat takes care of all the networking-level
stuff, such as NAT traversal and getting through firewalls ‚Äî it‚Äôs quite a tricky problem just to get
the packets from one end to the other. And then we built a layer on top of that, using CRDTs, which
is a way of allowing several people to edit some document or dataset and to exchange those edits in
an efficient way. I think you can probably build this sort of thing on IPFS as well: you can
probably ignore the Filecoin aspect and just use the P2P replication aspect, and it will probably do
the job just as well.</p>

<p><strong>Vadim</strong>: Sure, though using IPFS might lead to lower responsiveness, because WebRTC underlying Dat
connects P2P nodes directly, and IPFS works like a distributed hash table thing.</p>

<p><strong>Martin</strong>: Well, WebRTC is at a different level of the stack, since it‚Äôs intended mostly for
connecting two people together who might be having a video call; in fact, the software we‚Äôre using
for this interview right now may well be using WebRTC. And WebRTC does give you a data channel that
you can use for sending arbitrary binary data over it, but building a full replication system on top
of that is still quite a bit of work. And that‚Äôs something that Dat or IPFS do already.</p>

<p>You mentioned responsiveness ‚Äî that is certainly one thing to think about. Say you wanted to build
the next Google Docs in a decentralized way. With Google Docs, the unit of changes that you make is
a single keystroke. Every single letter that you type on your keyboard may get sent in real time to
your collaborators, which is great from the point of view of fast real-time collaboration. But it
also means that over the course of writing a large document you might have hundreds of thousands of
these single-character edits that accumulate, and a lot of these technologies right now are not very
good at compressing this kind of editing data. You can keep all of the edits that you‚Äôve ever made
to your document, but even if you send just a hundred bytes for every single keystroke that you make
and you write a slightly larger document with, say, 100,000 keystrokes, you suddenly now have 10 MB
of data for a document that would only be a few tens of kilobytes normally. So we have this huge
overhead for the amount of data that needs to be sent around, unless we get more clever at
compressing and packaging up changes.</p>

<p>Rather than sending somebody the full list of every character that has ever been typed, we might
just send the current state of the document, and after that we send any updates that have happened
since. But a lot of these peer-to-peer systems don‚Äôt yet have a way of doing those state snapshots
in a way that would be efficient enough to use them for something like Google Docs. This is actually
an area I‚Äôm actively working on, trying to find better algorithms for synchronizing up different
users for something like a text document, where we don‚Äôt want to keep every single keystroke because
that would be too expensive, and we want to make more efficient use of the network bandwidth.</p>

<h2 id="new-crdts-formal-verification-with-isabelle">New CRDTs. Formal verification with Isabelle</h2>

<p><strong>Vadim</strong>: Have you managed to compress that keystroke data substantially? Have you invented new
CRDTs or anything similar?</p>

<p><strong>Martin</strong>: Yes. So far we only have prototypes for this, it‚Äôs not yet fully implemented, and we
still need to do some more experiments to measure how efficient it actually is in practice. But we
have developed some compression schemes that look very promising. In my prototype I reduced it from
about 100 bytes per edit to something like 1.7 bytes of overhead per edit. And that‚Äôs a lot more
reasonable of course. But as I say, these experiments are still ongoing, and the number might still
change slightly. But I think the bottom line is that there‚Äôs a lot of room there for optimization
still, so we can still make it a lot better.</p>

<p><strong>Vadim</strong>: So this is what <a href="https://hydraconf.com/2019/talks/6i4lkw8pwjnadgff5ylr11/">your talk</a>
will be about at the Hydra conference, am I right?</p>

<p><strong>Martin</strong>: Yes, exactly. I‚Äôll give a quick introduction to the area of CRDTs, collaborative
software and some of the problems that arise in that context. Then I‚Äôll describe some of the
research that we‚Äôve been doing in this area. It‚Äôs been quite fun because the research we‚Äôve been
doing has been across a whole range of different concerns. On the very applied side, we‚Äôve got a
JavaScript implementation of these algorithms, and we‚Äôre using that to build real pieces of
software, trying to use that software ourselves to see how it behaves. On the other end of the
spectrum, we‚Äôve been working with formal methods to prove these algorithms correct, because some of
these algorithms are quite subtle and we want to be very sure that the systems we‚Äôre making are
actually correct, i.e. that they always reach a consistent state. There have been a lot of
algorithms in the past that have actually failed to do that, which were simply wrong, that is, in
certain edge cases, they would remain permanently inconsistent. And so, in order to avoid these
problems that algorithms have had in the past, we‚Äôve been using formal methods to prove our
algorithms correct.</p>

<p><strong>Vadim:</strong> Wow. Do you really use theorem provers, like Coq or Isabelle or anything else?</p>

<p><strong>Martin</strong>: Exactly, we‚Äôve been using Isabelle for that.</p>

<blockquote>
  <p>You can attend
<a href="https://thestrangeloop.com/2019/correctness-proofs-of-distributed-systems-with-isabelle.html">Martin‚Äôs talk</a>
‚ÄúCorrectness proofs of distributed systems with Isabelle‚Äù at The Strange Loop conference in September.</p>
</blockquote>

<p><strong>Vadim</strong>: Sounds great! Are those proofs going to be published?</p>

<p><strong>Martin</strong>: Yes, our first set of proofs is already public. We
<a href="https://martin.kleppmann.com/papers/crdt-isabelle-oopsla17.pdf">published</a> that a year and a half
ago: it was a framework for verifying CRDTs, and we verified three particular CRDTs within that
framework, the main one of which was RGA (<a href="http://csl.skku.edu/papers/jpdc11.pdf">Replicated Growable
Array</a>), which is a CRDT for collaborative text editing.
While it is not very complicated, it is quite a subtle algorithm, and so it‚Äôs a good case where
proof is needed, because it‚Äôs not obvious just from looking at it that it really is correct. And so
the proof gives us the additional certainty that it really is correct. Our previous work there was
on verifying a couple of existing CRDTs, and our most recent work in this area is about our own
CRDTs for new data models we‚Äôve been developing, and proving our own CRDTs correct as well.</p>

<p><strong>Vadim</strong>: How much larger is the proof compared to the description of the algorithm? Because it can
be a problem sometimes.</p>

<p><strong>Martin</strong>: Yes, that is a problem ‚Äî the proofs are often a lot of work. I think in our latest
example‚Ä¶ Actually, let me have a quick look at the code. The description of the algorithm and the
data structures is about 60 lines of code. So it‚Äôs quite a small algorithm. The proof is over 800
lines. So we‚Äôve got roughly 12:1 ratio between the proof and the code. And that is unfortunately
quite typical. The proof is a large amount of additional work. On the other hand, once we have the
proof, we have gained very strong certainty in the correctness of the algorithm. Moreover, we have
ourselves, as humans, understood the algorithm much better. Often I find that through trying to
formalize it, we end up understanding the thing we‚Äôre trying to formalize much better than we did
before. And that in itself is actually a useful outcome from this work: besides the proof itself we
gain a deeper understanding, and that is often very helpful for creating better implementations.</p>

<p><strong>Vadim</strong>: Could you please describe the target audience of your talk, how hardcore is it going to
be? What is the preliminary knowledge you expect the audience to have?</p>

<p><strong>Martin</strong>: I like to make my talks accessible with as little previous knowledge requirement as
possible, and I try to lift everybody up to the same level. I cover a lot of material, but I start
at a low base. I would expect people to have some general distributed systems experience: how do you
send some data over a network using TCP, or maybe a rough idea of how Git works, which is quite a
good model for these things. But that‚Äôs about all you need, really. Then, understanding the work
we‚Äôve been doing on top of that is actually not too difficult. I explain everything by example,
using pictures to illustrate everything. Hopefully, everybody will be able to follow along.</p>

<h2 id="event-sourcing-and-apache-kafka">Event sourcing and Apache Kafka</h2>

<p><strong>Vadim</strong>: Sounds really great. Actually, we have some time and I would like to discuss one of your
<a href="https://queue.acm.org/detail.cfm?id=3321612">recent articles</a> about online event processing. You‚Äôre
a great supporter of the idea of event sourcing, is that correct?</p>

<p><strong>Martin</strong>: Yes, sure.</p>

<p><strong>Vadim</strong>: Nowadays this approach is getting momentum, and in the pursuit of all the advantages of
globally ordered log of operations, many engineers try to deploy it everywhere. Could you please
describe some cases where event sourcing is not the best option? Just to prevent its misuse and
possible disappointment with the approach itself.</p>

<p><strong>Martin</strong>: There are two different layers of the stack that we need to talk about first. Event
sourcing, as proposed by Greg Young and some others, is intended as a mechanism for data modeling,
that is: if you have a database schema and you‚Äôre starting to lose control of it because there are
so many different tables and they‚Äôre all getting modified by different transactions ‚Äî then event
sourcing is a way of bringing better clarity to this data model, because the events can express very
directly what is happening at a business level. What is the action that the user took? And then, the
consequences of that action might be updating various tables and so on. Effectively, what you‚Äôre
doing with event sourcing is you‚Äôre separating out the action (the event) from its effects, which
happen somewhere downstream.</p>

<p>I‚Äôve come to this area from a slightly different angle, which is a lower-level point of view of
using systems like Kafka for building highly scalable systems. This view is similar in the sense
that if you‚Äôre using something like Kafka you are using events, but it doesn‚Äôt mean you‚Äôre
necessarily using event sourcing. And conversely, you don‚Äôt need to be using Kafka in order to do
event sourcing; you could do event sourcing in a regular database, or you could use a special
database that was designed specifically for event sourcing. So these two ideas are similar, but
neither requires the other, they just have some overlap.</p>

<p>The case for wanting to use a system like Kafka is mostly the scalability argument: in that case
you‚Äôve simply got so much data coming in that you cannot realistically process it on a single-node
database, so you have to partition it in some way, and using an event log like Kafka gives you a
good way of spreading that work over multiple machines. It provides a good, principled way for
scaling systems. It‚Äôs especially useful if you want to integrate several different storage systems.
So if, for example, you want to update not just your relational database but also, say, a full-text
search index like Elasticsearch, or a caching system like Memcached or Redis or something like that,
and you want one event to have an updating effect on all of these different systems, then something
like Kafka is very useful.</p>

<p>In terms of the question you asked (what are the situations in which I would not use this event
sourcing or event log approach) ‚Äî I think it‚Äôs difficult to say precisely, but as a rule of thumb I
would say: use whatever is the simplest. That is, whatever is closest to the domain that you‚Äôre
trying to implement. And so, if the thing you‚Äôre trying to implement maps very nicely to a
relational database, in which you just insert and update and delete some rows, then just use a
relational database and insert and update and delete some rows. There‚Äôs nothing wrong with
relational databases and using them as they are. They have worked fine for us for quite a long time
and they continue to do so. But if you‚Äôre finding yourself in a situation where you‚Äôre really
struggling to use that kind of database, for example because the complexity of the data model is
getting out of hand, then it makes sense to switch to something like an event sourcing approach.</p>

<p>And similarly, on the lower level (scalability), if the size of your data is such that you can just
put it in PostgreSQL on a single machine ‚Äî that‚Äôs probably fine, just use PostgreSQL on a single
machine. But if you‚Äôre at the point where there is no way that a single machine can handle your
load, you have to scale across a large system, then it starts making sense to look into more
distributed systems like Kafka. I think the general principle here is: use whatever is simplest for
the particular task you‚Äôre trying to solve.</p>

<h2 id="integrating-storage-systems-postgresql-memcached-redis-elasticsearch">Integrating storage systems: PostgreSQL, Memcached, Redis, Elasticsearch</h2>

<p><strong>Vadim</strong>: It‚Äôs really good advice. As your system evolves you can‚Äôt precisely predict the direction
of development, all the queries, patterns and data flows.</p>

<p><strong>Martin</strong>: Exactly, and for those kinds of situations relational databases are amazing, because
they are very flexible, especially if you include the JSON support that they now have. PostgreSQL
now has pretty good support for JSON. You can just add a new index if you want to query in a
different way. You can just change the schema and keep running with the data in a different
structure. And so if the size of the data set is not too big and the complexity is not too great,
relational databases work well and provide a great deal of flexibility.</p>

<p><strong>Vadim</strong>: Let‚Äôs talk a little bit more about event sourcing. You mentioned an interesting example
with several consumers consuming events from one queue based on Kafka or something similar. Imagine
that new documents get published, and several systems are consuming events: a search system based on
Elasticsearch, which makes the documents searchable, a caching system which puts them into key-value
cache based on Memcached, and a relational database system which updates some tables accordingly. A
document might be a car selling offer or a realty advert. All these consuming systems work
simultaneously and concurrently.</p>

<p><strong>Martin</strong>: So your question is how do you deal with the fact that if you have these several
consumers, some of them might have been updated, but the others have not yet seen an update and are
still lagging behind slightly?</p>

<p><strong>Vadim</strong>: Yes, exactly. A user comes to your website, enters a search query, gets some search
results and clicks a link. But she gets 404 HTTP status code because there is no such entity in the
database, which hasn‚Äôt been able to consume and persist the document yet.</p>

<p><strong>Martin</strong>: Yes, this is a bit of a challenge actually. Ideally, what you want is what we would call
‚Äúcausal consistency‚Äù across these different storage systems. If one system contains some data that
you depend on, then the other systems that you look at will also contain those dependencies.
Unfortunately, putting together that kind of causal consistency across different storage
technologies is actually very hard, and this is not really the fault of event sourcing, because no
matter what approach or what system you use to send the updates to the various different systems,
you can always end up with some kind of concurrency issues.</p>

<p>In your example of writing data to both Memcached and Elasticsearch, even if you try to do the
writes to the two systems simultaneously you might have a little bit of network delay, which means
that they arrive at slightly different times on those different systems, and get processed with
slightly different timing. And so somebody who‚Äôs reading across those two systems may see an
inconsistent state. Now, there are some research projects that are at least working towards
achieving that kind of causal consistency, but it‚Äôs still difficult if you just want to use
something like Elasticsearch or Memcached or so off the shelf.</p>

<p>A good solution here would be that you get presented, conceptually, with a consistent point-in-time
snapshot across both the search index and the cache and the database. If you‚Äôre working just within
a relational database, you get something called snapshot isolation, and the point of snapshot
isolation is that if you‚Äôre reading from the database, it looks as though you‚Äôve got your own
private copy of the entire database. Anything you look at in the database, any data you query will
be the state as of that point in time, according to the snapshot. So even if the data has afterwards
been changed by another transaction, you will actually see the older data, because that older data
forms part of a consistent snapshot.</p>

<p>And so now, in the case where you‚Äôve got Elasticsearch and Memcached, really what you would ideally
want is a consistent snapshot across these two systems. But unfortunately, neither Memcached nor
Redis nor Elasticsearch have an efficient mechanism for making those kinds of snapshots that can be
coordinated with different storage systems. Each storage system just thinks for itself and typically
presents you the latest value of every key, and it doesn‚Äôt have this facility for looking back and
presenting a slightly older version of the data, because the most recent version of the data is not
yet consistent.</p>

<p>I don‚Äôt really have a good answer for what the solution would look like. I fear that the solution
would require code changes to any of the storage systems that participate in this kind of thing. So
it will require changes to Elasticsearch and to Redis and to Memcached and any other systems. And
they would have to add some kind of mechanism for point-in-time snapshots that is cheap enough that
you can be using it all the time, because you might be wanting the snapshot several times per second
‚Äî it‚Äôs not just a once-a-day snapshot, it‚Äôs very fine-grained. And at the moment the underlying
systems are not there in terms of being able to do these kinds of snapshots across different storage
systems. It‚Äôs a really interesting research topic. I‚Äôm hoping that somebody will work on it, but I
haven‚Äôt seen any really convincing answers to that problem yet so far.</p>

<h2 id="distributed-transactions-and-recovering-from-bugs">Distributed transactions and recovering from bugs</h2>

<p><strong>Vadim</strong>: Yeah, we need some kind of shared <a href="https://en.wikipedia.org/wiki/Multiversion_concurrency_control">Multiversion Concurrency
Control</a>.</p>

<p><strong>Martin</strong>: Exactly, like the distributed transaction systems. XA distributed transactions will get
you some of the way there, but unfortunately XA, as it stands, is not really very well suited
because it only works if you‚Äôre using locking-based concurrency control. This means that if you read
some data, you have to take a lock on it so that nobody can modify that data while you have that
lock. And that kind of locking-based concurrency control has terrible performance, so no system
actually uses that in practice nowadays. But if you don‚Äôt have that locking then you don‚Äôt get the
necessary isolation behavior in a system like XA distributed transactions. So maybe what we need is
a new protocol for distributed transactions that allows snapshot isolation as the isolation
mechanism across different systems. But I don‚Äôt think I‚Äôve seen anything that implements that yet.</p>

<p><strong>Vadim</strong>: Yes, I hope somebody is working on it.</p>

<p><strong>Martin</strong>: Yes, it would be really important. Also in the context of microservices, for example:
the way that people promote that you should build microservices is that each microservice has its
own storage, its own database, and you don‚Äôt have one service directly accessing the database of
another service, because that would break the encapsulation of the service. Therefore, each service
just manages its own data.</p>

<p>For example, you have a service for managing users, and it has a database for the users, and
everyone else who wants to find out something about users has to go through the user service. From
the point of view of encapsulation that is nice: you‚Äôre hiding details of the database schema from
the other services for example.</p>

<p>But from the point of view of consistency across different services ‚Äî well, you‚Äôve got a huge
problem now, because of exactly the thing we were discussing: we might have data in two different
services that depends upon each other in some way, and you could easily end up with one service
being slightly ahead of or slightly behind the other in terms of timing, and then you could end up
with someone who reads across different services, getting inconsistent results. And I don‚Äôt think
anybody building microservices currently has an answer to that problem.</p>

<p><strong>Vadim</strong>: It is somewhat similar to workflows in our society and government, which are inherently
asynchronous and there are no guarantees of delivery. You can get your passport number, then you can
change it, and you need to prove that you changed it, and that you are the same person.</p>

<p><strong>Martin</strong>: Yes, absolutely. As humans we have ways of dealing with this, for example, we might know
that oh, sometimes that database is a bit outdated, I‚Äôll just check back tomorrow. And then tomorrow
it‚Äôs fine. But if it‚Äôs software that we‚Äôre building, we have to program all that kind of handling
into the software. The software can‚Äôt think for itself.</p>

<p><strong>Vadim</strong>: Definitely, at least not yet. I have another question about the advantages of event
sourcing. Event sourcing gives you the ability to stop processing events in case of a bug, and
resume consuming events having deployed the fix, so that the system is always consistent. It‚Äôs a
really strong and useful property, but it might not be acceptable in some cases like banking where
you can imagine a system that continues to accept financial transactions, but the balances are stale
due to suspended consumers waiting for a bugfix from developers. What might be a workaround in such
cases?</p>

<p><strong>Martin</strong>: I think it‚Äôs a bit unlikely to stop the consumer, deploying the fix and then restart it,
because, as you say, the system has got to continue running, you can‚Äôt just stop it. I think what is
more likely to happen is: if you discover a bug, you let the system continue running, but while it
continues running with the buggy code, you produce another version of the code that is fixed, you
deploy that fixed version separately and run the two in parallel for a while. In the fixed version
of the code you might go back in history and reprocess all of the input events that have happened
since the buggy code was deployed, and maybe write the results to a different database. Once you‚Äôve
caught up again you‚Äôve got two versions of the database, which are both based on the same event
inputs, but one of the two processed events with the buggy code and the other processed the events
with the correct code. At that point you can do the switchover, and now everyone who reads the data
is going to read the correct version instead of the buggy version, and you can shut down the buggy
version. That way you never need to stop the system from running, everything keeps working all the
time. And you can take the time to fix the bug, and you can recover from the bug because you can
reprocess those input events again.</p>

<p><strong>Vadim</strong>: Indeed, it‚Äôs a really good option if the storage systems are under your control, and we
are not talking about side effects applied to external systems.</p>

<p><strong>Martin</strong>: Yes, you‚Äôre right, once we send the data to external systems it gets more difficult
because you might not be able to easily correct it. But this is again something you find in
financial accounting, for example. In a company, you might have quarterly accounts. At the end of
the quarter, everything gets frozen, and all of the revenue and profit calculations are based on the
numbers for that quarter. But then it can happen that actually, some delayed transaction came in,
because somebody forgot to file a receipt in time. The transaction comes in after the calculations
for the quarter have been finalized, but it still belongs in that earlier quarter.</p>

<p>What accountants do in this case is that in the next quarter, they produce corrections to the
previous quarter‚Äôs accounts. And typically those corrections will be a small number, and that‚Äôs no
problem because it doesn‚Äôt change the big picture. But at the same time, everything is still
accounted for correctly. At the human level of these accounting systems that has been the case ever
since accounting systems were invented, centuries ago. It‚Äôs always been the case that some late
transactions would come in and change the result for some number that you thought was final, but
actually, it wasn‚Äôt because the correction could still come in. And so we just build the system with
the mechanism to perform such corrections. I think we can learn from accounting systems and apply
similar ideas to many other types of data storage systems, and just accept the fact that sometimes
they are mostly correct but not 100% correct and the correction might come in later.</p>

<p><strong>Vadim</strong>: It‚Äôs a different point of view to building systems.</p>

<p><strong>Martin</strong>: It is a bit of a new way of thinking, yes. It can be disorienting when you come across
it at first. But I don‚Äôt think there‚Äôs really a way round it, because this impreciseness is inherent
in the fact that we do not know the entire state of the world ‚Äî it is fundamental to the way
distributed systems work. We can‚Äôt just hide it, we can‚Äôt pretend that it doesn‚Äôt happen, because
that imprecision is necessarily exposed in the way we process the data.</p>

<h2 id="professional-growth-and-development">Professional growth and development</h2>

<p><strong>Vadim</strong>: Do you think that conferences like Hydra are anticipated? Most distributed systems are
quite different, and it is hard to imagine that many attendees will get to work and will start
applying what they have learned in day-to-day activities.</p>

<p><strong>Martin</strong>: It is broad, but I think that a lot of the interesting ideas in distributed systems are
conceptual. So the insights are not necessarily like ‚Äúuse this database‚Äù or ‚Äúuse this particular
technology‚Äù. They are more like ways of thinking about systems and about software. And those kinds
of ideas can be applied quite widely. My hope is that when attendees go away from this conference,
the lessons they take away are not so much what piece of software they should be using or which
programming language they should be using ‚Äî really, I don‚Äôt mind about that ‚Äî but more like how to
<em>think</em> about the systems they are building.</p>

<p><strong>Vadim</strong>: Why do you think it‚Äôs important to give conference talks on such complex topics as your
talk, compared to publishing papers, covering all their details and intricacies? Or should anyone do
both?</p>

<p><strong>Martin</strong>: I think they serve different purposes. When we write papers, the purpose is to have a
very definitive, very precise analysis of a particular problem, and to go really deep in that. On
the other hand, the purpose of a talk is more to get people interested in a topic and to start a
conversation around it. I love going to conferences partly because of the discussions I then have
around the talk, where people come to me and say: ‚Äúoh, we tried something like this, but we ran into
this problem and that problem, what do you think about that?‚Äù Then I get to think about other
people‚Äôs problems, and that‚Äôs really interesting because I get to learn a lot from that.</p>

<p>So, from my point of view, the selfish reason for going to conferences is really to learn from other
people, what their experiences have been, and to help share the experiences that we‚Äôve made in the
hope that other people will find them useful as well. But fundamentally, a conference talk is often
an introduction to a subject, whereas a paper is a deep analysis of a very narrow question. I think
those are different genres and I think we need both of them.</p>

<p><strong>Vadim</strong>: And the last question. How do you personally grow as a professional engineer and a
researcher? Could you please recommend any conferences, blogs, books, communities for those who wish
to develop themselves in the field of distributed systems?</p>

<p><strong>Martin</strong>: That‚Äôs a good question. Certainly, there are things to listen to and to read. There‚Äôs no
shortage of conference talks that have been recorded and put online. There are books like my own
book for example, which provides a bit of an introduction to the topic, but also lots of references
to further reading. So if there are any particular detailed questions that you‚Äôre interested in, you
can follow those references and find the original papers where these ideas were discussed. They can
be a very valuable way of learning about something in greater depth.</p>

<p>A really important part is also trying to implement things and seeing how they work out in practice,
and talking to other people and sharing your experiences. Part of the value of a conference is that
you get to talk to other people as well, live. But you can have that through other mechanisms as
well; for example, there‚Äôs a Slack channel that people have set up for people <a href="https://dist-sys-slack.herokuapp.com/">interested in
distributed systems</a>. If that‚Äôs your thing you can join that.
You can, of course, talk to your colleagues in your company and try to learn from them. I don‚Äôt
think there‚Äôs one right way of doing this ‚Äî there are many different ways through which you can
learn and get a deeper experience, and different paths will work for different people.</p>

<p><strong>Vadim</strong>: Thank you very much for your advice and interesting discussion! It has been a pleasure
talking to you.</p>

<p><strong>Martin</strong>: No problem, yeah, it‚Äôs been nice talking to you.</p>

<p><strong>Vadim</strong>: Let‚Äôs meet on July 11 <a href="https://hydraconf.com/?utm_source=medium&amp;utm_medium=kleppman">at the conference</a>.</p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Should you put several event types in the same Kafka topic?</title>
                <link>http://martin.kleppmann.com/2018/01/18/event-types-in-kafka-topic.html</link>
                <comments>http://martin.kleppmann.com/2018/01/18/event-types-in-kafka-topic.html#disqus_thread</comments>
                <pubDate>Thu, 18 Jan 2018 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2018/01/18/event-types-in-kafka-topic.html</guid>
                
                <description><![CDATA[ This article was originally published on the Confluent blog. It has also been translated into Chinese. If you adopt a streaming data platform such as Apache Kafka, one of the most important questions to answer is: what topics are you going to use? In particular, if you have a bunch... ]]></description>
                <content:encoded><![CDATA[
                    <p><em>This article was originally published
<a href="https://www.confluent.io/blog/put-several-event-types-kafka-topic/">on the Confluent blog</a>.
It has also been <a href="http://www.infoq.com/cn/articles/event-types-in-kafka-topic">translated into Chinese</a>.</em></p>

<p>If you adopt a streaming data platform such as <a href="http://kafka.apache.org/">Apache Kafka</a>, one of the most important questions to answer is: <em>what topics are you going to use?</em>
In particular, if you have a bunch of different events that you want to publish to Kafka as messages, do you put them in the same topic, or do you split them across different topics?</p>

<p>The most important function of a topic is to allow a consumer to specify which subset of messages it wants to consume.
At the one extreme, putting absolutely all your data in a single topic is probably a bad idea, since it would mean consumers have no way of selecting the events of interest ‚Äì they would just get everything.
At the other extreme, having millions of different topics is also a bad idea, since each topic in Kafka has a cost, and thus having a large number of topics will harm performance.</p>

<p>Actually, from a performance point of view, it‚Äôs the number of <em>partitions</em> that matters.
But since each topic in Kafka has at least one partition, if you have <em>n</em> topics, you inevitably have at least <em>n</em> partitions.
A while ago, <a href="https://www.confluent.io/blog/how-to-choose-the-number-of-topicspartitions-in-a-kafka-cluster/">Jun Rao wrote a blog post</a> explaining the cost of having many partitions (end-to-end latency, file descriptors, memory overhead, recovery time after a failure).
As a rule of thumb, if you care about latency, you should probably aim for (order of magnitude) hundreds of topic-partitions per broker node.
If you have thousands, or even tens of thousands of partitions per node, your latency will suffer.</p>

<p>That performance argument provides some guidance for designing your topic structure: if you‚Äôre finding yourself with many thousands of topics, it would be advisable to merge some of the fine-grained, low-throughput topics into coarser-grained topics, and thus reduce the proliferation of partitions.</p>

<p>However, performance is not the end of the story.
Even more important, in my opinion, are the data integrity and data modelling aspects of your topic structure.
We will discuss those in the rest of this article.</p>

<h2 id="topic--collection-of-events-of-the-same-type">Topic = collection of events of the same type?</h2>

<p>The common wisdom (according to several conversations I‚Äôve had, and according to a <a href="https://groups.google.com/forum/#!topic/confluent-platform/XQTjNJd-TrU">mailing list thread</a>) seems to be: put all events of the same type in the same topic, and use different topics for different event types.
That line of thinking is reminiscent of relational databases, where a table is a collection of records with the same type (i.e. the same set of columns), so we have an analogy between a relational table and a Kafka topic.</p>

<p>The <a href="https://docs.confluent.io/current/schema-registry/docs/index.html">Confluent Avro Schema Registry</a> has traditionally reinforced this pattern, because it encourages you to use the same Avro schema for all messages in a topic.
That schema can be evolved while maintaining compatibility (e.g. by adding optional fields), but ultimately all messages have been expected to conform to a certain record type.
We‚Äôll revisit this later in the post, after we‚Äôve covered some more background.</p>

<p>For some types of streaming data, such as logged activity events, it makes sense to require that all messages in the same topic conform to the same schema.
However, some people are using Kafka for more database-like purposes, such as <a href="https://docs.microsoft.com/en-us/azure/architecture/patterns/event-sourcing">event sourcing</a>, or <a href="https://www.confluent.io/blog/build-services-backbone-events/">exchanging data between microservices</a>.
In this context, I believe it‚Äôs less important to define a topic a grouping of messages with the same schema.
Much more important is the fact that Kafka maintains <strong>ordering</strong> of messages within a topic-partition.</p>

<p>Imagine a scenario in which you have some entity (say, a customer), and many different things can happen to that entity: a customer is created, a customer changes their address, a customer adds a new credit card to their account, a customer makes a customer support enquiry, a customer pays an invoice, a customer closes their account.</p>

<p>The order of those events matters.
For example, we might expect that a customer is created before anything else can happen to a customer, and we might expect that after a customer closes their account nothing more will happen to them.
When using Kafka, you can preserve the order of those events by putting them all in the same partition.
In this example, you would use the customer ID as the partitioning key, and then put all these different events in the <strong>same</strong> topic.
They must be in the same topic because different topics mean different partitions, and ordering is not preserved across partitions.</p>

<h2 id="ordering-problems">Ordering problems</h2>

<p>If you did use different topics for (say) the customerCreated, customerAddressChanged, and customerInvoicePaid events, then a consumer of those topics may see the events in a nonsensical order.
For example, the consumer may see an address change for a customer that does not exist (because it has not yet been created, since the corresponding customerCreated event has been delayed).</p>

<p>The risk of reordering is particularly high if a consumer is shut down for a while, perhaps for maintenance or to deploy a new version.
While the consumer is stopped, events continue to be published, and those events are stored in the selected topic-partition on the Kafka brokers.
When the consumer starts up again, it consumes the backlog of events from all of its input partitions.
If the consumer has only one input, that‚Äôs no problem: the pending events are simply processed sequentially in the order they are stored.
But if the consumer has several input topics, it will pick input topics to read in some arbitrary order.
It may read all of the pending events from one input topic before it reads the backlog on another input topic, or it may interleave the inputs in some way.</p>

<p>Thus, if you put the customerCreated, customerAddressChanged, and customerInvoicePaid events in three separate topics, the consumer may well see all of the customerAddressChanged events before it sees any of the customerCreated events.
And so it is likely that the consumer will see a customerAddressChanged event for a customer that, according to its view of the world, has not yet been created.</p>

<p>You might be tempted to attach a timestamp to each message and use that for event ordering.
That might just about work if you are importing events into a data warehouse, where you can order the events after the fact.
But in a stream process, timestamps are not enough: if you get an event with a certain timestamp, you don‚Äôt know whether you still need to wait for some previous event with a lower timestamp, or if all previous events have arrived and you‚Äôre ready to process the event.
And relying on clock synchronisation generally leads to nightmares; for more detail on the problems with clocks, I refer you to Chapter 8 of <a href="http://dataintensive.net/">my book</a>.</p>

<h2 id="when-to-split-topics-when-to-combine">When to split topics, when to combine?</h2>

<p>Given that background, I will propose some rules of thumb to help you figure out which things to put in the same topic, and which things to split into separate topics:</p>

<ol>
  <li>
    <p>The most important rule is that <strong>any events that need to stay in a fixed order must go in the same topic</strong> (and they must also use the same partitioning key).
Most commonly, the order of events matters if they are about the same entity.
So, as a rule of thumb, we could say that all events <strong>about the same entity</strong> need to go in the same topic.</p>

    <p>The ordering of events is particularly relevant if you are using an <a href="https://msdn.microsoft.com/en-us/library/jj591559.aspx">event sourcing</a> approach to data modelling.
Here, the state of an <a href="https://www.martinfowler.com/bliki/DDD_Aggregate.html">aggregate object</a> is derived from a log of events by replaying them in a particular order.
Thus, even though there may be many different event types, all of the events that define an aggregate must go in the same topic.</p>
  </li>
  <li>
    <p>When you have events about different entities, should they go in the same topic or different topics?
I would say that if one entity depends on another (e.g. an address belongs to a customer), or if they are often needed together, they might as well go in the same topic.
On the other hand, if they are unrelated and managed by different teams, they are better put in separate topics.</p>

    <p>It also depends on the throughput of events: if one entity type has a much higher rate of events than another entity type, they are better split into separate topics, to avoid overwhelming consumers who only want the entity with low write throughput (see point four).
But several entities that all have a low rate of events can easily be merged.</p>
  </li>
  <li>
    <p>What if an event involves several entities?
For example, a purchase relates a product and a customer, and a transfer from one account to another involves at least those two accounts.</p>

    <p>I would recommend initially recording the event as a single atomic message, and not splitting it up into several messages in several topics.
It‚Äôs best to record events exactly as you receive them, in a form that is <a href="https://vimeo.com/123985284">as raw as possible</a>.
You can always split up the compound event later, using a stream processor ‚Äì but it‚Äôs much harder to reconstruct the original event if you split it up prematurely.
Even better, you can give the initial event a unique ID (e.g. a UUID); that way later on when you split the original event into one event for each entity involved, you can carry that ID forward, making the provenance of each event traceable.</p>
  </li>
  <li>
    <p>Look at the number of topics that a consumer needs to subscribe to.
If several consumers all read a particular group of topics, this suggests that maybe those topics <a href="http://grokbase.com/t/kafka/users/15a7k5f1rr/mapping-events-to-topics">should be combined</a>.</p>

    <p>If you combine the fine-grained topics into coarser-grained ones, some consumers may receive unwanted events that they need to ignore.
That is not a big deal: consuming messages from Kafka is very cheap, so even if a consumer ends up ignoring half of the events, the cost of this overconsumption is probably not significant.
Only if the consumer needs to ignore the vast majority of messages (e.g. 99.9% are unwanted) would I recommend splitting the low-volume event stream from the high-volume stream.</p>
  </li>
  <li>
    <p>A changelog topic for a <a href="http://kafka.apache.org/10/documentation/streams/developer-guide#streams_duality">Kafka Streams state store</a> (KTable) should be a separate from all other topics.
In this case, the topic is managed by Kafka Streams process, and it should not be shared with anything else.</p>
  </li>
  <li>
    <p>Finally, what if none of the rules above tell you whether to put some events in the same topic or in different topics?
Then by all means group them by event type, by putting events of the same type in the same topic.
However, I think this rule is the least important of all.</p>
  </li>
</ol>

<h2 id="schema-management">Schema management</h2>

<p>If you are using a data encoding such as JSON, without a statically defined schema, you can easily put many different event types in the same topic.
However, if you are using a schema-based encoding such as Avro, a bit more thought is needed to handle multiple event types in a single topic.</p>

<p>As mentioned above, the Avro-based <a href="https://docs.confluent.io/current/schema-registry/docs/index.html">Confluent Schema Registry for Kafka</a> currently relies on the assumption that there is one schema for each topic (or rather, one schema for the key and one for the value of a message).
You can register new versions of a schema, and the registry checks that the schema changes are forward and backward compatible.
A nice thing about this design is that you can have different producers and consumers using different schema versions at the same time, and they still remain compatible with each other.</p>

<p>More precisely, when Confluent‚Äôs Avro serializer registers a schema in the registry, it does so under a <em>subject name</em>.
By default, that subject is <code>&lt;topic&gt;-key</code> for message keys and <code>&lt;topic&gt;-value</code> for message values.
The schema registry then checks the mutual compatibility of all schemas that are registered under a particular subject.</p>

<p>I have recently <a href="https://github.com/confluentinc/schema-registry/pull/680">made a patch to the Avro serializer</a> that makes the compatibility check more flexible.
The patch adds two new configuration options: <code>key.subject.name.strategy</code> (which defines how to construct the subject name for message keys), and <code>value.subject.name.strategy</code> (how to construct the subject name for message values).
The options can take one of the following values:</p>

<ul>
  <li><code>io.confluent.kafka.serializers.subject.TopicNameStrategy</code> (default): The subject name for message keys is <code>&lt;topic&gt;-key</code>, and <code>&lt;topic&gt;-value</code> for message values.
This means that the schemas of all messages in the topic must be compatible with each other.</li>
  <li><code>io.confluent.kafka.serializers.subject.RecordNameStrategy</code>: The subject name is the fully-qualified name of the Avro record type of the message.
Thus, the schema registry checks the compatibility for a particular record type, regardless of topic. This setting allows any number of different event types in the same topic.</li>
  <li><code>io.confluent.kafka.serializers.subject.TopicRecordNameStrategy</code>: The subject name is <code>&lt;topic&gt;-&lt;type&gt;</code>, where <code>&lt;topic&gt;</code> is the Kafka topic name, and <code>&lt;type&gt;</code> is the fully-qualified name of the Avro record type of the message.
This setting also allows any number of event types in the same topic, and further constrains the compatibility check to the current topic only.</li>
</ul>

<p>With this new feature, you can easily and cleanly put all the different events for a particular entity in the same topic.
Now you can freely choose the granularity of topics based on the criteria above, and not be limited to a single event type per topic.</p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Drawing a map of distributed data systems</title>
                <link>http://martin.kleppmann.com/2017/03/15/map-distributed-data-systems.html</link>
                <comments>http://martin.kleppmann.com/2017/03/15/map-distributed-data-systems.html#disqus_thread</comments>
                <pubDate>Wed, 15 Mar 2017 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2017/03/15/map-distributed-data-systems.html</guid>
                
                <description><![CDATA[ How we created an illustrated guide to help you find your way through the data landscape. Designing Data-Intensive Applications, the book I‚Äôve been working on for four years, is finally finished, and should be available in your favorite bookstore in the next week or two. An incomplete beta (Early Release)... ]]></description>
                <content:encoded><![CDATA[
                    <p><strong>How we created an illustrated guide to help you find your way through the data landscape.</strong></p>

<p><a href="http://dataintensive.net/"><em>Designing Data-Intensive Applications</em></a>, the book I‚Äôve been working on
for four years, is finally finished, and should be available in your favorite bookstore in the next
week or two. An incomplete beta (Early Release) edition has been available for the last 2¬Ω years as
I continued working on the final chapters.</p>

<p>Throughout that process, we have been quietly working on a surprise. Something that has not been
part of any of the Early Releases of the book. In fact, something that I have never seen in any tech
book. And today we are excited to share it with you.</p>

<p>In <em>Designing Data-Intensive Applications</em>, each of the 12 chapters is accompanied by a map. The map
is a kind of graphical table of contents of the chapter, showing how some of the main ideas in the
chapter relate to each other.</p>

<p>Here is an example, from Chapter 3 (on storage engines):</p>

<p><a href="https://d3ansictanv2wj.cloudfront.net/ch03-map_lg-12f05f19df9e49098d509847518f1c03.jpg"><img src="/2017/03/ch3-map.jpg" width="550" height="419" alt="Chapter 3 map illustration from Designing Data-Intensive Applications" /></a><br />
<small>Figure 1. Map illustration from <em>Designing Data-Intensive Applications</em>, O‚ÄôReilly Media, 2017.</small></p>

<p>Don‚Äôt take it too seriously‚Äîsome of it is a little tongue-in-cheek, we have taken some artistic
license, and the things included on the map are not exhaustive.</p>

<p>But it does reflect the structure of the chapter: political or geographic regions represent ways of
doing something, and cities represent particular implementations of those approaches. Similar things
are more likely to be close together, and roads or rivers represent concepts that connect different
implementations or regions.</p>

<p>Most computing books describe one particular piece of software and discuss all the aspects of how it
works. This book is structured differently: it starts with the concepts‚Äîdiscussing the high-level
approaches of how you might solve some problem, and comparing the pros and cons of each‚Äîand then
points out which pieces of software use which approach. The maps use the same structure: the region
in which a city is located tells you what approach it uses.</p>

<p>For example, in the map above, you can see a high-level subdivision into two countries: transaction
processing and analytics. Within transaction processing, there are two regions: log-structured
storage and B-trees, which are two ways of implementing OLTP storage engines. Within the B-tree
region, you see databases like MySQL and PostgreSQL<sup><a href="#_ftn1">[1]</a></sup>, while within
the log-structured region you see databases like Cassandra and HBase. On the analytics side, you can
see that the mountain range representing column storage reaches into both the data warehousing and
the Hadoop regions, since the approach applies to both.</p>

<p>The maps are in black and white, both because of practicalities of printing and also because I was
looking for a
<a href="http://www.tolkien.co.uk/file/IfbTdA8/5d04a105-e66b-4d9b-b218-928c691eb83d.jpg">Tolkien-esque style</a>.
You are, of course, welcome to color them in yourself. In fact, by coloring them in, you would be
following a fine tradition: for over three centuries, maps were printed in black and white from an
<a href="http://www.maphistory.info/understanding.html">engraved copper plate</a>, and then colored in by hand.</p>

<p>Each of the chapters has a map like that, focusing on the particular aspects discussed in that
chapter. This means that some cities appear on multiple islands‚Äîthe data landscape is
multidimensional, so a city may lie in more than one (conceptual) realm. For example, the map below
is for Chapter 5 (on the topic of replication):</p>

<p><a href="https://d3ansictanv2wj.cloudfront.net/ch05-map_lg-32ac8f29402e391034f664a63461eef5.jpg"><img src="/2017/03/ch5-map.jpg" width="550" height="550" alt="Chapter 5 map illustration from Designing Data-Intensive Applications" /></a><br />
<small>Figure 2. Map illustration from <em>Designing Data-Intensive Applications</em>, O‚ÄôReilly Media, 2017.</small></p>

<p>Cities representing Cassandra, MongoDB, MySQL, and others appear on both this map, the Chapter 3 map
above, and some other maps, too.</p>

<p>Shipping routes connect some of the ports shown in the maps, in cases where there is a noteworthy
link between chapters. Most of the maps are of islands, but there are some exceptions. (I won‚Äôt give
away too much, but I just want to say‚Ä¶beware of the
<a href="https://en.wikipedia.org/wiki/Kraken">Kraken</a>.)</p>

<p>I am incredibly delighted that O‚ÄôReilly was willing to take on this crazy idea of creating maps. It
took a whole team to make them happen: from my
<a href="https://www.dropbox.com/s/yvuj7rqg66i93os/chapter3-map.jpg?dl=0">rough pencil sketches</a> (which
showed the structure but had absolutely zero artistic value),
<a href="http://shabbirdiwan.com/category/technique/scratch-board/">Shabbir Diwan</a>,
<a href="http://www.ediefreedman.com/">Edie Freedman</a>, and <a href="http://www.oreilly.com/pub/au/3771">Ron Bilodeau</a>
created the beautifully illustrated versions you see above, and
<a href="https://twitter.com/cmariebeau">Marie Beaugureau</a> patiently managed several rounds of revisions, in
which we painstakingly polished all the details.</p>

<p>Perhaps you‚Äôre curious to know how we got onto the idea of creating maps. Early on in the Early
Release of the book, some readers told me they would love some kind of flowchart to help them decide
quickly which database they should use for their application. Such flowcharts
<a href="https://medium.baqend.com/nosql-databases-a-survey-and-decision-guidance-ea7823a822d">have been attempted</a>,
but I never liked them much‚Äîit is tempting to read them out of context and jump to conclusions too
quickly, and they have to simplify the issues to the point of almost being intellectually dishonest.</p>

<p>My goal for <em>Designing Data-Intensive Applications</em> was different. I can‚Äôt in good faith give you
a recommendation for one particular tool because I don‚Äôt know enough about your particular
requirements. However, I can teach you what questions to ask and how to evaluate vendors‚Äô claims
critically. That requires more subtlety and detail than can be expressed in a one-page flowchart,
which is why the book is 600 pages long, not one page.</p>

<p>However, I did think that some kind of graphical representation of the main ideas and structure
would be useful. I thought about <a href="https://en.wikipedia.org/wiki/Venn_diagram">Venn diagrams</a>, but
they‚Äôre excruciatingly boring. I thought about <a href="https://en.wikipedia.org/wiki/Mind_map">mind maps</a>,
and then started taking the ‚Äúmaps‚Äù bit more literally. I thought about the
<a href="https://www.goodreads.com/book/show/1175738.The_Atlas_of_Experience">Atlas of Experience</a> by Louise
van Swaaij and Jean Klare, a sublime book that represents aspects of human life as fictitious
<a href="http://www.imaginaryatlas.com/wp-content/uploads/2013/04/worldof-experience.jpg">places on a geographic map</a>.
(It is <a href="http://www.deharmonie.nl/titel/grote-atlas-van-de-belevingswereld/">originally in Dutch</a>;
the English translation is, sadly, out of print.)
<a href="https://www.goodreads.com/book/show/378.The_Phantom_Tollbooth">The Phantom Tollbooth</a> by Norton
Juster and Jules Feiffer does
<a href="http://bowenpeters.weebly.com/uploads/8/1/1/9/8119969/phantom_tollbooth_map.jpg">a similar thing</a>.</p>

<p>Closer to technology, similar map-style visualizations have been used to
<a href="https://xkcd.com/802/">represent online communities</a>, to visualize the
<a href="https://hpi.de/naumann/projects/rdbms-genealogy.html">history of relational databases</a> and
<a href="http://www.oreilly.com/go/languageposter">programming languages</a>, and to
<a href="https://twitter.com/linclark/status/708071521286266881">explain libraries related to Facebook‚Äôs React</a>.
I simply love the style.</p>

<p>Other inspirations for me are the ornate maps produced in the medieval and renaissance periods,
especially when it comes to whimsical
<a href="https://www.bl.uk/shop/sea-monsters-on-medieval-and-renaissance-maps/p-284">sea monsters</a>.
For example, around 1590, Abraham Ortelius published a
<a href="https://commons.wikimedia.org/wiki/File:Abraham_Ortelius-Islandia-ca_1590.jpg">wonderful map of Iceland</a>,
with <a href="https://en.wikipedia.org/wiki/Hekla">Mount Hekla</a> spewing fire, and surrounded by fantastical
sea monsters:</p>

<p><a href="/2017/03/ortelius-iceland.jpg"><img src="/2017/03/ortelius-iceland.jpg" width="550" height="410" alt="‚ÄúIslandia‚Äù map by Abraham Ortelius, ca. 1590." /></a><br />
<small>Figure 3. ‚ÄúIslandia‚Äù map by Abraham Ortelius, ca. 1590. Source:
<a href="https://commons.wikimedia.org/wiki/File:Abraham_Ortelius-Islandia-ca_1590.jpg">Wikimedia Commons</a></small></p>

<p>I feel those maps and sea monsters reflect the 16th-century sense of excitement to explore the earth
and discover new places, as well as the dangers of sailing across unknown seas. And perhaps a bit of
that excitement exists today in our exploration of technologies for storing, processing, and using
data. There seems to be a lot of potential, but we also don‚Äôt really know what we‚Äôre doing, and it‚Äôs
sometimes a bit dangerous (raise your hand if you‚Äôve lost data at some point because something went
wrong).</p>

<p>We hope the maps in <em>Designing Data-Intensive Applications</em> will help convey some of that
excitement, and also make you smile. In both the print and ebook editions, the map for each chapter
appears at the start of each chapter.</p>

<p>What‚Äôs more, we have taken all the individual chapter maps and assembled them into a poster‚Äîan
archipelago of islands representing technologies in the sea of distributed data. The poster also
includes some bonus sea monsters (of course).</p>

<p>If you are at
<a href="https://conferences.oreilly.com/strata/strata-ca">Strata + Hadoop World, San Jose</a>
this week, you can drop by the O‚ÄôReilly booth to pick up a free print of the poster so you have
something geeky and cool to hang on the wall in your office. Alternatively,
<a href="https://d3ansictanv2wj.cloudfront.net/ddia-poster-web-89b1c62f6eb4b57336c6cbe2148cc9a9.jpg">you can download a JPG version for free from the O‚ÄôReilly website</a>
for your personal, noncommercial use.</p>

<p>We hope you enjoy <em>Designing Data-Intensive Applications</em> and the maps as much as we enjoyed making them!</p>

<p><a href="/2017/03/ddia-poster.jpg"><img src="/2017/03/ddia-poster.jpg" width="550" height="733" alt="Me holding the poster with the maps" /></a><br />
<small>Figure 4. Me holding the poster with the maps.</small></p>

<p id="_ftn1">
[1] Footnote for the <a href="http://tirania.org/blog/archive/2011/Feb-17.html">well-actually</a>
crowd: yes, I know about hash indexes and GiST in PostgreSQL, various other index types in other
databases, and the fact that in MySQL the index type is actually a matter of the storage engine
(such as InnoDB), but those details are beside the point here. I am highlighting a dichotomy between
a page-oriented update-in-place approach and a log-structured, compaction-based approach. This
distinction is best explained with concrete examples, and the graphical representation cannot
capture all the subtleties that are discussed in the text of the book.
</p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>The probability of data loss in large clusters</title>
                <link>http://martin.kleppmann.com/2017/01/26/data-loss-in-large-clusters.html</link>
                <comments>http://martin.kleppmann.com/2017/01/26/data-loss-in-large-clusters.html#disqus_thread</comments>
                <pubDate>Thu, 26 Jan 2017 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2017/01/26/data-loss-in-large-clusters.html</guid>
                
                <description><![CDATA[ This blog post uses MathJax to render mathematics. You need JavaScript enabled for MathJax to work. Many distributed storage systems (e.g. Cassandra, Riak, HDFS, MongoDB, Kafka, ‚Ä¶) use replication to make data durable. They are typically deployed in a ‚ÄúJust a Bunch of Disks‚Äù (JBOD) configuration ‚Äì that is, without... ]]></description>
                <content:encoded><![CDATA[
                    <p><em>This blog post uses <a href="https://www.mathjax.org/">MathJax</a> to render mathematics. You need JavaScript
enabled for MathJax to work.</em></p>

<p>Many distributed storage systems (e.g. Cassandra, Riak, HDFS, MongoDB, Kafka, ‚Ä¶) use replication
to make data durable. They are typically deployed in a <a href="https://en.wikipedia.org/wiki/Non-RAID_drive_architectures">‚ÄúJust a Bunch of Disks‚Äù</a> (JBOD)
configuration ‚Äì that is, without RAID to handle disk failure. If one of the disks on a node dies,
that disk‚Äôs data is simply lost. To avoid losing data permanently, the database system keeps a copy
(replica) of the data on some other disks on other nodes.</p>

<p>The most common replication factor is 3 ‚Äì that is, the database keeps copies of every piece of data
on three separate disks attached to three different computers. The reasoning goes something like
this: disks only die once in a while, so if a disk dies, you have a bit of time to replace it, and
then you still have two copies from which you can restore the data onto the new disk. The risk that
a second disk dies before you restore the data is quite low, and the risk that all three disks die
at the same time is so tiny that you‚Äôre more likely to get hit by an asteroid.</p>

<p>As a back-of-the-envelope calculation, if the probability of a single disk failing within some time
period is 0.1% (to pick an arbitrary number), then the probability of two disks failing is
(0.001)<sup>2</sup>¬†=¬†10<sup>-6</sup>, and the probability of all three disks failing is
(0.001)<sup>3</sup>¬†=¬†10<sup>-9</sup>, or one in a billion. This calculation assumes that
one disk‚Äôs failure is independent from another disk‚Äôs failure ‚Äì which is not actually true, since
for example disks from the same manufacturing batch may show correlated failures ‚Äì but it‚Äôs a good
enough approximation for our purposes.</p>

<p>So far the common wisdom. It sounds reasonable, but unfortunately it turns out to be untrue for many
data storage systems. In this post I will show why.</p>

<h2 id="its-so-easy-to-lose-data-la-la-laaa">It‚Äôs so easy to lose data, la la laaa</h2>

<p>If your database cluster really only consists of three machines, then the probability of all three
of them dying simultaneously is indeed very low (ignoring correlated faults, such as the datacenter
burning down). However, as you move to larger clusters, the probabilities change. The more nodes and
disks you have in your cluster, the more likely it is that you lose data.</p>

<p>This is a counter-intuitive idea. ‚ÄúSurely,‚Äù you think, ‚Äúevery piece of data is still replicated on
three disks. The probability of a disk dying doesn‚Äôt depend on the size of the cluster. So why
should the size of the cluster matter?‚Äù But I calculated the probabilities and drew a graph, and it
looked like this:</p>

<p><a href="/2017/01/dataloss.png"><img src="/2017/01/dataloss.png" width="550" alt="Graph of data loss probability depending on the number of nodes in the cluster" /></a></p>

<p>To be clear, this isn‚Äôt the probability of a single node failing ‚Äì this is the probability of
<strong>permanently losing all three replicas</strong> of some piece of data, so restoring from backup (if you
have one) is the only remaining way to recover that data. The bigger your cluster, the more likely
you are haemorrhaging data. This is probably not what you intended when you decided to pay for
a replication factor of 3.</p>

<p>The y axis on that graph is a bit arbitrary, and depends on a lot of assumptions, but the direction
of the line is scary. Under the assumption that a node has a 0.1% chance of dying within some time
period, the graph shows that in a 8,000-node cluster, the chance of permanently losing all three
replicas of some piece of data (within the same time period) is about 0.2%. Yes, you read that
correctly: the risk of losing <strong>all three</strong> copies of some data is <strong>twice as great</strong> as the risk of
losing a single node! What is the point of all this replication again?</p>

<p>The intuition behind this graph is as follows: in an 8,000-node cluster it‚Äôs almost certain that
a <em>few</em> nodes are always dead at any given moment. That is normally not a problem: a certain rate of
churn and node replacement is expected and a part of routine maintenance. However, if you get
unlucky, there is <em>some piece of data</em> whose three replicas just happen to be three of those nodes
that have died ‚Äì and if this is the case, that piece of data is gone forever. The data that is lost
is only a small fraction of the total dataset in the cluster, but still that‚Äôs not great: when you
use a replication factor of 3, you generally mean ‚ÄúI really don‚Äôt want to lose this data‚Äù, not ‚ÄúI
don‚Äôt mind occasionally losing a bit of this data, as long as it‚Äôs not too much‚Äù. Maybe that piece
of lost data was a particularly important one.</p>

<p>The probability that all three replicas are dead nodes depends crucially on the algorithm that the
system uses to assign data to replicas. The graph above is calculated under the assumption that the
data is split into a number of partitions (shards), and that each partition is stored on three
<em>randomly chosen</em> nodes (or pseudo-randomly with a hash function). This is the case with
<a href="https://www.akamai.com/kr/ko/multimedia/documents/technical-publication/consistent-hashing-and-random-trees-distributed-caching-protocols-for-relieving-hot-spots-on-the-world-wide-web-technical-publication.pdf">consistent hashing</a>, used in Cassandra and Riak, among others (as far as I know). With
other systems I‚Äôm not sure how the replica assignment works, so I‚Äôd appreciate any insights from
people who know about the internals of various storage systems.</p>

<h2 id="calculating-the-probability-of-data-loss">Calculating the probability of data loss</h2>

<p>Let me show you how I calculated that graph above, using a probabilistic model of a replicated
database.</p>

<p>Let‚Äôs assume that the probability of losing an individual node is \(p=P(\text{node loss})\). I am
going to ignore time in this model, and simply look at the probability of failure in some arbitrary
time period. For example, we could assume that \(p=0.001\) is the probability of a node failing
within a given day, which would make sense if it takes about a day to replace the node and restore
the lost data onto new disks. For simplicity I won‚Äôt distinguish between node failure and
<a href="https://www.backblaze.com/blog/hard-drive-reliability-update-september-2014/">disk failure</a>, and I will consider only permanent failures (ignoring crashes where
the node comes back again after a reboot).</p>

<p>Let \(n\) be the number of nodes in the cluster. Then the probability that \(f\) out of \(n\)
nodes have failed (assuming that failures are independent) is given by the
<a href="https://en.wikipedia.org/wiki/Binomial_distribution">binomial distribution</a>:</p>

<p>\[ P(f \text{ nodes failed}) = \binom{n}{f} \, p^f \, (1-p)^{n-f} \]</p>

<p>The term \(p^f\) is the probability that \(f\) nodes have failed, the term \((1-p)^{n-f}\) is
the probability that the remaining \(n-f\) have not failed, and \(\binom{n}{f}\) is the number
of different ways of picking \(f\) out of \(n\) nodes. \(\binom{n}{f}\) is pronounced
‚Äún choose f‚Äù, and it is defined as:</p>

<p>\[ \binom{n}{f} = \frac{n!}{f! \; (n-f)!} \]</p>

<p>Let \(r\) be the replication factor (typically \(r=3\)). If we assume that \(f\) out of
\(n\) nodes have failed, what is the probability that a particular partition has all \(r\)
replicas on failed nodes?</p>

<p>Well, in a system that uses consistent hashing, each partition is assigned to nodes independently
and randomly (or pseudo-randomly). For a given partition, there are \(\binom{n}{r}\) different
ways of assigning the \(r\) replicas to nodes, and these assignments are all equally likely to
occur. Moreover, there are \(\binom{f}{r}\) different ways of choosing \(r\) replicas out of
\(f\) failed nodes ‚Äì these are the ways in which all \(r\) replicas can be assigned to failed
nodes. We then work out the fraction of the assignments that result in all replicas having failed:</p>

<p>\[ P(\text{partition lost} \mid f \text{ nodes failed}) = \frac{\binom{f}{r}}{\binom{n}{r}} =
    \frac{f! \; (n-r)!}{(f-r)! \; n!} \]</p>

<p>(The vertical bar after ‚Äúpartition lost‚Äù is pronounced ‚Äúgiven that‚Äù, and it indicates a
<a href="https://en.wikipedia.org/wiki/Conditional_probability">conditional probability</a>: the probability is given <em>under the assumption</em> that \(f\)
nodes have failed.)</p>

<p>So that‚Äôs the probability that all replicas of one particular partition has been lost. What about
a cluster with \(k\) partitions? If one or more partitions have been lost, we have lost data.
Thus, in order to not lose data, we require that all \(k\) partitions are not lost:</p>

<p>\begin{align}
P(\text{data loss} \mid f \text{ nodes failed})
    &amp;= 1 - P(\text{partition not lost} \mid f \text{ nodes failed})^k \\
    &amp;= 1 - \left( 1 - \frac{f! \; (n-r)!}{(f-r)! \; n!} \right)^k
\end{align}</p>

<p>Cassandra and Riak call partitions ‚Äúvnodes‚Äù instead, but they are the same thing. In general, the
number of partitions \(k\) is independent from the number of nodes \(n\). In the case of
Cassandra, there is usually a <a href="http://www.datastax.com/dev/blog/virtual-nodes-in-cassandra-1-2">fixed number of partitions per node</a>; the default
is \(k=256\,n\) (configured by the <code>num_tokens</code> parameter), and this is also what I assumed for the
graph above. In Riak, the number of partitions is <a href="https://docs.basho.com/riak/kv/2.1.4/setup/planning/cluster-capacity/#ring-size-number-of-partitions">fixed when you create the cluster</a>, but
generally more nodes also mean more partitions.</p>

<p>With all of this in place, we can now work out the probability of losing one or more partitions in
a cluster of size \(n\) with a replication factor of \(r\). If the number of failures \(f\) is
less than the replication factor, we can be sure that no data is lost. Thus, we need to add up the
probabilities for all possible numbers of failures \(f\) with \(r \le f \le n\):</p>

<p>\begin{align}
P(\text{data loss})
    &amp;= \sum_{f=r}^{n} \; P(\text{data loss} \;\cap\; f \text{ nodes failed}) \\
    &amp;= \sum_{f=r}^{n} \; P(f \text{ nodes failed}) \; P(\text{data loss} \mid f \text{ nodes failed}) \\
    &amp;= \sum_{f=r}^{n} \binom{n}{f} \, p^f \, (1-p)^{n-f}
       \left[ 1 - \left( 1 - \frac{f! \; (n-r)!}{(f-r)! \; n!} \right)^k \right]
\end{align}</p>

<p>That is a bit of a mouthful, but I think it‚Äôs accurate. And if you plug in \(r=3\),
\(p=0.001\) and \(k=256\,n\), and vary \(n\) between 3 and 10,000, then you
get the graph above. I wrote <a href="https://gist.github.com/ept/1e094caaab5fa6471f529f589c4aaaf0">a little Ruby program</a> to do the calculation.</p>

<p>We can get a simpler approximation using the <a href="https://en.wikipedia.org/wiki/Boole%27s_inequality">union bound</a>:</p>

<p>\begin{align}
P(\text{data loss})
    &amp;= P(\ge\text{ 1 partition lost}) \\
    &amp;= P\left( \bigcup_{i=1}^k \text{partition } i \text{ lost} \right) \\
    &amp;\le k\, P(\text{partition lost}) = k\, p^r
\end{align}</p>

<p>Even though one partition failing is not independent from another partition failing, this
approximation still applies. And it seems to match the exact result quite closely: in the graph, the
data loss probability looks like a straight line, proportional to the number of nodes. The
approximation says that the probability is proportional to the number of partitions, which is
equivalent since we assumed a fixed 256 partitions per node.</p>

<p>Moreover, if we plug in the numbers for 10,000 nodes into the approximation, we get 
\(P(\text{data loss}) \le 256 \cdot 10^4 \cdot (10^{-3})^3 = 0.00256\), which matches the result
from the Ruby program very closely.</p>

<h2 id="and-in-practice">And in practice‚Ä¶?</h2>

<p>Is this a problem in practice? I don‚Äôt know. Mostly I think it‚Äôs an interesting and
counter-intuitive phenomenon. I‚Äôve heard rumours that it is causing real data loss at companies with
large database clusters, but I‚Äôve not seen the issue documented anywhere. If you‚Äôre aware of any
discussions on this topic, please point me at them.</p>

<p>The calculation indicates that in order to reduce the probability of data loss, you can reduce the
number of partitions or increase the replication factor. Using more replicas costs more, so it‚Äôs not
ideal for large clusters that are already expensive. However, the number of partitions presents an
interesting trade-off. Cassandra originally used one partition per node, but then
<a href="http://www.datastax.com/dev/blog/virtual-nodes-in-cassandra-1-2">switched to 256 partitions per node</a> a few years ago in order to achieve better
load distribution and more efficient rebalancing. The downside, as we can see from this calculation,
is a much higher probability of losing at least one of the partitions.</p>

<p>I think it‚Äôs probably possible to devise replica assignment algorithms in which the probability of
data loss does not grow with the cluster size, or at least does not grow as fast, but which
nevertheless have good load distribution and rebalancing properties. That would be an interesting
area to explore further. In that context, my colleague <a href="http://www.cl.cam.ac.uk/~sak70/">Stephan</a> pointed out that the expected
rate of data loss is constant in a cluster of a particular size, independent of the replica
assignment algorithm ‚Äì in other words, you can choose between a high probability of losing a small
amount of data, and a low probability of losing a large amount of data! Is the latter better?</p>

<p>You need fairly large clusters before this effect really shows up, but clusters of thousands of
nodes are used by various large companies, so I‚Äôd be interested to hear from people with operational
experience at such scale. If the probability of permanently losing data in a 10,000 node cluster is
really 0.25% per day, that would mean a 60% chance of losing data in a year. That‚Äôs way higher than
the ‚Äúone in a billion‚Äù getting-hit-by-an-asteroid probability that I talked about at the start.</p>

<p>Are the designers of distributed data systems aware of this issue? If I got this right, it‚Äôs
something that should be taken into account when designing replication schemes. Hopefully this blog
post will raise some awareness of the fact that just because you have three replicas you‚Äôre not
automatically guaranteed to be safe.</p>

<p><em>Thank you to <a href="https://twitter.com/matclayton">Mat Clayton</a> for bringing this issue to my attention, and to
<a href="http://www.cl.cam.ac.uk/~arb33/">Alastair Beresford</a>, <a href="http://www.cl.cam.ac.uk/~sak70/">Stephan Kollmann</a>, <a href="https://twitter.com/cmeik">Christopher Meiklejohn</a>,
and <a href="http://www.cl.cam.ac.uk/~drt24/">Daniel Thomas</a> for comments on a draft of this post.</em></p>


                ]]></content:encoded>
            </item>
        
    </channel>
</rss>
