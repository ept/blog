<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
    xmlns:content="http://purl.org/rss/1.0/modules/content/"
    xmlns:dc="http://purl.org/dc/elements/1.1/"
    xmlns:atom="http://www.w3.org/2005/Atom"
    xmlns:sy="http://purl.org/rss/1.0/modules/syndication/">

    <channel>
        <title>Martin Kleppmann's blog</title>
        <atom:link href="http://martin.kleppmann.com/feed.rss" rel="self" type="application/rss+xml" />
        <link>http://martin.kleppmann.com/</link>
        <description></description>
        <lastBuildDate>Mon, 07 Feb 2022 11:36:00 GMT</lastBuildDate>
        <language>en</language>
        <sy:updatePeriod>hourly</sy:updatePeriod>
        <sy:updateFrequency>1</sy:updateFrequency>

        
        
            <item>
                <title>Book Review: The Future of Fusion Energy</title>
                <link>http://martin.kleppmann.com/2022/01/03/future-of-fusion-energy.html</link>
                <comments>http://martin.kleppmann.com/2022/01/03/future-of-fusion-energy.html#disqus_thread</comments>
                <pubDate>Mon, 03 Jan 2022 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2022/01/03/future-of-fusion-energy.html</guid>
                
                <description><![CDATA[ I give a five-star ⭐️⭐️⭐️⭐️⭐️ rating to the following book: Jason Parisi and Justin Ball. The Future of Fusion Energy. World Scientific, 2019. ISBN 978-1-78634-749-7. Available from Amazon US, Amazon UK, and many other retailers. I came to this book looking for answers to questions such as: Is there still... ]]></description>
                <content:encoded><![CDATA[
                    <p>I give a five-star ⭐️⭐️⭐️⭐️⭐️ rating to the following book:</p>

<p>Jason Parisi and Justin Ball. <em>The Future of Fusion Energy</em>. World Scientific, 2019. ISBN 978-1-78634-749-7. Available from <a href="https://amzn.to/3sUypW6">Amazon US</a>, <a href="https://amzn.to/3eHCpkB">Amazon UK</a>, and many other retailers.</p>

<p style="text-align:center"><img src="/2022/01/fusion-book.jpg" alt="Cover of the book 'The Future of Fusion Energy'" width="70%" /></p>

<p>I came to this book looking for answers to questions such as: Is there still hope that a fusion power plant will ever be viable? If so, what exactly are the main obstacles on the way there? Why has progress in this field been so slow? And what should I make of the various startups claiming to have a fusion power plant just round the corner?</p>

<p>The book provides an excellent, detailed answer to these questions, and more. It’s the best kind of popular science book: you don’t need a physics degree to read it, but it doesn’t fob you off with oversimplified hand-waving either; all of the core arguments are convincingly backed up with evidence. There are some equations, but they are not necessary for understanding the book: as long as you know the difference between an electron, a proton, and a neutron, you’ll be able to follow it.</p>

<p>The book is clear about which constraints on fusion energy are fundamental limits of nature, and which constraints can be overcome with better technology. It offers optimism that fusion power is possible, highlighting the most promising paths to getting there, while remaining honest about the open problems that are yet to be solved. My take-away was that core problems, such as plasma turbulence, are very difficult, but likely solvable with more brainpower and experiments.</p>

<p>The book also provides compelling arguments in favour of fusion: not only the obvious case of providing cheap energy without carbon emissions or seasonal variation, but also that compared to fission, there is much less risk that the technology will facilitate the proliferation of nuclear weapons.</p>

<p>The need to transition away from fossil fuels is so urgent that we can’t afford to wait for fusion — renewables and fission are still crucial. However, for the medium to long term, fusion offers optimism. From about 1970 to 2000, fusion research made very impressive progress, with the <a href="https://en.wikipedia.org/wiki/Lawson_criterion">key performance metric</a> doubling every 1.8 years — faster even than Moore’s law, and getting pretty close to the point where the fusion reaction is self-sustaining without having to continually feed in external energy (the dotted line labelled “ignition” on the following diagram)!</p>

<p><img src="/2022/01/fusion-progress.jpg" alt="Figure 4.25 from the book. The x axis shows years from 1965 to 2030; the y axis shows the 'triple product' performance metric of various experimental reactors on a log scale. From about 1970 to 2000, progress follows a straight line on the log scale, i.e. exponential improvement. In the late 1990s it comes within less than an order of magnitude of 'ignition', which is where the fusion reaction becomes self-sustaining." width="100%" /></p>

<p style="text-align:center"><small><i>Figure 4.25 from the book. Note the log scale on the y axis, so the straight line from 1970 to 2000 is actually exponential growth.</i></small></p>

<p>Since 2000, progress has stalled, and the book argues that this is primarily because research in the field has been under-funded, not because of any particular fundamental limit. Of course, anybody can claim that more money will solve their problems, but in this case I’m inclined to believe it. What changed in 2000 is that the fusion research community started putting all their eggs in one basket (<a href="https://en.wikipedia.org/wiki/ITER">ITER</a>), because there wasn’t the money for multiple baskets. More money would allow more parallel experiments to explore different approaches and see which ones work better.</p>

<p>Investment in fusion research is small compared with investment in renewables and fission R&amp;D, and tiny compared to things like agricultural and fossil fuel subsidies. Even if it’s not guaranteed that fusion will work, given the potentially transformative nature of cheap, climate-friendly energy to human civilisation, it seems well worth putting some more money in it and giving it our best shot (in addition to faster ways of getting off fossil fuels, such as renewables, of course).</p>

<p>I won’t try to summarise the technical details of the book, but if you are interested in them, I can assure you that you will find this book worthwhile.</p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Several podcast interviews</title>
                <link>http://martin.kleppmann.com/2021/09/01/podcast-interviews.html</link>
                <comments>http://martin.kleppmann.com/2021/09/01/podcast-interviews.html#disqus_thread</comments>
                <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2021/09/01/podcast-interviews.html</guid>
                
                <description><![CDATA[ I regularly get asked to give interviews on the topics that I work on, especially for podcasts. To make them easier to find for anybody who’s interested, I thought I would make a list. They touch on a range of different topics, although there is also some overlap so I... ]]></description>
                <content:encoded><![CDATA[
                    <!--
Also got asked to go on https://podcast.sustainoss.org/ and https://nurkiewicz.com/
and https://www.dataengineeringpodcast.com/
-->

<p>I regularly get asked to give interviews on the topics that I work on, especially for podcasts.
To make them easier to find for anybody who’s interested, I thought I would make a list.
They touch on a range of different topics, although there is also some overlap so I wouldn’t
recommend listening to them all in a row!</p>

<p>(By the way, if you want a list of conference talks I have given, I have a
<a href="https://www.youtube.com/playlist?list=PLeKd45zvjcDHJxge6VtYUAbYnvd_VNQCx">YouTube playlist</a> for that.)</p>

<p>Here’s a list of interviews I’ve given as of September 2021:</p>

<ul>
  <li>
    <p>Interview with <a href="https://www.wix.engineering/">Wix Engineering</a>, in which we discuss my book, the
state of Automerge, the convergence of streaming systems and databases, Kafka’s move to replace
ZooKeeper with their own Raft implementation, impact of my research, and more.
Recorded 16 June 2021, published 26 August 2021.
<a href="https://www.youtube.com/watch?v=jtK7LOcP76s">Video</a>,
<a href="https://www.wix.engineering/post/wix-engineering-tech-interviews-martin-kleppmann-natan-silnitsky">transcript</a>.</p>
  </li>
  <li>
    <p>Interview with the <a href="https://museapp.com/podcast/">Metamuse podcast</a>, in which we discuss local-first
software: how the concept has evolved since we <a href="https://www.inkandswitch.com/local-first.html">first articulated it</a>,
and where it’s heading in the future.
Recorded 17 August 2021, published 14 October 2021.
<a href="https://museapp.com/podcast/41-local-first-software/">Episode link</a></p>
  </li>
  <li>
    <p>Interview with the <a href="https://www.torocloud.com/podcast">Coding over Cocktails podcast</a>, in which we
discuss making systems scalable, how data systems have evolved over the years, and local-first
software. Recorded 26 August 2021, published 30 August 2021.
<a href="https://www.torocloud.com/podcast/designing-data-intensive-applications-martin-kleppmann">Episode link and transcript</a>,
<a href="https://soundcloud.com/codingovercocktails/designing-data-intensive-applications-with-martin-kleppman">Soundcloud</a>,
<a href="https://podcasts.apple.com/ph/podcast/designing-data-intensive-applications-with-martin/id1531450276?i=1000533284011">iTunes</a>,
<a href="https://podcasts.google.com/feed/aHR0cHM6Ly9mZWVkcy5zb3VuZGNsb3VkLmNvbS91c2Vycy9zb3VuZGNsb3VkOnVzZXJzOjg3MjM0NTQxNi9zb3VuZHMucnNz/episode/dGFnOnNvdW5kY2xvdWQsMjAxMDp0cmFja3MvMTExMzg4MDIxNg?sa=X&amp;ved=0CAUQkfYCahcKEwjo-NOKhdjyAhUAAAAAHQAAAAAQAQ">Google Play</a>.</p>
  </li>
  <li>
    <p>Interview with the <a href="https://programming.love/">Programming Love</a> podcast, in which we discuss
peer-to-peer systems for collaboration, CRDTs and conflict resolution, undo and other challenges
of collaboration software, my <a href="/2018/10/17/kafka-summit.html">“Is Kafka a Database?” talk</a>, and more.
Recorded 9 July 2020, published 19 October 2020.
<a href="https://programming.love/programming-love-with-martin-kleppmann/">Episode link</a>,
<a href="https://podcasts.apple.com/us/podcast/programming-love-with-martin-kleppmann/id1518407590?i=1000495317576">Apple Podcasts</a>,
<a href="https://open.spotify.com/episode/7oc4i8h0LaFUx5l8ghJOOD">Spotify</a>,
<a href="https://www.stitcher.com/show/programming-love/episode/programming-love-with-martin-kleppmann-78699629">Stitcher</a>.</p>
  </li>
  <li>
    <p>Interview with <a href="https://medium.com/csr-tales">CSR (Computer Science Research) Tales</a>, in which we
discuss formally proving the correctness of distributed systems, and verifying CRDTs in particular.
Published 30 July 2019.
<a href="https://medium.com/csr-tales/csrtale-13-formal-verification-of-strong-eventual-consistency-1cc0af942e64">Transcript</a>.</p>
  </li>
  <li>
    <p>Interview with the <a href="https://hydraconf.com/">Hydra conference</a> on seeing through technology hype,
the CAP theorem, decentralisation, proving the correctness of CRDTs, event-based systems, and
personal growth. Recorded 3 June 2019, published 27 June 2019.
<a href="https://medium.com/@hydraconference/the-big-interview-with-martin-kleppmann-figuring-out-the-future-of-distributed-data-systems-28a680d99ae6">Transcript</a>.</p>
  </li>
  <li>
    <p>Interview with the <a href="https://codepodcast.com/">Code Podcast</a>, in which we talked in depth about
my research on CRDTs, what they can and cannot do, and how we deal with time in distributed systems.
Recorded 4 September 2018. Not sure this episode ever got published.</p>
  </li>
  <li>
    <p>Interview for an internal podcast at Booz Allen Hamilton. Recorded 9 April 2018. I don’t think it
ever got made publicly available.</p>
  </li>
  <li>
    <p>Interview with the <a href="https://www.investedinvestor.com/index">Invested Investor podcast</a>, in which
we talked about my startup career before I got into academia, selling two companies, going
through Y Combinator, moving to Silicon Valley, and all that jazz.
Recorded 20 November 2017, published 24 January 2018.
<a href="https://www.investedinvestor.com/articles/2018/1/23/martin-kleppmann">Episode link</a>,
<a href="https://audioboom.com/posts/6621031-martin-kleppmann-to-silicon-valley-and-back-again-with-two-exits-along-the-way">Audioboom</a>,
<a href="https://www.investedinvestor.com/martin-kleppmann-transcription">Transcript</a>.</p>
  </li>
  <li>
    <p>First interview with <a href="https://softwareengineeringdaily.com/">Software Engineering Daily</a>, in which
we talk about data-intensive applications, the CAP theorem, scalability, data models, data formats,
the challenges of distributed systems, and ideas for the future.
Recorded 20 April 2017, published 2 May 2017.
I am told that this was the most popular episode ever of this podcast!
<a href="https://softwareengineeringdaily.com/2017/05/02/data-intensive-applications-with-martin-kleppmann/">Episode link</a>,
<a href="http://traffic.libsyn.com/sedaily/dataintensive_edited_fixed.mp3">Download</a>,
<a href="http://softwareengineeringdaily.com/wp-content/uploads/2017/05/SEDT15-Data-Intensive-Apps.pdf">Transcript</a>.</p>
  </li>
  <li>
    <p>Second interview with <a href="https://softwareengineeringdaily.com/">Software Engineering Daily</a>, in which
we talk about decentralisation, CRDTs, blockchains, consensus, concurrency, and how to make CRDTs
work in practice. Recorded 15 November 2017, published 8 December 2017.
<a href="https://softwareengineeringdaily.com/2017/12/08/decentralized-objects-with-martin-kleppman/">Episode link</a>,
<a href="http://traffic.libsyn.com/sedaily/CRDTs_Decentralized_Files.mp3">Download</a>,
<a href="https://softwareengineeringdaily.com/wp-content/uploads/2017/12/SED477-CRDTs-Decentralized-Files.pdf">Transcript</a>.</p>
  </li>
  <li>
    <p>Interview with <a href="https://advancetechmedia.org/">Advance Tech Podcast</a>, in which we discuss a wide
range of topics: my past life in startups, security and decentralisation, event streaming systems,
data consistency, and formal verification.
Recorded and published 27 October 2017.
<a href="https://advancetechmedia.org/episode-008-martin-kleppmann/">Episode link</a>.</p>
  </li>
  <li>
    <p>Interview with <a href="https://www.infoq.com/">InfoQ</a> about log-based messaging, stream processing, and
change data capture. Recorded 24 April 2015, published 28 June 2015.
<a href="https://www.infoq.com/interviews/kleppmann-data-infrastructure-logs-crdt/">Video and transcript</a>.</p>
  </li>
</ul>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>It's time to say goodbye to the GPL</title>
                <link>http://martin.kleppmann.com/2021/04/14/goodbye-gpl.html</link>
                <comments>http://martin.kleppmann.com/2021/04/14/goodbye-gpl.html#disqus_thread</comments>
                <pubDate>Wed, 14 Apr 2021 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2021/04/14/goodbye-gpl.html</guid>
                
                <description><![CDATA[ The trigger for this post is the reinstating of Richard Stallman, a very problematic character, to the board of the Free Software Foundation (FSF). I am appalled by this move, and join others in the call for his removal. This occasion has caused me to reevaluate the position of the... ]]></description>
                <content:encoded><![CDATA[
                    <p>The trigger for this post is the
<a href="https://www.fsf.org/news/statement-of-fsf-board-on-election-of-richard-stallman">reinstating</a>
of Richard Stallman, a very <a href="https://rms-open-letter.github.io/">problematic character</a>, to the
board of the <a href="https://www.fsf.org/">Free Software Foundation</a> (FSF). I am appalled by this move, and
join others in the call for his removal.</p>

<p>This occasion has caused me to reevaluate the position of the FSF in computing. It is the steward of
the GNU project (a part of Linux distributions,
<a href="https://www.gnu.org/gnu/incorrect-quotation.en.html">loosely speaking</a>), and of a family of
software licenses centred around the
<a href="https://en.wikipedia.org/wiki/GNU_General_Public_License">GNU General Public License</a> (GPL). These
efforts are unfortunately tainted by Stallman’s behaviour. However, this is not what I actually want
to talk about today.</p>

<p>In this post I argue that we should move away from the GPL and related licenses (LGPL, AGPL), for
reasons that have nothing to do with Stallman, but simply because I think they have failed to
achieve their purpose, and they are more trouble than they are worth.</p>

<p>First, brief background: the defining feature of the GPL family of licenses is the concept of
<a href="https://en.wikipedia.org/wiki/Copyleft">copyleft</a>, which states (roughly) that if you take some
GPL-licensed code and modify it or build upon it, you must also make your modifications/extensions
(known as a “<a href="https://en.wikipedia.org/wiki/Derivative_work">derivative work</a>”) freely available
under the same license. This has the effect that the GPL’ed source code cannot be incorporated into
closed-source software. At first glance, this seems like a great idea. So what is the problem?</p>

<h2 id="the-enemy-has-changed">The enemy has changed</h2>

<p>In the 1980s and 1990s, when the GPL was written, the enemy of the free software movement was
Microsoft and other companies that sold closed-source (“proprietary”) software. The GPL intended to
disrupt this business model for two main reasons:</p>

<ol>
  <li>Closed-source software cannot easily be modified by users; you can take it or leave it, but you
cannot adapt it to your own needs. To counteract this, the GPL was designed to force companies to
release the source code of their software, so that users of the software could study it, modify
it, compile and use their modified version, and thus have the freedom to customise their
computing devices to their needs.</li>
  <li>Moreover, GPL was motivated by a desire for fairness: if you write some software in your spare
time and release it for free, it’s understandable that you don’t want others to profit from your
work without giving something back to the community. Forcing derivative works to be open source
ensures at least some baseline of “giving back”.</li>
</ol>

<p>While this made sense in 1990, I think the world has changed, and closed-source software is no
longer the main problem. <strong>In the 2020s, the enemy of freedom in computing is cloud software</strong> (aka
software as a service/SaaS, aka web apps) – i.e. software that runs primarily on the vendor’s
servers, with all your data also stored on those servers. Examples include Google Docs, Trello,
Slack, Figma, Notion, and many others.</p>

<p>This cloud software may have a client-side component (a mobile app, or the JavaScript running in
your web browser), but it only works in conjunction with the vendor’s server. And there are lots of
problems with cloud software:</p>

<ul>
  <li>If the company providing the cloud software goes out of business or decides to
<a href="https://killedbygoogle.com/">discontinue a product</a>, the software stops working, and you are
locked out of the documents and data you created with that software. This is an especially common
problem with software made by a startup, which may get
<a href="https://ourincrediblejourney.tumblr.com/">acquired by a bigger company</a> that has no interest in
continuing to maintain the startup’s product.</li>
  <li>Google and other cloud services may
<a href="https://twitter.com/Demilogic/status/1358661840402845696">suddenly suspend your account</a> with no
warning and <a href="https://www.paullimitless.com/google-account-suspended-no-reason-given/">no recourse</a>,
for example if an automated system thinks you have violated its terms of service. Even if your own
behaviour has been faultless, someone else may have hacked into your account and used it to send
malware or phishing emails without your knowledge, triggering a terms of service violation. Thus,
you could suddenly find yourself permanently locked out of every document you ever created on
Google Docs or another app.</li>
  <li>With software that runs on your own computer, even if the software vendor goes bust, you can
continue running it forever (in a VM/emulator if it’s no longer compatible with your OS, and
assuming it doesn’t need to contact a server to check for a license check). For example, the
Internet Archive has a collection of
<a href="https://archive.org/details/softwarelibrary">over 100,000 historical software titles</a> that you
can run in an emulator inside your web browser! In contrast, if cloud software gets shut down,
there is no way for you to preserve it, because you never had a copy of the server-side software,
neither as source code nor in compiled form.</li>
  <li>The 1990s problem of not being able to customise or extend software you use is aggravated further
in cloud software. With closed-source software that runs on your own computer, at least someone
could reverse-engineer the file format it uses to store its data, so that you could load it into
alternative software (think pre-<a href="https://en.wikipedia.org/wiki/Office_Open_XML">OOXML</a> Microsoft
Office file formats, or Photoshop files before the
<a href="https://www.adobe.com/devnet-apps/photoshop/fileformatashtml/">spec</a> was published). With cloud
software, not even that is possible, since the data is only stored in the cloud, not in files on
your own computer.</li>
</ul>

<p>If all software was free and open source, these problems would all be solved. However, making the
source code available is not actually necessary to solve the problems with cloud software; even
closed-source software avoids the aforementioned problems, as long as it is running on your own
computer rather than the vendor’s cloud server. Note that the Internet Archive is able to keep
historical software working without ever having its source code: for purposes of preservation,
running the compiled machine code in an emulator is just fine. Maybe having the source code would
make it a little easier, but it’s not crucial. The important thing is having a copy of the software
<strong>at all</strong>.</p>

<h2 id="local-first-software">Local-first software</h2>

<p>My collaborators and I have previously argued for
<a href="https://www.inkandswitch.com/local-first.html">local-first software</a>, which is a response to these
problems with cloud software. Local-first software runs on your own computer, and stores its data on
your local hard drive, while also retaining the convenience of cloud software, such as real-time
collaboration and syncing your data across all of your devices. It is nice for local-first software
to also be open source, but this is not necessary: 90% of its benefits apply equally to
closed-source local-first software.</p>

<p>Cloud software, not closed-source software, is the real threat to software freedom, because the harm
from being suddenly locked out of all of your data at the whim of a cloud provider is much greater
than the harm from not being able to view and modify the source code of your software. For that
reason, it is much more important and pressing that we make local-first software ubiquitous. If, in
that process, we can also make more software open-source, then that would be nice, but that is less
critical. Focus on the biggest and most urgent challenges first.</p>

<h2 id="legal-tools-to-promote-software-freedom">Legal tools to promote software freedom</h2>

<p>Copyleft software licenses are a legal tool that attempts to force more software vendors to release
their source code. In particular, the
<a href="https://en.wikipedia.org/wiki/Affero_General_Public_License">AGPL</a> is an attempt to force providers
of cloud services to release the source of their server-side software. However, this hasn’t really
worked: most vendors of cloud software simply refuse to use AGPL-licensed software, and either use
a different implementation with a more permissive license, or re-implement the necessary
functionality themselves, or
<a href="https://www.elastic.co/pricing/faq/licensing">buy a commercial license</a> that comes without the
copyleft clauses. I don’t think the license has caused any source code to become available that
wouldn’t have been open source anyway.</p>

<p>As a legal tool to promote greater software freedom, I believe copyleft software licenses have
largely failed, since they have done nothing to stop the rise of cloud software, and probably not
done much to increase the share of software whose source is available. Open source software has
become very successful, but much of this success is in projects with non-copyleft licenses (e.g.
Apache, MIT, or BSD licenses), and even in the GPL-licensed projects (e.g. Linux) I am skeptical
that the copyleft aspect was really an important factor in the project’s success.</p>

<p>I believe a much more promising legal tool to promote software freedom is in government regulation.
For example, the GDPR includes a
<a href="https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/individual-rights/right-to-data-portability/">right to data portability</a>,
which means that users must be able to move their data from one service to another. Existing
implementations of portability, such as
<a href="https://en.wikipedia.org/wiki/Google_Takeout">Google Takeout</a>, are quite rudimentary (what can you
really do with a big zip archive of JSON files?), but we can lobby regulators to
<a href="https://interoperability.news/">push for better portability/interoperability</a>, e.g. requiring
real-time bidirectional sync of your data between two apps by competing providers.</p>

<p>Another promising route I see is pushing
<a href="https://joinup.ec.europa.eu/sites/default/files/document/2011-12/OSS-procurement-guideline%20-final.pdf">public-sector procurement to prefer open source, local-first software</a>
over closed-source cloud software. This creates a positive incentive for businesses to develop and
maintain high-quality open source software, in a way that copyleft clauses do not.</p>

<p>You might argue that a software license is something that an individual developer can control,
whereas governmental regulation and public policy is a much bigger issue outside of any one
individual’s power. Yes, but how much impact can you really have by choosing a software license?
Anyone who doesn’t like your license can simply choose not to use your software, in which case your
power is zero. Effective change comes from collective action on big issues, not from one person’s
little open source side project choosing one license over another.</p>

<h2 id="other-problems-with-gpl-family-licenses">Other problems with GPL-family licenses</h2>

<p>You can force a company to make their source code of a GPL-derived software project available, but
you cannot force them to be good citizens of the open source community (e.g. continuing to maintain
the features they have added, fixing bugs, helping other contributors, providing good documentation,
participating in project governance). What worth is source code that is just “thrown over the wall”
without genuine engagement in the open source project? At best it’s worthless, and at worst it’s
harmful because it shifts the burden of maintenance to other contributors of the project.</p>

<p>We need people to be good contributors to the open source community, and this is achieved by setting
up the right incentives and by being welcoming, not by software licenses.</p>

<p>Finally, a practical problem of GPL-family licenses is their
<a href="http://gplv3.fsf.org/wiki/index.php/Compatible_licenses">incompatibility with other widely-used licenses</a>,
making it difficult to use certain combinations of libraries in the same project and unnecessarily
fragmenting the open source ecosystem. Maybe it would be worth putting up with this problem if the
GPL had other strong advantages, but as I have explained, I don’t think those advantages exist.</p>

<h2 id="conclusion">Conclusion</h2>

<p>The GPL and other copyleft licenses are not bad; I just think they’re pointless. They have practical
problems, and they are tainted by the behaviour of the FSF, but most importantly, I do not believe
they have been an effective contributor to software freedom. The only real use for copyleft nowadays
is by commercial software vendors
(<a href="https://www.mongodb.com/licensing/server-side-public-license/faq">MongoDB</a>,
<a href="https://www.elastic.co/pricing/faq/licensing">Elastic</a>) who want to stop Amazon from providing
their software as a service – which is fine, but it’s motivated purely by business concerns, not by
software freedom.</p>

<p>Open source software has been tremendously successful, and it has come a long way since the origins
of the free software movement born from 1990s anti-Microsoft sentiment. I will acknowledge that the
FSF was instrumental in getting this all started. However, 30 years on, the ecosystem has changed,
but the FSF has failed to keep up, and has
<a href="https://r0ml.medium.com/free-software-an-idea-whose-time-has-passed-6570c1d8218a">become more and more out of touch</a>.
It has failed to establish a coherent response to cloud software and other recent threats to
software freedom, and it just continues to rehash tired old arguments from decades ago. Now, by
reinstating Stallman and dismissing the concerns about him, the FSF is
<a href="https://lu.is/blog/2021/04/07/values-centered-npos-with-kmaher/">actively harming</a> the cause of
free software. We must distance ourselves from the FSF and their worldview.</p>

<p>For all these reasons, I think it no longer makes sense to cling on to the GPL and copyleft. Let
them go. Instead, I would encourage you to adopt a permissive license for your projects (e.g.
<a href="https://opensource.org/licenses/MIT">MIT</a>, <a href="https://opensource.org/licenses/BSD-2-Clause">BSD</a>,
<a href="https://opensource.org/licenses/Apache-2.0">Apache 2.0</a>), and then focus your energies on the
things that will really make a difference to software freedom:
<a href="https://www.inkandswitch.com/local-first.html">counteracting</a> the monopolising effects of cloud
software, developing sustainable business models that allow open source software to thrive, and
pushing for regulation that prioritises the interests of software users over the interests of
vendors.</p>

<p><em>Thank you to <a href="https://ramcq.net/">Rob McQueen</a> for feedback on a draft of this post.</em></p>

<p><em>Update: <a href="https://twitter.com/lexi_lambda/status/1295426437583982592">related Twitter thread by Alexis King</a></em></p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Building the future of computing, with your help</title>
                <link>http://martin.kleppmann.com/2021/02/23/patreon.html</link>
                <comments>http://martin.kleppmann.com/2021/02/23/patreon.html#disqus_thread</comments>
                <pubDate>Tue, 23 Feb 2021 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2021/02/23/patreon.html</guid>
                
                <description><![CDATA[ For the last five or six years, since I bid goodbye to the startup scene and Silicon Valley, I have been increasingly working in public. I have written a book, given around 100 talks (many of which are available on YouTube), published over 20 research papers (all freely available from... ]]></description>
                <content:encoded><![CDATA[
                    <p>For the last five or six years, since I bid goodbye to the startup scene and Silicon Valley, I have
been increasingly working in public. I have <a href="https://dataintensive.net/">written a book</a>,
given <a href="https://martin.kleppmann.com/talks.html">around 100 talks</a> (many of which are
<a href="https://www.youtube.com/playlist?list=PLeKd45zvjcDHJxge6VtYUAbYnvd_VNQCx">available on YouTube</a>),
published <a href="https://martin.kleppmann.com/#publications">over 20 research papers</a>
(all freely available from my website), and released and maintained
<a href="https://github.com/ept">some open source projects</a>.
Just a few months ago I released a new undergraduate-level course on distributed systems, consisting of
<a href="https://www.youtube.com/playlist?list=PLeKd45zvjcDFUEv_ohr_HdUFe97RItdiB">7 hours of video lectures</a> and
<a href="https://www.cl.cam.ac.uk/teaching/2021/ConcDisSys/dist-sys-notes.pdf">87 pages of notes</a> and
exercises, all free; in student evaluation at the <a href="https://www.cst.cam.ac.uk/">University of Cambridge</a>,
over 80% rated my lectures and notes as “excellent”.</p>

<p>I love doing first-rate work and making it broadly available. In fact, apart from my book, I give
everything away for free, because I want to be able to reach and help the broadest possible set of
people. And even my book is very cheap compared to the value that many people get out of it (just
<a href="https://dataintensive.net/buy.html">read the reviews</a>).</p>

<p>Of course, nobody goes into academia because of the money (or the job security of untentured posts,
for that matter). I would probably be earning five times my current salary if I had stayed in
industry. But I have absolutely no regrets about taking that pay cut: I love the freedom to work on
whatever I find interesting, and the freedom to publish everything so that others can use it. If you
have found any of my talks, writing, or code useful, then you have also benefitted from the freedom
that I enjoy.</p>

<p>Of course, like everybody else, I have bills to pay. At the moment I’m employed at the
<a href="https://www.cst.cam.ac.uk/">University of Cambridge</a> on a fixed-term contract, funded by
a charitable research grant. This grant gives me wonderful freedom to pursue my research and make it
publicly available, but it’s a fixed amount of money, and once it runs out, my job disappears in
a puff of smoke. This sort of grant is not renewable, regardless how amazing the work it has
enabled. I can try applying for follow-on grants from other funders, but this takes a lot of time
and has a low chance of success.</p>

<p>Therefore I am setting up crowdfunding through <a href="https://www.patreon.com/martinkl">Patreon</a>, in the
hope of establishing a sustainable basic income that will allow me to continue my work of research
and teaching long-term. I want to continue making most of my work freely available, so that the
maximum number of people can benefit from it.</p>

<h2 id="why-support-me">Why support me?</h2>

<p>I am offering <a href="https://www.patreon.com/martinkl">three membership tiers</a> for anyone who wants to support my work:</p>

<ol>
  <li>At the lowest tier, you will get regular news about new things I am working on, and exclusive
early access to drafts and work-in-progress. Keep your finger on the pulse of new research as it
is happening. I will also send you some nice stickers (once I’ve got them printed).</li>
  <li>At the middle tier, you will additionally be invited to participate in an exclusive community
with other supporters and myself, with both live and asynchronous discussions. I hope to
cultivate thoughtful, high-quality exchange of ideas with likeminded people in this community.</li>
  <li>At the highest tier, you get all the aforementioned benefits, plus the ability to influence my
direction when I’m choosing what to work on next. Not saying I will definitely do what you want;
also not saying that I will only take input from paying supporters (I still welcome ideas from
everyone). However, I will consult and engage with supporters at this tier to get your opinions.
I will also acknowledge you in any papers and books I write, making your name permanently etched
into the scientific literature.</li>
</ol>

<p>However, the biggest benefit is that by supporting me on Patreon you are enabling the creation of
future work: that is, new thinking, writing, talks, and code that would not be created if I had to
spend my time writing grant proposals or working for some company instead. If I have to go and get
a job somewhere, you will mostly hear me giving bland talks promoting the technology of whatever
company I happen to work for. Being independent allows me to pick topics that I find interesting and
important (such as <a href="https://www.youtube.com/watch?v=5ZjhNTM8XU8">database transactions</a>,
<a href="https://www.youtube.com/watch?v=Uav5jWHNghY">formal verification</a>,
<a href="https://www.youtube.com/watch?v=B5NULPSiOGw">CRDTs</a>, or
<a href="https://martin.kleppmann.com/papers/curve25519.pdf">elliptic curve cryptography</a>),
and present them in an accessible and neutral way.</p>

<p>I will continue making most of my work publicly available for free (except for books): even if you
cannot afford to be a Patreon supporter, it will still be available to you. Patreon supporters
simply get earlier access, plus the warm fuzzy feeling of knowing that you enabled the creation of
new work that, without your support, may never have existed. Supporting me on Patreon is <em>not
a donation</em>: it is an investment in future work that will hopefully be valuable to you.</p>

<p>If you have found my work useful – for example, if you have applied ideas from my talks in your
work, or if my book helped you get a job – then I would be delighted to welcome you as a
<a href="https://www.patreon.com/martinkl">supporter</a>! And if your company uses my book for training
engineers, please find out how your company can support me: even my highest supporter tier is a tiny
amount of money for a company that uses my work to improve the skills of their staff.  I only get
around $2 to $5 for every copy of my book that is sold; if you’re getting a lot more value than this
out of it, it would only be fair of you to <a href="https://www.patreon.com/martinkl">support me more substantially</a>.</p>

<p>If you cannot contribute financially, worry not. I equally appreciate your support in the form of
contributions to the open source community, discussing interesting ideas with me, and sharing useful
material with others. I will continue to engage with you and answer your questions, regardless of
whether you are a paying supporter. And most things I produce will continue to be free, so that
everyone can benefit from them.</p>

<h2 id="planned-work">Planned work</h2>

<p>Keep in mind that when you support me, you are not buying a product. You don’t know exactly what
you’re going to get, because I don’t know exactly what I am going to do in advance either. That’s
why it’s called research – it’s open-ended, and part of its purpose is to go down unexpected
rabbit-holes if they seem important! You are funding a person because this person has done good work
in the past, and is likely to continue doing good work in the future.</p>

<p>I do have a lot of plans, though. At a high level, I am hoping to do these things over the next few years:</p>

<ul>
  <li>Write another book to complement <a href="https://dataintensive.net/">Designing Data-Intensive Applications</a>;</li>
  <li>Develop the foundational technologies to enable the
<a href="https://www.cl.cam.ac.uk/research/dtg/trve/">next generation of collaboration software</a> (such as
Google Docs), in a way that does not require
<a href="https://www.inkandswitch.com/local-first.html">giving Google all of our data</a>;</li>
  <li>Continue writing research papers, blog posts, and giving talks/making videos on distributed
systems and related topics.</li>
</ul>

<p>There is no concrete timescale for these things; most likely I will work on several of them in
tandem, as I have been doing over the last several years.</p>

<p>Part of this story is creating educational content on topics that I find important, and part is
a vision for the future of collaborative computing, which my collaborators and I are realising in
the form of <a href="https://github.com/automerge/automerge">Automerge</a>, an open source project. Our vision
is articulated in the essay-cum-manifesto on
<a href="https://www.inkandswitch.com/local-first.html">local-first software</a>, which I suggest you read if
you haven’t already.</p>

<h2 id="research-philosophy">Research philosophy</h2>

<p>For me it is important to have this mixture of research, open source software development, and
teaching (through speaking and writing), because all of these activities feed off each other.
I don’t want to just work on open source without doing research, because that only leads to
incremental improvements, no fundamental breakthroughs. I don’t want to just do research without
applying it, because that would mean losing touch with reality. And I don’t want to just be
a YouTuber or writer without doing original research, because I would run out of ideas and my
content would get stale and boring; good teaching requires actively working in the area.</p>

<p>This interaction was articulated wonderfully by
<a href="https://amturing.acm.org/award_winners/gray_3649936.cfm">Turing award winner Jim Gray</a>:</p>

<blockquote>
  <p>I aspire to be a scholar of computer science. All fields of scholarship, from religion to
medicine, emphasize three aspects: meditation, teaching and service. Meditation (called research
by scientists) is the official part of research. But, teaching (writing papers, explaining your
ideas, and transferring technology) and service (making computer systems and helping people use
them) are also major aspects of the scholarly process. They keep the scholar in touch with
reality.</p>

  <p>— <a href="http://jimgray.azurewebsites.net/papers/critiqueofibm%27scsresearch.pdf">Jim Gray, 1980</a></p>
</blockquote>

<p>(That’s from Gray’s letter of resignation from IBM. The whole letter is a fascinating read if you’re
into computing history. At the time Gray was working on
<a href="https://people.eecs.berkeley.edu/~brewer/cs262/SystemR.pdf">System R</a>, the precursor of all
relational databases we use today. It’s fair to say that his work has had a huge impact.)</p>

<p>Another aspect of my research philosophy is that good work rarely happens with one person alone, but
through collaboration with other good people. Quoting Jim Gray again:</p>

<blockquote>
  <p>Computer science is an empirical and multi-disciplinary field. The aspect of it that I work on,
computer systems, requires lots of good people, time and equipment to produce anything of
interest. Projects of five or ten people working for five or ten years seem to be about the right
scale. More modest projects are unable to attack significant problems. More ambitious projects
have unclear goals and have management problems.</p>
</blockquote>

<p>You might be wondering: even if I get enough Patreon funding to cover my own living expenses, it
seems unlikely that I will be able to crowdfund a team of five to ten people. Fortunately, I have
found over the last years that collaboration does not require all team members to be funded out of
the same purse. I constantly collaborate with people without being responsible for their payroll.
In open source, it is common for contributors to a project to be employed by several different
organisations, and indeed such diversity makes projects better and more resilient.</p>

<p>I work closely with the <a href="https://www.inkandswitch.com/">Ink &amp; Switch lab</a>, who have their own
funding. Some of my collaborators are PhD students who have their own stipends, or research fellows
who have their own grants. We come together because of our common interests, and because nobody is
trying to profit from the others. We have a vision of the future that we want to realise, and the
funding just lets us pay the bills as we work towards the greater goal.</p>

<p>Of course, if my Patreon ends up being successful and generates more money than I need for my own
living expenses, I will use it to help fund collaborators. I am not aiming to recreate the lavish
Silicon Valley engineering salary that I left behind; I just want to do good work without having to
spend a lot of time chasing grants.</p>

<h2 id="alternatives-to-crowdfunding">Alternatives to crowdfunding</h2>

<p>Before moving to <a href="https://www.patreon.com/martinkl">Patreon</a> I considered several alternatives:</p>

<ul>
  <li>Academic jobs and fellowships? It’s a difficult to get a stable position at a research-focussed
university. Both jobs and funding are fiercely competitive (hundreds of applicants for one place),
and they require a strong track record of publications. Unfortunately, there is a
<a href="https://cacm.acm.org/blogs/blog-cacm/248824-how-objective-is-peer-review/fulltext">large degree of randomness</a>
in the choice of papers that get accepted to top-tier publication venues. I am still interested in
an academic career, but it seems unwise to put all eggs in this uncertain basket. Oh, and due to
the pandemic my current university has a hiring freeze anyway, so no jobs anytime soon.</li>
  <li>Founding a startup? Been there, <a href="https://www.crunchbase.com/person/martin-kleppmann">done that</a>
(twice). A startup is a great way of productising technology on a 1–2 year time scale; it also
needs fast growth and/or a strong revenue model. My current work does not fit that model since it
focusses on foundational technolgies with a longer time-scale (the 5–10 years mentioned by Jim
Gray), and it aims for public benefit rather than private profit.</li>
  <li>Getting a job at someone else’s company? I want to be free to choose what to work on based on what
I believe is important, not whatever happens to suit a company’s agenda. I also want to be free to
publish that work openly. Not many companies are willing to support such positions long-term.</li>
  <li>Consulting work and training? I could spend a fraction of my time helping companies solve problems
within my area of expertise, or running training workshops. However, this type of income can
fluctuate wildly, and generating a steady stream of clients is a lot of work and very distracting.
It’s difficult to make consulting compatible with the deep thinking and long-term view required
for research.</li>
  <li>Becoming a professional author? I have been able to draw a reasonable income from
<a href="https://martin.kleppmann.com/2020/09/29/is-book-writing-worth-it.html">royalties for sales of my book</a>.
However, I have no idea how long those sales will last, and I have no idea whether any future book
I write will sell similarly well. Given this unpredictability, it seems unwise to bet on royalties
as only income. Moreover, book-writing is only one of several things I do, and I believe the
other things generate value too. I believe my funding situation should reflect that.</li>
</ul>

<p>With crowdfunding, I hope to not only generate a steady income stream, but also build a community of
people who are excited about the same topics as me, and who are invested in making these ideas
a reality. It is an opportunity for me to share early-stage work with enthusiasts, and to improve
that work through feedback from the community. And it is an opportunity for you to get an insider
view of the research process as we build the future of computing.</p>

<p>If you believe in our vision for
<a href="https://www.inkandswitch.com/local-first.html">a better future of collaborative computing</a>, or if
you want to see more high-quality educational materials for computer science, then why not head over
to Patreon and <a href="https://www.patreon.com/martinkl">pledge your support</a>? It will make a huge
difference. Thank you!</p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Decentralised content moderation</title>
                <link>http://martin.kleppmann.com/2021/01/13/decentralised-content-moderation.html</link>
                <comments>http://martin.kleppmann.com/2021/01/13/decentralised-content-moderation.html#disqus_thread</comments>
                <pubDate>Wed, 13 Jan 2021 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2021/01/13/decentralised-content-moderation.html</guid>
                
                <description><![CDATA[ Who is doing interesting work on decentralised content moderation? With Donald Trump suspended from Twitter and Facebook, and Parler kicked off AWS, there is renewed discussion about what sort of speech is acceptable online, and how it should be enforced. Let me say up front that I believe that these... ]]></description>
                <content:encoded><![CDATA[
                    <p><strong>Who is doing interesting work on decentralised content moderation?</strong></p>

<p>With Donald Trump suspended from Twitter and Facebook, and
<a href="https://en.wikipedia.org/wiki/Parler">Parler</a> kicked off AWS, there is renewed discussion about
what sort of speech is acceptable online, and how it should be enforced. Let me say up front that
I believe that these bans were justified. However, they do raise questions that need to be
discussed, especially within the technology community.</p>

<p>As many have already pointed out, Twitter, Facebook and Amazon are corporations that are free to
enforce their terms of service in whatever way they see fit, within the bounds of applicable law
(e.g. anti-discrimination legislation). However, we should also realise that <em>almost all</em> social
media, the public spaces of the digital realm, are in fact privately owned spaces subject to
a corporation’s terms of service. There is currently no viable, non-corporate alternative space that
we could all move to. For better or for worse, Mark Zuckerberg, Jack Dorsey, and Jeff Bezos (and
their underlings) are, for now, the arbiters of what can and cannot be said online.</p>

<p>This situation draws attention to the <a href="https://redecentralize.org/">decentralised web community</a>,
a catch-all for a broad set of projects that are aiming to reduce the degree of centralised
corporate control in the digital sphere. This includes self-hosted/federated social networks such as
<a href="https://joinmastodon.org/">Mastodon</a> and <a href="https://diasporafoundation.org/">Diaspora</a>, peer-to-peer
social networks such as <a href="https://scuttlebutt.nz/">Scuttlebutt</a>, and miscellaneous blockchain
projects. The exact aims and technicalities of those projects are not important for this post.
I will start by focussing on one particular design goal that is mentioned by many decentralised web
projects, and that is <em>censorship resistance</em>.</p>

<h2 id="censorship-resistance">Censorship resistance</h2>

<p>When we think of censorship, we think of totalitarian states exercising violent control over their
population, crushing dissent and stifling the press. Against such an adversary, technologies that
provide censorship resistance seem like a positive step forward, since they promote individual
liberty and human rights.</p>

<p>However, often the adversary is not a totalitarian state, but other users. Censorship resistance
means that anybody can say anything, without suffering consequences. And unfortunately there are
a lot of people out there who say and do rather horrible things. Thus, as soon as
a censorship-resistant social network becomes sufficiently popular, I expect that it will be filled
with messages from spammers, neo-nazis, and child pornographers (or any other type of content that
you consider despicable). One person’s freedom from violence is another person’s censorship, and
thus, a system that emphasises censorship resistance will inevitably invite violence against some
people.</p>

<p>I fear that many decentralised web projects are designed for censorship resistance not so much
because they deliberately want to become hubs for neo-nazis, but rather out of a kind of naive
utopian belief that more speech is always better. But I think we have learnt in the last decade that
this is not the case. If we want technologies to help build the type of society that we want to live
in, then certain abusive types of behaviour must be restricted. Thus, content moderation is needed.</p>

<h2 id="the-difficulty-of-content-moderation">The difficulty of content moderation</h2>

<p>If we want to declare some types of content as unacceptable, we need a process for distinguishing
between acceptable and unacceptable material. But this is difficult. Where do you draw the line
between healthy scepticism and harmful conspiracy theory? Where do you draw the line between healthy
satire, using exaggeration for comic effect, and harmful misinformation? Between legitimate
disagreement and harassment? Between honest misunderstanding and malicious misrepresentation?</p>

<p>With all of these, some cases will be very clearly on one side or the other of the dividing line,
but there will always be a large grey area of cases that are unclear and a matter of subjective
interpretation. “<a href="https://en.wikipedia.org/wiki/I_know_it_when_I_see_it">I know it when I see it</a>”
is difficult to generalise into a rule that can be applied objectively and consistently; and without
objectivity and consistency, moderation can easily degenerate into a situation where one group of
people forces their opinions on everyone else, like them or not.</p>

<p>In a service that is used around the world, there will be cultural differences on what is considered
acceptable or not. Maybe one culture is sensitive about nudity and tolerant of depictions of
violence, while another culture is liberal about nudity and sensitive about violence. One person’s
terrorist is another person’s freedom fighter. There is no single, globally agreed standard of what
is or is not considered acceptable.</p>

<p>Nevertheless, it is possible to come to agreement. For example, Wikipedia editors successfully
manage to agree on what should and should not be included in Wikipedia articles, even those on
contentious subjects. I won’t say that this process is perfect: Wikipedia editors are predominantly
white, male, and from the Anglo-American cultural sphere, so there is bound to be bias in their
editorial decisions. I haven’t participated in this community, but I assume the process of coming to
agreement is sometimes messy and will not make everybody happy.</p>

<p>Moreover, being an encyclopaedia, Wikipedia is focussed on widely accepted facts backed by evidence.
Attempting to moderate social media in the same way as Wikipedia would make it joyless, with no room
for satire, comedy, experimental art, or many of the other things that make it interesting and
humane. Nevertheless, Wikipedia is an interesting example of decentralised content moderation that
is not controlled by a private entity.</p>

<p>Another example is federated social networks such as Mastodon or Diaspora. Here, each individual
server administrator has the authority to
<a href="https://docs.joinmastodon.org/admin/moderation/">set the rules for the users of their server</a>, but
they have no control over activity on other servers (other than to block another server entirely).
Despite the decentralised architecture, there is a
<a href="https://arxiv.org/pdf/1909.05801.pdf">trend towards centralisation</a> (10% of Mastodon instances
account for almost half the users), leaving a lot of power in the hand of a small number of server
administrators. If these social networks are to go more mainstream, I expect these effects to be
amplified.</p>

<h2 id="filter-bubbles">Filter bubbles</h2>

<p>One form of social media is private chat for small groups, as provided e.g. by WhatsApp, Signal, or
even email. Here, when you post a message to a group, the only people who can see it are members of
that group. In this setting, not much content moderation is needed: group members can kick out other
members if they say things considered unacceptable. If one group says things that another group
considers objectionable, that’s no problem, because the two groups can’t see each other’s
conversations anyway. If one user is harassing another, the victim can block the harasser. Thus,
private groups are comparatively easy to deal with.</p>

<p>The situation is harder with social media that is public (anyone can read) and open (anyone can join
a conversation), or when the groups are very large. Twitter is an example of this model (and
Facebook to some degree, depending on your privacy settings). When anybody can write a message that
you will see (e.g. a reply to something you posted publicly), the door is opened to harassment and
abuse.</p>

<p>One response might be to retreat into our filter bubbles. For example, we could say that you see
only messages posted by your immediate friends and friends-of-friends. I am pretty sure that there
are no neo-nazis among my direct friends, and probably also among my second-degree network, so such
a rule would shield me from extremist content of one sort, at least.</p>

<p>It is also possible for users to collaborate on creating filters. For example,
<a href="https://github.com/freebsdgirl/ggautoblocker">ggautoblocker</a> was a tool to block abusive Twitter
accounts during <a href="https://en.wikipedia.org/wiki/Gamergate_controversy">GamerGate</a>, a 2014
misogynistic harassment campaign that
<a href="https://www.theguardian.com/technology/2016/dec/01/gamergate-alt-right-hate-trump">foreshadowed</a>
the rise of the alt-right and Trumpism. In the absence of central moderation by Twitter, victims of
this harassment could use this tool to automatically block a large number of harmful users so that
they wouldn’t have to see the abusive messages.</p>

<p>Of course, even though such filtering saves you from having to see things you don’t like, it doesn’t
stop the objectionable content from existing. Moreover, other people may have the opposite sort of
filter bubble in which they see <em>lots</em> of extremist content, causing them to become radicalised.
Personalised filters also stop us from seeing alternative (valid) opinions that would help broaden
our worldview and enable better mutual understanding of different groups in society.</p>

<p>Thus, subjective filtering of who sees what, such as blocking users, is an important part of
reducing harm on social media, but by itself it is not sufficient. It is also necessary to uphold
minimum standards on what can be posted at all, for example by requiring a baseline of civility and
truthfulness.</p>

<h2 id="democratic-content-moderation">Democratic content moderation</h2>

<p>I previously argued that there is no universally agreed standard of acceptability of content; and
yet, we must somehow keep the standard of discourse high enough that it does not become intolerable
for those involved, and to minimise the harms e.g. from harassment, radicalisation, and incitement
of violence. How do we solve this contradiction? Leaving the power in the hands of a small number of
tech company CEOs, or any other small and unelected group of people, does not seem like a good
long-term solution.</p>

<p>A purely technical solution does not exist either, since code cannot make value judgements about
what sort of behaviour is acceptable. It seems like some kind of democratic process is the only
viable long-term solution here, perhaps supported by some technological mechanisms, such as
AI/machine learning to flag potentially abusive material. But what might this democratic process
look like?</p>

<p>Moderation should not be so heavy-handed that it drowns out legitimate disagreement. Disagreement
need not always be polite; indeed,
<a href="https://everydayfeminism.com/2015/12/tone-policing-and-privilege/">tone policing</a> should not be
a means of silencing legitimate complaints. On the other hand, aggressive criticism may quickly flip
into the realm of harassment, and it may be unclear when exactly this line has been crossed.
Sometimes it may be appropriate to take into account the power relationships between the people
involved, and hold the privileged and powerful to a higher standard than the oppressed and
disadvantaged, since otherwise the system may end up reinforcing existing imbalances. But there are
no hard and fast rules here, and much depends on the context and background of the people involved.</p>

<p>This example indicates that the moderation process needs to embed ethical principles and values. One
way of doing this would be to have a board of moderation overseers that is elected by the user base.
In their manifesto, candidates for this board can articulate the principles and values that they
will bring to the job. Different candidates may choose to represent people with different world
views, such as conservatives and liberals. Having a diverse set of opinions and cultures represented
on such a board would both legitimise its authority and improve the quality of its decision-making.
In time, maybe even parties and factions may emerge, which I would regard as a democratic success.</p>

<p>Facebook employs
<a href="https://bhr.stern.nyu.edu/tech-content-moderation-june-2020">around 15,000 content moderators</a>, and
on all accounts it’s
<a href="https://www.theverge.com/2019/2/25/18229714/cognizant-facebook-content-moderator-interviews-trauma-working-conditions-arizona">a horrible job.</a>
Who would want to do it? On the other hand, 15,000 is a tiny number compared to Facebook’s user
count. Rather than concentrating all the content moderation work on a comparatively small number of
moderators, maybe every user should have to do a stint at moderation from time to time as part of
their conditions for using a service? Precedents for this sort of thing exist: in a number of
countries, individuals may be called to jury duty to help decide criminal cases; and researchers are
regularly asked to review articles written by their peers. These things are not great fun either,
but we do them for the sake of the civic system that we all benefit from.</p>

<p>Moderators with differing political views may disagree on whether a certain piece of content is
acceptable or not. In cases of such disagreement, additional people can be brought in, hopefully
allowing the question to be settled through debate. If no agreement can be found, the matter can be
escalated to the elected board, which has the final say and which uses the experience to set
guidelines for future moderation.</p>

<h2 id="implications-for-decentralised-technologies">Implications for decentralised technologies</h2>

<p>In decentralised social media, I believe that ultimately it should be the users themselves who
decide what is acceptable or not. This governance will have to take place through some human process
of debate and deliberation, although technical tools and some degree of automation may be able to
support the process and make it more efficient. Rather than simplistic censorship resistance, or
giving administrators dictatorial powers, we should work towards ethical principles, democratic
control, and accountability.</p>

<p>I realise that my proposals are probably naive and smack of “computer scientist finally discovers
why the humanities are important”. Therefore, if you know of any work that is relevant to this topic
and can help technological systems learn from centuries of experience in democracy in the civil
society, please send it to me — I am keen to learn more. Moreover, if there is existing work in the
decentralised web community on enabling this kind of grassroots democracy, I would love to hear
about it too.</p>

<p>You can find me on Twitter <a href="https://twitter.com/martinkl">@martinkl</a>, or contact me by email
(firstname at lastname dot com). I will update this post with interesting things that are sent to
me.</p>

<h2 id="updates-related-work">Updates: related work</h2>

<p>Here are some related projects that have been pointed out to me since this post was published. I
have not vetted them, so don’t take this as an endorsement.</p>

<ul>
  <li>The <a href="https://oversightboard.com/">Facebook/Instagram Oversight Board</a> is quite close to what
I have in mind, and it has <a href="https://oversightboard.com/news/226612455899839-oversight-board-upholds-former-president-trump-s-suspension-finds-facebook-failed-to-impose-proper-penalty/">upheld</a>
the suspension of Trump’s account.</li>
  <li>The recently launched
<a href="https://news.mit.edu/2021/center-constructive-communication-0113">MIT Center for Constructive Communication</a>
is an ambitious effort in this area.</li>
  <li>“<a href="https://foundation.mozilla.org/en/blog/fellow-research-decentralized-web-hate/">The Decentralized Web of Hate</a>”
is a detailed report by <a href="http://emmibevensee.com/">Emmi Bevensee</a> on use of decentralised
technologies by extremists.</li>
  <li><a href="https://homes.cs.washington.edu/~axz/publications.html">Amy X. Zhang</a> and her collaborators have
done a lot of research on moderation.</li>
  <li><a href="https://twitter.com/arcalinea">Jay Graber</a> recently published a comprehensive
<a href="https://twitter.com/arcalinea/status/1352316972654944257">report comparing decentralised social protocols</a>, and a
<a href="https://jaygraber.medium.com/designing-decentralized-moderation-a76430a8eab">blog post</a>
on decentralised content moderation.</li>
  <li><a href="https://twitter.com/weschow">Wes Chow</a> has written a
<a href="https://medium.com/@wesc/opportunities-in-the-design-of-decentralized-social-networks-d66cce42d74b">thoughtful and nunanced article</a>
on decentralised content moderation, with lots of references to further reading at the end.</li>
  <li>A few <a href="https://twitter.com/xmal/status/1349413781953273857">people</a>
<a href="https://twitter.com/weschow/status/1349417270179737604">mentioned</a> Slashdot, Reddit, and Stack Overflow
as successful examples of community-run moderation.</li>
  <li><a href="https://cblgh.org/articles/trustnet.html">Trustnet</a> is a way of computing numerical scores for
the degree of trust in indvidual users, based on the social graph.</li>
  <li><a href="https://matrix.org/">Matrix</a>, a federated messaging system, is
<a href="https://matrix.org/blog/2020/10/19/combating-abuse-in-matrix-without-backdoors">working on</a> a
decentralised, subjective reputation system.</li>
  <li><a href="https://freenetproject.org/">Freenet</a> has a web-of-trust-based, decentralised
<a href="https://www.draketo.de/english/freenet/friendly-communication-with-anonymity">user reputation system</a>
(see also this <a href="https://github.com/xor-freenet/plugin-WebOfTrust/blob/master/developer-documentation/core-developers-manual/OadSFfF-version1.2-non-print-edition.pdf">Bachelor’s thesis</a>).</li>
  <li><a href="https://github.com/Freechains/README">Freechains</a> is a peer-to-peer content distribution
protocol with an embedded user reputation system.</li>
  <li><a href="https://github.com/Murmuration-Labs/songbird-decentralized-moderation">Songbird</a> is a sketch of a
decentralised moderation system for IPFS.</li>
  <li><a href="https://cabal.chat/">Cabal</a> allows users to
<a href="https://twitter.com/substack/status/1349471659653124098">subscribe</a> to other users’ moderation
actions, such as blocking and hiding posts.</li>
  <li>An app called <a href="https://kc-fantastic-app.medium.com/decentralized-content-moderation-on-fantastic-app-3768989ced19">Fantastic</a>
is exploring mechanisms for moderation.</li>
  <li>Felix Dietze’s <a href="https://github.com/fdietze/notes/blob/master/felix_dietze_master_thesis_2015.pdf">2015 master’s thesis</a>
explores community-run moderation. He is also working on
<a href="https://felix.unote.io/hacker-news-scores">ranking</a>
<a href="https://github.com/fdietze/downvote-scoring">algorithms</a>
for news aggregators.</li>
  <li>Twitter is trialling <a href="https://blog.twitter.com/en_us/topics/product/2021/introducing-birdwatch-a-community-based-approach-to-misinformation.html">Birdwatch</a>,
a crowdsourced effort to tackle misinformation.</li>
  <li><a href="https://blog.coinbase.com/coinbases-philosophy-on-account-removal-and-content-moderation-c80d1aa452b7">Coinbase’s approach</a>
is to ban only content that is illegal in jurisdictions where they operate, or content that is
<a href="https://en.wikipedia.org/wiki/United_States_free_speech_exceptions">not considered protected speech</a>
under the U.S. First Amendment.</li>
</ul>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Using Bloom filters to efficiently synchronise hash graphs</title>
                <link>http://martin.kleppmann.com/2020/12/02/bloom-filter-hash-graph-sync.html</link>
                <comments>http://martin.kleppmann.com/2020/12/02/bloom-filter-hash-graph-sync.html#disqus_thread</comments>
                <pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2020/12/02/bloom-filter-hash-graph-sync.html</guid>
                
                <description><![CDATA[ This blog post uses MathJax to render mathematics. You need JavaScript enabled for MathJax to work. In some recent research, Heidi and I needed to solve the following problem. Say you want to sync a hash graph, such as a Git repository, between two nodes. In Git, each commit is... ]]></description>
                <content:encoded><![CDATA[
                    <p><em>This blog post uses <a href="https://www.mathjax.org/">MathJax</a> to render mathematics. You need JavaScript enabled for MathJax to work.</em></p>

<p>In some recent research, <a href="http://heidihoward.co.uk/">Heidi</a> and I needed to solve the following problem.
Say you want to sync a hash graph, such as a Git repository, between two nodes.
In Git, each commit is identified by its hash, and a commit may include the hashes of predecessor commits (a commit may include more than one hash if it’s a merge commit).
We want to figure out the minimal set of commits that the two nodes need to send to each other in order to make their graphs the same.</p>

<p>You might wonder: isn’t this a solved problem?
Git has to do this every time you do <code class="language-plaintext highlighter-rouge">git pull</code> or <code class="language-plaintext highlighter-rouge">git push</code>!
You’re right, and some cases are easy, but other cases are a bit trickier.
What’s more, the algorithm used by Git is not particularly well-documented, and in any case we think that we can do better.</p>

<p>For example, say we have two nodes, and each has one of the following two hash graphs (circles are commits, arrows indicate one commit referencing the hash of another).
The blue part (commit A and those to the left of it) is shared between the two graphs, while the dark grey and light grey parts exist in only one of the two graphs.</p>

<p><a href="/2020/12/hash-dag.png"><img src="/2020/12/hash-dag.png" width="550" height="258" alt="Illustration of two hash graphs" /></a></p>

<p>We want to reconcile the two nodes’ states so that one node sends all of the dark-grey-coloured commits, the other sends all of the light-grey-coloured commits, and both end up with the following graph:</p>

<p><a href="/2020/12/hash-dag2.png"><img src="/2020/12/hash-dag2.png" width="550" height="143" alt="Hash graph after reconciliation" /></a></p>

<p>How do we efficiently figure out which commits the two nodes need to send to each other?</p>

<h2 id="traversing-the-graph">Traversing the graph</h2>

<p>First, some terminology.
Let’s say commit A is a <em>predecessor</em> of commit B if B references the hash of A, or if there is some chain of hash references from B leading to A.
If A is a predecessor of B, then B is a <em>successor</em> of A.
Finally, define the <em>heads</em> of the graph to be those commits that have no successors.
In the example above, the heads are B, C, and D.
(This is slightly different from how Git defines <code class="language-plaintext highlighter-rouge">HEAD</code>.)</p>

<p>The reconciliation algorithm is easy if it’s a “fast-forward” situation: that is, if one node’s heads are commits that the other node already has.
In that case, one node sends the other the hashes of its heads, and the other node replies with all commits that are successors of the first node’s heads.
However, the situation is tricker in the example above, where one node’s heads B and C are unknown to the other node, and likewise head D is unknown to the first node.</p>

<p>In order to reconcile the two graphs, we want to figure out which commits are the latest common predecessors of both graphs’ heads (also known as <em>common ancestors</em>, marked A in the example), and then the nodes can send each other all commits that are successors of the common predecessors.</p>

<p>As a first attempt, we can try this: the two nodes send each other their heads; if those contain any unknown predecessor hashes, they request those, and repeat until all hashes resolve to known commits.
Thus, the nodes gradually work their way from the heads towards the common predecessors.
This works, but it is slow if your graph contains long chains of commits, since the number of round trips required equals the length of the longest path from a head to a common predecessor.</p>

<p>The “smart” transfer protocol used by Git essentially <a href="https://www.git-scm.com/docs/http-protocol">works like this</a>, except that it sends 32 hashes at a time in order to reduce the number of round trips.
Why 32? Who knows.
It’s a trade-off: send more hashes to reduce the number of round trips, but each request/response is bigger.
Presumably they decided that 32 was a reasonable compromise between latency and bandwidth.</p>

<p>Recent versions of Git also support an experimental <a href="https://github.com/git/git/commit/42cc7485a2ec49ecc440c921d2eb0cae4da80549">“skipping” algorithm</a>, which can be enabled using the <a href="https://git-scm.com/docs/git-config#Documentation/git-config.txt-fetchnegotiationAlgorithm"><code class="language-plaintext highlighter-rouge">fetch.negotiationAlgorithm</code> config option</a>.
Rather than moving forward by a fixed number of predecessors in each round trip, this algorithm allows some commits to be skipped, so that it reaches the common predecessors faster.
The skip size grows similarly to the Fibonacci sequence (i.e. exponentially) with each round trip.
This reduces the number of round trips to \(O(\log n)\), but you can end up overshooting the common predecessors, and thus the protocol may end up unnecessarily transmitting commits that the other node already has.</p>

<h2 id="bloom-filters-to-the-rescue">Bloom filters to the rescue</h2>

<p>In our new paper draft, which we are <a href="https://arxiv.org/abs/2012.00472">making available on arXiv today</a>, Heidi and I propose a different algorithm for performing this kind of reconciliation.
It is quite simple if you know how <a href="https://en.wikipedia.org/wiki/Bloom_filter">Bloom filters</a> work.</p>

<p>In addition to sending the hashes of their heads, each node constructs a Bloom filter containing the hashes of the commits that it knows about.
In our prototype, we allocate 10 bits (1.25 bytes) per commit.
This number can be adjusted, but note that it is a lot more compact than sending the full 16-byte (for SHA-1, used by Git) or 32-byte (for SHA-256, which is more secure) hash for each commit.
Moreover, we keep track of the heads from the last time we reconciled our state with a particular node, and then the Bloom filter only needs to include commits that were added since the last reconciliation.</p>

<p>When a node receives such a Bloom filter, it checks its own commit hashes to see whether they appear in the filter.
Any commits whose hash does not appear in the Bloom filter, and its successors, can immediately be sent to the other node, since we can be sure that the other node does not know about those commits.
For any commits whose hash does appear in the Bloom filter, it is likely that the other node knows about that commit, but due to false positives it is possible that the other node actually does not know about those commits.</p>

<p>After receiving all the commits that did not appear in the Bloom filter, we check whether we know all of their predecessor hashes.
If any are missing, we request them in a separate round trip using the same graph traversal algrorithm as before.
Due to the way the false positive probabilities work, the probability of requiring n round trips decreases exponentially as n grows.
For example, you might have a 1% chance of requiring two round trips, a 0.01% chance of requiring three round trips, a 0.0001% chance of requiring four round trips, and so on.
Almost all reconciliations complete in one round trip.</p>

<p>Unlike the skipping algorithm used by Git, our algorithm never unnecessarily sends any commits that the other side already has, and the Bloom filters are very compact, even for large commit histories.</p>

<h2 id="practical-relevance">Practical relevance</h2>

<p>In the paper we also prove that this algorithm allows nodes to sync their state even in the presence of arbitrarily many malicious nodes, making it immune to <a href="https://en.wikipedia.org/wiki/Sybil_attack">Sybil attacks</a>.
We then go on to prove a theorem that shows which types of applications can and cannot be implemented in this Sybil-immune way, without requiring any Sybil countermeasures such as <a href="https://en.wikipedia.org/wiki/Proof_of_work">proof-of-work</a> or the centralised control of <a href="https://arxiv.org/pdf/1711.03936.pdf">permissioned blockchains</a>.</p>

<p>All of this is directly relevant for <a href="https://www.inkandswitch.com/local-first.html">local-first</a> peer-to-peer applications in which apps running on different devices need to sync up their state without necessarily trusting each other or relying on any trusted servers.
I assume it’s also relevant for <a href="https://www.swirlds.com/downloads/SWIRLDS-TR-2016-01.pdf">blockchains that use hash graphs</a>, but I don’t know much about them.
So, syncing a Git commit history is just one of many possible use cases – I just used it because most developers will be at least roughly familiar with it!</p>

<p>The details of the algorithm and the theorems are in the <a href="https://arxiv.org/abs/2012.00472">paper</a>, so I won’t repeat them here.
Instead, I will briefly mention a few interesting things that didn’t make it into the paper.</p>

<h2 id="why-bloom-filters">Why Bloom filters?</h2>

<p>One thing you might be wondering: rather than creating a Bloom filters with 10 bits per commit, can we not just truncate the commit hashes to 10 bits and send those instead?
That would use the same amount of network bandwidth, and intuitively it may seem like it should be equivalent.</p>

<p>However, that is not the case: Bloom filters perform vastly better than truncated hashes.
I will use a small amount of probability theory to explain why.</p>

<p>Say we have a hash graph containing \(n\) distinct items, and we want to use \(b\) bits per item (so the total size of the data structure is \(m=bn\) bits).
If we are using truncated hashes, there are \(2^b\) possible values for each \(b\)-bit hash.
Thus, given two independently chosen, uniformly distributed hashes, the probability that they are the same is \(2^{-b}\).</p>

<p>If we have \(n\) uniformly distributed hashes, the probability that they are all different from a given \(b\)-bit hash is \((1-2^{-b})^n\).
The false positive probability is therefore the probability that a given \(b\)-bit hash equals one or more of the \(n\) hashes:</p>

<p>\[ P(\text{false positive in truncated hashes}) = 1 - (1 - 2^{-b})^n \]</p>

<p>On the other hand, with a Bloom filter, we start out with all \(m\) bits set to zero, and then for each item, we set \(k\) bits to one.
After one uniformly distributed bit-setting operation, the probability that a given bit is zero is \(1 - 1/m\).
Thus, after \(kn\) bit-setting operations, the probability that a given bit is still zero is \((1 - 1/m)^{kn}\).</p>

<p>A Bloom filter has a false positive when we check \(k\) bits for some item and they are all one, even though that item was not in the set.
The probability of this happening is</p>

<p>\[ P(\text{false positive in Bloom filter}) = (1 - (1 - 1/m)^{kn})^k \]</p>

<p>It’s not obvious from those expressions which of the two is better, so I plotted the false positive probabilities of truncated hashes and Bloom filters for varying numbers of items \(n\), and with parameters \(b=10\), \(k=7\), \(m=bn\):</p>

<p><a href="/2020/12/false-pos.png"><img src="/2020/12/false-pos.png" width="550" height="200" alt="Plot of false positive probability for truncated hashes and Bloom filters" /></a></p>

<p>For a Bloom filter, as long as we grow the size of the filter proportionally to the number of items (here we have 10 bits per item), the false positive probability remains pretty much constant at about 0.8%.
But truncated hashes of the same size behave much worse, and with more than about 1,000 items the false positive probability exceeds 50%.</p>

<p>The reason for this: with 10-bit truncated hashes there are only 1,024 possible hash values, and if we have 1,000 different items, then most of those 1,024 possible values are already taken.
With truncated hashes, if we wanted to keep the false positive probability constant, we would have to use more bits per item as the number of items grows, so the total size of the data structure would grow faster than linearly in the number of items.</p>

<p>Viewing it like this, it is quite remarkable that Bloom filters work as well as they do, using only a constant number of bits per item!</p>

<h2 id="further-details">Further details</h2>

<p>The Bloom filter false positive formula given above is the one that is commonly quoted, but it’s actually not quite correct.
To be precise, it is a <a href="https://www.sciencedirect.com/science/article/abs/pii/S0020019008001579">lower bound</a> on the exact false positive probability (<a href="https://git.gnunet.org/bibliography.git/plain/docs/FalsepositiverateBloomFilter2008Bose.pdf">open access paper</a>).</p>

<p>Out of curiosity I wrote a <a href="https://gist.github.com/ept/83b91aa07e2495c86ddd8c364a8cfbc7">little Python script</a> that calculates the false positive probability for truncated hashes, Bloom filters using the approximate formula, and Bloom filters using the exact formula.
Fortunately, for the parameter values we are interested in, the difference between approximate and exact probability is very small.
The <a href="https://gist.github.com/ept/83b91aa07e2495c86ddd8c364a8cfbc7">gist</a> also contains a <a href="http://www.gnuplot.info/">Gnuplot</a> script to produce the graph above.</p>

<p><a href="https://twitter.com/pvh">Peter</a> suggested that a <a href="https://en.wikipedia.org/wiki/Cuckoo_filter">Cockoo filter</a> may perform even better than a Bloom filter, but we haven’t looked into that yet.
To be honest, the Bloom filter approach already works so well, and it’s so simple, that I’m not sure the added complexity of a more sophisticated data structure would really be worth it.</p>

<p>That’s all for today.
Our paper is at <a href="https://arxiv.org/abs/2012.00472">arxiv.org/abs/2012.00472</a>.
Hope you found this interesting, and please let us know if you end up using the algorithm!</p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>New courses on distributed systems and elliptic curve cryptography</title>
                <link>http://martin.kleppmann.com/2020/11/18/distributed-systems-and-elliptic-curves.html</link>
                <comments>http://martin.kleppmann.com/2020/11/18/distributed-systems-and-elliptic-curves.html#disqus_thread</comments>
                <pubDate>Wed, 18 Nov 2020 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2020/11/18/distributed-systems-and-elliptic-curves.html</guid>
                
                <description><![CDATA[ I have just published new educational materials that might be of interest to computing people: a new 8-lecture course on distributed systems, and a tutorial on elliptic curve cryptography. Distributed Systems Since last year I have been delivering an 8-lecture undergraduate course on distributed systems at the University of Cambridge.... ]]></description>
                <content:encoded><![CDATA[
                    <p>I have just published new educational materials that might be of interest to computing people:
a new 8-lecture course on distributed systems, and a tutorial on elliptic curve cryptography.</p>

<h2 id="distributed-systems">Distributed Systems</h2>

<p>Since last year I have been delivering an 8-lecture undergraduate course on distributed systems at the University of Cambridge.
The first time I delivered it, I inherited the slides and exercises from the people who lectured it in previous years (Richard Mortier, Anil Madhavapeddy, Robert Watson, Jean Bacon, and Steven Hand), and I just used those materials with minor modifications.
It was a good course, but it was getting quite dated (e.g. lots of material on <a href="https://en.wikipedia.org/wiki/Common_Object_Request_Broker_Architecture">CORBA</a>, which is now of mostly historical interest).</p>

<p>Therefore, this year I decided to do a thorough refresh of the course content, and wrote a brand new set of slides and lecture notes.
Also, due to the pandemic we are not having any in-person lectures, so I recorded videos for all of the lectures.
I decided to make all of this available publicly under a <a href="https://creativecommons.org/licenses/by-sa/4.0/">creative commons CC BY-SA license</a>, which means that you’re welcome to use it freely (including incorporating it into your own work), provided that you give credit to me, and that you share your derived work under the same license.</p>

<p>The result is here:</p>

<ul>
  <li><a href="https://www.cl.cam.ac.uk/teaching/2021/ConcDisSys/dist-sys-notes.pdf">Lecture notes (PDF)</a> (including exercises)</li>
  <li>Slides: <a href="https://www.cl.cam.ac.uk/teaching/2021/ConcDisSys/dist-sys-slides.pdf">slideshow</a> and <a href="https://www.cl.cam.ac.uk/teaching/2021/ConcDisSys/dist-sys-handout.pdf">printable</a> (PDF)</li>
  <li><a href="https://www.youtube.com/playlist?list=PLeKd45zvjcDFUEv_ohr_HdUFe97RItdiB">Lecture videos (YouTube)</a></li>
  <li><a href="https://www.cst.cam.ac.uk/teaching/2021/ConcDisSys">Course web page</a></li>
  <li>Solution notes for the exercises are available on demand (<a href="/contact.html">email me</a> and convince me that you’re not a student trying to cheat).
Cambridge supervisors can <a href="https://www.cl.cam.ac.uk/teaching/2021/ConcDisSys/supervisors/dist-sys-solutions.pdf">download the solution notes directly</a> (Raven login required).</li>
</ul>

<p>The course is primarily designed for Cambridge undergraduate students, and it includes some cross-references to other courses.
Many other courses also make their notes or slides publicly available, so you can still look them up if you’re not at Cambridge by going to the <a href="https://www.cl.cam.ac.uk/teaching/2021/part1b-75.html">course web pages</a>.
(Many lecturers restrict their video recordings to Cambridge users only, so those might not be publicly available.)</p>

<p>The distributed systems course comprises about 7 hours of video and 87 pages of lecture notes.
It covers the following topics:</p>

<ol>
  <li>Introduction: distributed systems, computer networks, and RPC</li>
  <li>System models: network faults, crash and Byzantine faults, synchrony assumptions</li>
  <li>Physical clocks, clock synchronisation, and causality</li>
  <li>Logical time, broadcast protocols (reliable, FIFO, causal, total order)</li>
  <li>Replication, quorum protocols, state machine replication</li>
  <li>Consensus, details on the Raft consensus algorithm</li>
  <li>Replica consistency, two-phase commit, linearizability, eventual consistency</li>
  <li>Case studies: collaboration software, Google’s Spanner</li>
</ol>

<p>The main focus of this course is on understanding the algorithms and the principles that allow us to build robust and reliable distributed systems.
It uses examples of practical systems as motivation, and the videos include a few live demos of real distributed systems in action.
The aim is to convey the fundamentals without being excessively theoretical; there are a few mathematical proofs in the exercises, but most of the discussion is informal and example-based.</p>

<p>The level of this course is intended for second-year undergraduates.
Our students at this level have reasonable fluency with mathematical notation, and some background in programming languages and operating systems, so that’s what this course assumes.</p>

<h2 id="elliptic-curve-cryptography">Elliptic Curve Cryptography</h2>

<p>Another document I’m releasing today is called
<a href="https://martin.kleppmann.com/papers/curve25519.pdf">Implementing Curve25519/X25519: A Tutorial on Elliptic Curve Cryptography</a>.
There’s no video for this one, just a 30-page PDF.</p>

<p>Many textbooks cover the concepts behind Elliptic Curve Cryptography (ECC), but few explain how to go from the equations to a working, fast, and secure implementation.
On the other hand, while the code of many cryptographic libraries is available as open source, it can be <a href="https://github.com/jedisct1/libsodium/blob/master/src/libsodium/crypto_scalarmult/curve25519/ref10/x25519_ref10.c#L91-L132">rather opaque to the untrained eye</a>, and it is rarely accompanied by detailed documentation explaining how the code came about and why it is correct.</p>

<p>This tutorial bridges the gap between the mathematics and implementation of elliptic curve cryptography.
It is written for readers who are new to cryptography, and it assumes no more mathematical background than most undergraduate computer science courses.
Starting from first principles, this document shows how to derive every line of code in an implementation of the <a href="https://tools.ietf.org/html/rfc7748">X25519</a> Diffie-Hellman key agreement scheme, based on the <a href="https://ianix.com/pub/curve25519-deployment.html">widely-used Curve25519 elliptic curve</a>.
The implementation is based on Dan Bernstein et al.’s <a href="https://tweetnacl.cr.yp.to/">TweetNaCl</a>.
It is fast and secure; in particular, it uses constant-time algorithms to prevent side-channel attacks.</p>

<p>I wrote this because I wanted to learn how real implementations of ECC work, but I couldn’t find good resources that explained it, so I wrote the document as I figured it out step-by-step from a number of sources (and by doing a lot of the calculations myself).
I hope others will also find it useful.</p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Writing a book: is it worth it?</title>
                <link>http://martin.kleppmann.com/2020/09/29/is-book-writing-worth-it.html</link>
                <comments>http://martin.kleppmann.com/2020/09/29/is-book-writing-worth-it.html#disqus_thread</comments>
                <pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2020/09/29/is-book-writing-worth-it.html</guid>
                
                <description><![CDATA[ My book, Designing Data-Intensive Applications, recently passed the milestone of 100,000 copies sold. Last year, it was the second-best-selling book in O’Reilly’s entire catalogue, second only to Aurélien Géron’s machine learning book. Machine learning is obviously a hot topic, so I am quite content with coming second to it! 😄... ]]></description>
                <content:encoded><![CDATA[
                    <p>My book, <a href="https://dataintensive.net/">Designing Data-Intensive Applications</a>, recently passed the
milestone of 100,000 copies sold. Last year, it was the second-best-selling book in O’Reilly’s
entire catalogue, second only to
<a href="https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646/ref=as_li_ss_tl?ie=UTF8&amp;linkCode=ll1&amp;tag=dataintensive-20&amp;linkId=7d47ed0da85dc67659afbfbbad99f6ec&amp;language=en_US">Aurélien Géron’s machine learning book</a>.
Machine learning is obviously a hot topic, so I am quite content with coming second to it! 😄</p>

<p>To me, the success of this book was totally unexpected: while I was writing it, I thought that it
was going to be a bit niche, and I set myself the goal of selling 10,000 copies over the lifetime
of the book. Having passed that goal tenfold, this seems like a good opportunity to look back and
reflect on the process. I don’t want to make this post too self-congratulatory, but rather I will
try to share some insights into the business of book-writing.</p>

<h2 id="is-it-financially-worth-it">Is it financially worth it?</h2>

<p>Most books make very little money for both authors and publishers, but then occasionally something
like <em>Harry Potter</em> comes along. If you are considering writing a book, I strongly recommend that
you estimate the value of your future royalties to be close to zero. Like starting a band with
friends and hoping to become rock stars, it’s difficult to predict in advance what will be a hit and
what will flop. Maybe this applies less to technical books than to fiction and music, but I suspect
that even with technical books, there are a small number of hits, and most books sell quite modest
numbers.</p>

<p>That said, in my case, I am happy to report that writing this book has in retrospect turned out to
be a financially sound decision. These graphs show the royalties I have been paid since the book
first went on sale:</p>

<p><img src="/2020/09/royalties-cumulative.png" width="550" height="311" alt="Cumulative royalties chart" /></p>

<p><img src="/2020/09/royalties-monthly.png" width="550" height="311" alt="Monthly royalties chart" /></p>

<p>For the first 2½ years the book was in “early release”: during this period I was still writing, and
we released it in unedited form, one chapter at a time, as ebook only. Then in March 2017 the book
was officially published, and the print edition went on sale. Since then, the sales have fluctuated
from month to month, but on average they have stayed remarkably constant. At some point I would
expect the market to become saturated (i.e. most people who were going to buy the book have already
bought it), but that does not seem to have happened yet: indeed, sales noticeably increased in late
2018 (I don’t know why). The x axis ends in July 2020 because from the time of sale, it takes
a couple of months for the money to trickle through the system.</p>

<p>My contract with the publisher specifies that I get 25% of publisher revenue from ebooks, online
access, and licensing, 10% of revenue from print sales, and 5% of revenue from translations. That’s
a percentage of the wholesale price that retailers/distributors pay to the publisher, so it doesn’t
include the retailers’ markup. The figures in this section are the royalties I was paid, after the
retailer and publisher have taken their cut, but before tax.</p>

<p>The total sales since the beginning have been (in US dollars):</p>

<ul>
  <li>Print: 68,763 copies, $161,549 royalties ($2.35/book)</li>
  <li>Ebook: 33,420	copies, $169,350 royalties ($5.07/book)</li>
  <li>O’Reilly online access (formerly called Safari Books Online): $110,069 royalties
(I don’t get readership numbers for this channel)</li>
  <li><a href="http://dataintensive.net/translations.html">Translations</a>: 5,896 copies, $8,278 royalties ($1.40/book)</li>
  <li>Other licensing and sponsorship: $34,600 royalties</li>
  <li>Total: 108,079 copies, $477,916</li>
</ul>

<p>A lot of money, but I also put a lot of time into it! I estimate that I spent about 2.5 years of
full-time equivalent work researching and writing the book, spread out over the course of 4 years.
Of that time, I spent one year (2014–15) working full-time on the book without income, while the
rest of the time I worked on the book part-time alongside part-time employment.</p>

<p>Now, in retrospect, it turns out that those 2.5 years were a good investment, because the income
that this work has generated is in the same ballpark as the Silicon Valley software engineering
salary (including stock and benefits) I could have received in the same time if I hadn’t quit
LinkedIn in 2014 to work on the book. But of course I didn’t know that at the time! The royalties
could easily have turned out to be a factor of 10 lower, in which case it would have been
a financially much less compelling proposition.</p>

<h2 id="beyond-the-royalties">Beyond the royalties</h2>

<p>Part of the success of my book might also be explained by the fact that I put a lot of effort into
promoting it. Since the book went into early release I have given <a href="/talks.html">almost 50 talks</a> at
major conferences, plus a bunch of additional invited talks at companies and universities. Every
single talk contained at least a small advertisement for my book. Like a band going on tour to
promote their latest album, I suspect these talks contributed to the book being widely known.
A couple of my blog posts have also been quite popular, and these may also have brought the book to
potential readers’ attention. I have now significantly dialled back my speaking commitments, so
I assume it is mostly spreading via word of mouth
(<a href="https://twitter.com/intensivedata/likes">social media</a>, and readers recommending it to their
colleagues).</p>

<p>The combination of talks and the book have allowed me to establish a significant public presence and
reputation in this field. I now get far more invitations to speak at conferences than I can
realistically accept. Conference talks don’t generate income <em>per se</em> (good industry conferences
generally pay for speakers’ travel and accommodation, but they rarely pay speaking fees), but this
kind of reputation is helpful for getting consulting gigs.</p>

<p>I have only done a bit of consulting (and I now regularly turn down consulting requests from
companies because I’m focussing on my research), but I suspect that in my current position it would
be fairly easy to establish a lucrative consulting and training business, going into companies and
helping them with their data infrastructure problems. That is further financial value that writing
a book can bring: you become recognised as an expert and an authority in an area, and companies will
pay good money to get advice from such experts.</p>

<p>I have focussed a lot on the financial viability of writing a book because I believe that books are
an extremely valuable educational resource (more on this below). I want more people to write books,
and that requires book-writing to be a sustainable activity.</p>

<p>I was able to spend a great deal of time doing background research for my book because I was able to
afford to live without a salary for a year, but many people will not be able to do that. If people
can get <a href="https://jvns.ca/blog/2018/09/01/who-pays-to-educate-developers-/">paid fairly for creating educational materials</a>,
we will get more and better educational materials.</p>

<p>The economics of book-writing are challenging, and I reiterate that the success of my book is
atypical. However, I also find it heartening that it is <em>possible</em> to make a decent living from
technical writing. Not guaranteed, but possible, and that gives me hope.</p>

<h2 id="a-book-is-accessible-education">A book is accessible education</h2>

<p>Besides financial value to the author, there are lots of other good things about writing books.</p>

<p>A book is universally <strong>accessible</strong>: it is affordable to almost everyone, anywhere in the world. It
is vastly cheaper than a university course or corporate training, and you don’t have to move to
another city to take advantage of it. People in rural areas and developing countries can benefit
equally to those living in the global centres of tech. You can skim it or read it carefully cover to
cover, as you please. You don’t even need an internet connection to use it. Of course it doesn’t
confer all of the benefits of a university education (such as individual feedback, credentials,
professional network, social life), but as a medium for communicating knowledge, a book is almost
unbeatably efficient.</p>

<p>Of course there are also plenty of free resources online: Wikipedia, blog posts, videos, Stack
Overflow, API documentation, research papers, and so on. These are good as reference material for
answering a concrete question that you have (such as “what are the parameters of the function
foo?”), but they are piecemeal fragments that are difficult to assemble into a coherent education.
On the other hand, a good book provides a carefully selected and designed programme of study, and
a narrative that is particularly valuable when you are trying to make sense of a complex topic for
the first time.</p>

<p>Compared to teaching people in person, a book is vastly more scalable. Even if I lecture in my
university’s largest lecture theatre for the rest of my career, I will not get anywhere near
teaching 100,000 people. For individual and small-group teaching, the disparity is greater still.
Yet a book is able to reach such large numbers of people routinely.</p>

<h2 id="creating-more-value-than-you-capture">Creating more value than you capture</h2>

<p>Writing a book is an activity that
<a href="http://radar.oreilly.com/2009/01/work-on-stuff-that-matters-fir.html">creates more value than it captures</a>.
What I mean with this is that the benefits that readers get from it are greater than the price they
paid for the book. To back this up, let’s try roughly estimating the value created by my book.</p>

<p>Of the 100,000 people who have bought my book so far, let’s say that two thirds of them intend to
read it but actually haven’t got round to it yet. Of those who have read it, let’s say that one
third were able to actually apply some of the ideas in the book, and two thirds read it purely out
of interest. So let’s say conservatively that 10% of people who bought the book, that is 10,000
people, have applied it for some useful purpose.</p>

<p>What might such a useful purpose look like? In the case of my book, much of it is about making
architectural decisions regarding data storage. If you get them right, you can build some amazing
systems; if you get them wrong, you have to spend ages painfully digging yourself out of a mess that
you got yourself into.</p>

<p>It’s hard to quantify that, but let’s say that the people who applied ideas from the book avoided
a bad decision that would have taken them one month of engineering time to rectify. (I’d actually
love to claim that the time saving is much higher, but let’s be conservative in our estimates.)
Thus, the 10,000 readers who applied the knowledge freed up an estimated 10,000 months, or 833
years, of engineering time to spend on things that are more useful than digging yourself out of
a mess.</p>

<p>If I spend 2.5 years writing a book, and it saves other people 833 years of time in aggregate, that
is over 300x leverage. If we assume an average engineering salary on the order of $100k, that’s $80m
of value created. Readers have spent approximately $4m buying those 100,000 books, so the value
created is about 20 times greater than the value captured. And this is based on some very
conservative estimates.</p>

<p>There are further ways in which the book creates value. For example, lots of readers have sent me
emails and tweets saying that because they read my book, they did well in a job interview, landing
them their dream job and providing financial security for their family. I don’t know how to measure
that sort of value created, but I think it’s tremendous.</p>

<p>How to be a 10x engineer:
<a href="https://twitter.com/peterseibel/status/512615519934230528">help ten other engineers be twice as good</a>.
Producing high-quality educational materials enables you to be a 300x engineer.</p>

<h2 id="conclusions">Conclusions</h2>

<p>Writing a technical book is not easy, but it is:</p>

<ul>
  <li>valuable (it helps people be better at their job),</li>
  <li>scalable (large numbers of people can benefit from it),</li>
  <li>accessible (it doesn’t discriminate who can benefit), and</li>
  <li>economically viable (it is possible to generate a reasonable level of income from it).</li>
</ul>

<p>It would be interesting to compare it to working on open source software, another activity that can
have significant positive impact but is
<a href="https://www.amazon.com/Working-Public-Making-Maintenance-Software/dp/0578675862/ref=as_li_ss_tl?ref_=nav_signin&amp;&amp;linkCode=ll1&amp;tag=dataintensive-20&amp;linkId=a152ade47ab23a9351d3e24950a89515&amp;language=en_US">difficult to get paid for</a>.
I don’t have a strong opinion on this at the moment.</p>

<p>On the downside, writing a book is really hard, at least if you want to do it well. For me it was
about the same level of difficulty as building and selling a
<a href="https://www.crunchbase.com/organization/rapportive">startup</a> (YMMV), that is to say, involving
multiple existential crises. The writing process was not good for my mental health. For that reason
I haven’t rushed into writing another book: the scars from writing the first one are still too
fresh. But the scars gradually do fade, and I’m hoping (perhaps naively) that it might be easier
next time.</p>

<p>On balance, I do think that writing a technical book is worth it. The feeling of knowing that you
have helped a lot of people is gratifying.  The personal growth that comes from taking on such
a challenge is also considerable. And there is no better way to learn something in depth than by
explaining it to others.</p>

<p>In my next post I will provide some advice on writing and publishing from my experience so far.</p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>A brief Brexit lament</title>
                <link>http://martin.kleppmann.com/2020/01/31/brief-brexit-lament.html</link>
                <comments>http://martin.kleppmann.com/2020/01/31/brief-brexit-lament.html#disqus_thread</comments>
                <pubDate>Fri, 31 Jan 2020 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2020/01/31/brief-brexit-lament.html</guid>
                
                <description><![CDATA[ It’s Brexit day, and I am sad. For me, something precious is lost today, and I would like to attempt to explain why. My father is German and my mother British. I have dual citizenship, grew up bilingually in Germany, and then moved to the UK over 16 years ago.... ]]></description>
                <content:encoded><![CDATA[
                    <p>It’s Brexit day, and I am sad.
For me, something precious is lost today, and I would like to attempt to explain why.</p>

<p>My father is German and my mother British.
I have dual citizenship, grew up bilingually in Germany, and then moved to the UK over 16 years ago.
In my grandparents’ generation, our two countries fought a terrible war against each other, and yet, in my generation, many people like myself are the children of European love.</p>

<p>Comparing Germany and the UK, I have noticed how Europe seems to be perceived very differently in each country.
In the UK, the relationship with Europe is regarded mostly in economic terms, as a free-trade zone.
In Germany, it is regarded in the first instance as a peace project.</p>

<p>While the economic aspects are obviously important, the difference in perspective has profound implications.
The European project is not a marriage of mere economic convenience, to be divorced again as soon as we believe to have found a better “deal” elsewhere (ignoring the question of whether such a deal may or may not actually materialise).
If you regard Europe as a peace project, leaving it seems ridiculous.
Why would you <em>not</em> want to be part of a peace project?</p>

<p>In Europe, we learn each others’ languages, we visit our twinned cities for school exchanges, concerts etc., we study abroad for a year through the Erasmus programme, we set up funding so that the richer regions in Europe support the development in poorer regions, and so on. 
Basically, the thinking goes: if the people of Europe make friends and find their spouses in other European countries, if they understand each other, trade with each other and support each other, then it is less likely that a demagogue will be able to lure them into fighting with each other again.</p>

<p>Although many British fought and fell in the world wars, and even the smallest village has a war memorial, the memory seems to be different.
I get the impression that British remembrance is often framed in a context of “defeating evil”, while German memories are around the horrors of brutal dictatorship, senseless destruction, pointless death, and the terrible trauma of the Holocaust.
There is nothing glorious in war, only suffering.
No winners, only losers.</p>

<p>I’m not claiming the EU is perfect; every human institution has flaws.
However, the way to improve it is not by throwing a screaming tantrum, running out of the door and banging it shut.
The way to improve an institution is to work from the inside to reform it and shape it to be what we want.
And so I mourn that my home country of choice, which I like in so many other ways, has decided to turn up its nose at this project.</p>

                ]]></content:encoded>
            </item>
        
            <item>
                <title>Research update for 2019</title>
                <link>http://martin.kleppmann.com/2019/10/30/research-update.html</link>
                <comments>http://martin.kleppmann.com/2019/10/30/research-update.html#disqus_thread</comments>
                <pubDate>Wed, 30 Oct 2019 00:00:00 +0000</pubDate>
                <dc:creator>Martin Kleppmann</dc:creator>
                
                    <guid isPermaLink="true">http://martin.kleppmann.com/2019/10/30/research-update.html</guid>
                
                <description><![CDATA[ It has now been four years since I moved from industry into academic research, and a lot has happened in this time. In 2015 I posted a year in review blog post, and in 2016 I announced the TRVE DATA project (which I am still working on), but I haven’t... ]]></description>
                <content:encoded><![CDATA[
                    <p>It has now been four years since I moved from industry into academic research, and
a lot has happened in this time.
In 2015 I posted a <a href="/2015/12/28/year-2015-review.html">year in review</a> blog post, and
in 2016 I announced the <a href="/2016/04/15/announcing-trve-data.html">TRVE DATA</a> project
(which I am still working on), but I haven’t posted an update on my work since.
So here goes!</p>

<p>In academic terms, things have been going well. Last year I
<a href="https://twitter.com/martinkl/status/1066748982129504256">got my PhD</a>, 
and as of October 2019 I got another few upgrades to my titles:</p>

<ul>
  <li>I am now an Early Career Fellow of the <a href="https://www.leverhulme.ac.uk/early-career-fellowships">Leverhulme Trust</a>,
with a matching fellowship from the <a href="https://www.newtontrust.cam.ac.uk">Isaac Newton Trust</a>.</li>
  <li>In the department I am now a “Senior Research Associate” (SRA, up from “Research Associate” previously).</li>
  <li>At <a href="https://www.corpus.cam.ac.uk">Corpus Christi College</a> I am now a
<a href="https://www.corpus.cam.ac.uk/people/dr-martin-kleppmann">Fellow</a> and Director of Studies
(which means that I look after the computer science students in the college).</li>
</ul>

<p>The fellowships from the Leverhulme Trust and Isaac Newton Trust are paying my salary
for the next three years, along with additional support from the
<a href="https://mobicentre.cst.cam.ac.uk">Cambridge Centre for Mobile, Wearable Systems and Augmented Intelligence</a>.
The college fellowship mainly involves a very generous dining allowance, and being part of
a community of academics across a broad range of subjects (it’s nice sometimes to talk to people who
are not computer scientists).</p>

<p>My job title of “Senior Research Associate” reflects my status as an independent researcher: that is,
I set my own research agenda. However, to be clear, <em>independent</em> does not mean <em>alone</em>! My experience
of research has been a very sociable one, and all of my work has been in collaboration with others.
If you want to go far,
<a href="https://www.npr.org/sections/goatsandsoda/2016/07/30/487925796/it-takes-a-village-to-determine-the-origins-of-an-african-proverb">go together</a>.
On that note, shout-out to my fine collaborators of the last few years (in alphabetic order),
<a href="https://www.cl.cam.ac.uk/~arb33/">Alastair Beresford</a>,
<a href="https://www.cl.cam.ac.uk/~vb358/">Victor Gomes</a>,
<a href="https://www.cl.cam.ac.uk/~sak70/">Stephan Kollmann</a>,
<a href="http://dominic-mulligan.co.uk/">Dominic Mulligan</a>,
<a href="https://personal.cis.strath.ac.uk/d.thomas/">Daniel Thomas</a>,
<a href="https://twitter.com/pvh">Peter van Hardenberg</a>,
<a href="https://www.cl.cam.ac.uk/~dac53/">Diana Vasile</a>,
<a href="http://about.adamwiggins.com/">Adam Wiggins</a>,
and several more people from the <a href="https://www.inkandswitch.com">Ink &amp; Switch</a> research lab!</p>

<p>Huge thanks to <a href="http://www.boeing.com">The Boeing Company</a> for funding my work for the last four years.
Huge thanks also to <a href="https://www.cl.cam.ac.uk/~arb33/">Prof. Alastair Beresford</a>, my excellent
adviser, mentor, collaborator, and PI (that’s academic-speak for “boss”) over the last four years.</p>

<h2 id="research-funding">Research funding</h2>

<p>The Leverhulme Trust and Isaac Newton Trust, which are funding my work, are UK charities that support
research across many subjects and disciplines, including humanities and social sciences. They fund about
<a href="https://www.leverhulme.ac.uk/early-career-fellowships-2019">140 early career fellowships</a> per year
across all subjects; only one or two per year of these are in computer science. So it looks like
I’m the computer scientist for this year!</p>

<p>A great aspect of this charity funding is that I am free to publish all my work as open source and
open access, with no restrictions. All the code I write is in a public repository by default. This
is very important to me, because the goal behind the things I’m working on (see below) is to
maximise the public benefit of these technologies through open source and open standards.</p>

<p>A downside is that all my positions are for a fixed three-year term (they are not tenure-track),
and I don’t know what comes afterwards. But for now I am going to concentrate on making the most I
can out of those three years.</p>

<h2 id="background-to-my-research">Background to my research</h2>

<p>Nowadays, we increasingly depend on Internet services for communication and collaboration: for
example, we use Google Docs to collaborate on documents, spreadsheets and presentations; we copy
files between devices using Dropbox; we communicate with colleagues using Slack; and we use many
other online services for task tracking, note taking, project planning, knowledge management, and
more.</p>

<p>These services are very valuable and convenient, but their use is also risky because they are
provided through a centralised server infrastructure. If the company providing the service goes out
of business, or decides to discontinue a product, the servers are shut down, the software stops
working, and users are locked out of the documents and data created with that software.</p>

<p>Moreover, since those servers typically process user data in unencrypted form, a rogue employee, or
an adversary who gains access to the servers, can read and tamper with vast amounts of sensitive
data. The provider may also use the data in arbitrary ways, e.g. to train their machine learning
systems and target you with ads.</p>

<p>When these risks are unacceptable, we can fall back to what we might call “old-fashioned”
collaboration methods: for example, one person creates a spreadsheet with Excel and emails it to
their collaborator, who makes changes and then sends the modified file back again by email. This
approach has merits: it does not rely on any external services that might go away (besides the email
infrastructure), and the file can easily be encrypted. However, it quickly becomes messy if the
file is modified by more than one person at a time.</p>

<h2 id="research-goals">Research goals</h2>

<p>Together with my collaborators I am developing the foundations of a new kind of collaboration
software, which we are calling <a href="https://www.inkandswitch.com/local-first.html">local-first software</a>.
It aims to achieve the best of both worlds: allowing the user-friendly real-time collaboration of
applications like Google Docs, in which several people can make changes simultaneously without
suffering conflicts, but without relying on trusted, centralised servers.</p>

<p>While most of today’s Internet services keep the primary copy of any shared data on a server, the
local-first approach stores primary copies of the data as files on the collaborators’ devices, like
in “old-fashioned” collaboration. Servers may still be used, but rather than being a linchpin, they
become an optional enabling component. Because all the data is local, the software continues
working, even when the device has no Internet access or the servers are unavailable. When a user
modifies a document, local-first software automatically sends the changes to collaborators whenever
a network connection is available, so there is no need to email files back and forth.</p>

<p>Local-first software allows multiple users to make changes to the same document concurrently, even
while users are offline, and ensures that all of the changes are automatically merged into
a consistent result. We do this using <a href="https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type">CRDTs</a>.
In this regard our approach differs from version control systems such as SVN or Git, which require
conflicts to be resolved manually, and only offer merging of plain text files. By contrast, we can
perform all merges automatically, and support arbitrarily complex file formats such as spreadsheets,
CAD drawings, or databases with various data models.</p>

<p>Collaborators’ devices can either communicate directly, using fast local networks in a
<em>peer-to-peer</em> manner, or indirectly via servers. To protect the confidentiality and integrity of
the communication between collaborators we plan to use <em>end-to-end encryption</em>. In this approach, if
servers are used, they only ever handle encrypted data that they cannot decrypt. Thus, even if
communication networks or servers are compromised by an attacker, user privacy and data
confidentiality are protected.</p>

<p>This approach is particularly suitable for sensitive data such as a university’s student records,
a hospital’s patient records, legally privileged communication, journalistic investigations, law
enforcement, diplomatic correspondence, and many other settings where regulations and
confidentiality obligations prohibit the sharing of unencrypted data with third parties.</p>

<h2 id="research-outputs">Research outputs</h2>

<p>The results of our research will be published in two forms: as traditional research papers in
academic venues, and in the form of open source software.</p>

<p>Research papers are important because they document the thought process and reasoning behind our
designs, and help others build upon our work in the future. We have already written a series of
<a href="/#publications">publications</a> about our research in the last four years, and there is lots more
interesting material still to come.</p>

<p>Software releases are traditionally regarded as less important in academia (and, to be honest, a lot
of code written by researchers is not very good). However, I regard open source software as a
crucial output for this project. <a href="https://github.com/automerge/automerge">Automerge</a> is the main
CRDT implementation I am working on, and it is designed from the start to be production-quality (in
terms of reliability, test coverage, stability, API design, documentation, community, and so on).</p>

<p>Automerge is not yet perfect, especially in terms of its performance: if it was, our research would
already be done! But I have a plan that should lead to big improvements in the coming year, and the
goal is that Automerge will soon be suitable for building ambitious, large-scale, local-first
applications.</p>

<p>In order to maximise the number of projects that can benefit from this work, the code is licensed
under the liberal <a href="https://opensource.org/licenses/MIT">MIT license</a>. Moreover, as the data format
(for representing documents on disk and for network communication) becomes stable, I think it will
make sense to formalise it as an open standard, with interoperable implementations in several
different programming languages and platforms.</p>

<p>And finally, we are starting to see an emerging community of users and contributors around Automerge.
New community members are regularly popping up in the
<a href="https://communityinviter.com/apps/automerge/automerge">Automerge Slack</a>, users are helpfully
reporting bugs, and a steadily growing
<a href="https://github.com/automerge/automerge/graphs/contributors">set of contributors</a> have had their
pull requests merged. It’s exciting to see this growing community engagement.</p>

<h2 id="interviews">Interviews</h2>

<p>If you want to hear more, I’ve done a bunch of interviews with various podcasts and blogs over the
last few years:</p>

<ul>
  <li><a href="https://softwareengineeringdaily.com/2017/05/02/data-intensive-applications-with-martin-kleppmann/">With Software Engineering Daily about my book</a>,
“Designing Data-Intensive Applications” (May 2017)</li>
  <li><a href="https://softwareengineeringdaily.com/2017/12/08/decentralized-objects-with-martin-kleppman/">With Software Engineering Daily again</a>,
this time about my research on CRDTs (December 2017)</li>
  <li><a href="https://advancetechmedia.org/episode-008-martin-kleppmann/">With the Advance Tech Podcast</a>
about a wide range of topics that I find interesting (October 2017)</li>
  <li><a href="https://www.investedinvestor.com/articles/2018/1/23/martin-kleppmann">With the Invested Investor podcast</a>
about my startup career before I switched towards research (January 2018)</li>
  <li><a href="/2019/06/27/hydra-interview.html">With the Hydra conference</a> about distributed systems in general,
and my work specifically (June 2019)</li>
  <li><a href="https://medium.com/csr-tales/csrtale-13-formal-verification-of-strong-eventual-consistency-1cc0af942e64">With Computer Science Research (CSR) Tales</a>
(July 2019) about the background story behind our paper
“<a href="/2017/10/25/verifying-crdt-isabelle.html">Verifying Strong Eventual Consistency in Distributed Systems</a>”,
which won the Distinguished Paper Award and Distinguished Artifact Award at the OOPSLA 2017 conference.</li>
</ul>

                ]]></content:encoded>
            </item>
        
    </channel>
</rss>
